\input{../definitions}
\title{\SM{} - Study Guide - Milestone 11: Motion Compensation in the DWT Domain}

\maketitle

\section{Description}

In this milestone we will discuss the characteristics of the motion
estimation and compensation of frames (considering only one component)
in the DWT domain.

\subsection{Removing the temporal redundancy through Motion Compensation (MC)}
The next natural step in the process of decorrelating the sequence of
frames is to remove the temporal redundancy by means of Motion
Compensation (MC). Basically, MC consists in substracting to the video
data a prediction performed with the information that is avaliable to
the decoder. If this prediction is accurate, the result of this
operation is a residual video with a lower temporal redundancy, that
can be compressed with a higher compression ratio because there is
less information to encode in the residue sequence than in the
original one.

\subsection{Integer pixel accuracy In-Band Motion Estimation and Compensation}
At this stage of the encoding process, the video data is represented
in the DWT domain, and therefore, we need to perform a In-Band Motion
estimation and Compensation
(IBMC)~\cite{andreopoulos2005complete}). Let's suppose that the number
of levels of the DWT is 1, and therefore, each frame has been
decomposed into two 2D subbands $L$ and $H$ (remember that using the
notation introduces in the previous milestone, $H$ has inside the
three high-frequency subbands: $LH$, $HL$ and $HH$, and that
$L=LL$). This discussion will be also constrained to the case in which
the movement of the objects in the scene is a integer number of
pixels.

\subsection{The lack of shift-invariance in the DWT domain}
Unfortunately, DWT decompositions are shift-variant as a consequence
of the downsampling performed during the DWT to achieve a critical
representation. This can be seen in the Fig.~\ref{fig:DWT} were some
DWT coefficients of a test video with three test frames has been
shown. As it can be seen, when the circle moves to the left only one
pixel (as happens in the frames 0 and 1), the value of the
coefficients that correspond to the circunference of the circle are
different between the reference frame (0) and the predicted frame
(1). This makes quite difficult to estimate and compensate the motion
between frames. Notice also that the effects of shift-variance is also
visible after using the inverse transform when the coefficients are
filtered or quantized, because the aliasing between the filters is not
completely cancelled in this case~\cite{bradley2003shift}.

\begin{figure}
  \centering
  \begin{tabular}{ccc}
    \vbox{\png{frame_0_Y}{300}} & \vbox{\png{frame_1_Y}{300}} & \vbox{\png{frame_2_Y}{300}} \\
    & \vbox{\svg{movement_0}{300}} & \vbox{\svg{movement_1}{300}} \\
    \vbox{\png{f0_haar_LL}{300}} & \vbox{\png{f1_haar_LL}{300}} & \vbox{\png{f2_haar_LL}{300}} \\
    \vbox{\png{f0_haar_LH}{300}} & \vbox{\png{f1_haar_LH}{300}} & \vbox{\png{f2_haar_LH}{300}} \\
    \vbox{\png{f0_haar_HL}{300}} & \vbox{\png{f1_haar_HL}{300}} & \vbox{\png{f2_haar_HL}{300}} \\
    \vbox{\png{f0_haar_HH}{300}} & \vbox{\png{f1_haar_HH}{300}} & \vbox{\png{f2_haar_HH}{300}} \\
    & \vbox{\svg{f0_1_haar_LL}{300}} & \vbox{\svg{f0_2_haar_LL}{300}} \\
    & \vbox{\svg{f0_1_haar_LH}{300}} & \vbox{\svg{f0_2_haar_LH}{300}} \\
    & \vbox{\svg{f0_1_haar_HL}{300}} & \vbox{\svg{f0_2_haar_HL}{300}} \\
    & \vbox{\svg{f0_1_haar_HH}{300}} & \vbox{\svg{f0_2_haar_HH}{300}}
  \end{tabular}
  \caption{A demonstration of the shift-variance of the DWT. Similar
    results have been obtained for other filters. See this
    \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/study_guide/11-MC_in_DWT_domain/DWT_shift_invariance.ipynb}{notebook}.}
\label{fig:DWT}
\end{figure}

However, suprisingly, at it can be also seen in the
Fig.~\ref{fig:DWT}, when the circle has traveled two pixels (frames 0
and 2), a perfect match is achieved! The reason why the 1-pixel motion
generates different coefficients in the reference and the predicted
frames, and the same coefficients for a 2-pixel motion is because, in
the first case the right coefficients were discarded by the
downsamplers, and in the second case not.

Usually, we call phases to the two possible coefficients resulting
from one (1D) filter to be subsampled, being the even phase, the even
coefficients, and the odd phase, the odd coefficients. Therefore, when
the motion is of type ``even'' (when we have a $2N$-pixels motion), we
should use the even phase to compensate the frames, and viceversa (use
the odd phase to compensate a $2N+1$-pixels motion). Notice that in
the 2D case, and always working with only one level of the DWT, we
have up to four different phases: (even, even)-, (even, odd)-, (odd,
even)-, and (odd, odd)-phase coefficients. Thus, depending on the type
of motion detected, the corresponding phase should be selected.

\begin{figure}
  \centering
  \begin{tabular}{ccc}
    \vbox{\png{f0_ohaar_LL}{300}} & \vbox{\png{f1_ohaar_LL}{300}} & \vbox{\png{f2_ohaar_LL}{300}} \\
    \vbox{\png{f0_ohaar_LH}{300}} & \vbox{\png{f1_ohaar_LH}{300}} & \vbox{\png{f2_ohaar_LH}{300}} \\
    \vbox{\png{f0_ohaar_HL}{300}} & \vbox{\png{f1_ohaar_HL}{300}} & \vbox{\png{f2_ohaar_HL}{300}} \\
    \vbox{\png{f0_ohaar_HH}{300}} & \vbox{\png{f1_ohaar_HH}{300}} & \vbox{\png{f2_ohaar_HH}{300}} \\
    & \vbox{\svg{f0_1_ohaar_LL}{300}} & \vbox{\svg{f0_2_ohaar_LL}{300}} \\
    & \vbox{\svg{f0_1_ohaar_LH}{300}} & \vbox{\svg{f0_2_ohaar_LH}{300}} \\
    & \vbox{\svg{f0_1_ohaar_HL}{300}} & \vbox{\svg{f0_2_ohaar_HL}{300}} \\
    & \vbox{\svg{f0_1_ohaar_HH}{300}} & \vbox{\svg{f0_2_ohaar_HH}{300}}
  \end{tabular}
  \caption{A demonstration of the shift-invariance of the ODWT. See this
    \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/study_guide/11-MC_in_DWT_domain/ODWT_shift_invariance.ipynb}{notebook}.}
\label{fig:odwt}
\end{figure}

\subsection{Recovering the lost phases}
There are different alternatives for regenerating the phases discarded
by the subsamplers of the DWT. This is equivalent to compute the
Overcomplete DWT (ODWT)~\cite{mallat1999wavelet}.
\begin{enumerate}
\item Use the Algorithme \`a Trous~\cite{mallat1999wavelet}, which
  basically consists in removing the downsamplers, avoiding thus the
  aliasing artifacts generated by the noncompliance with the sampling
  theorem. See this
  \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/study_guide/11-MC_in_DWT_domain/regenerating.ipynb}{notebook}.
\item Considering the previous experiments, it's easy to see that if
  we shift the signal one sample and perform the DWT, we get the
  ``lost'' phase. This method has been used to perform efficient MC in
  the DWT domain~\cite{park2000motion,li2001all}. See this \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/study_guide/11-MC_in_DWT_domain/ODWT_with_delay.ipynb}{notebook}.
\item Apply some transform (such as for example, the CODWT
  (Complete-to-Overcomplete DWT)~\cite{andreopoulos2005complete}) to
  the DWT to reconstruct the ODWT.
\end{enumerate}
The Fig.~\ref{fig:odwt} shows the shift invariance of the ODWT.

\subsection{About using the lost phases in IBMC}
Up to date, all the video codecs that use critically sampled IBMC also
use
\href{https://vicente-gonzalez-ruiz.github.io/video_compression/}{block-based
  motion compensation}. This technique divides the frames into
non-overlaping blocks and computes a motion vector for every block,
that provides a projection (a prediction) $\hat{P}$ of the reference
frame $R$ that must be as close as possible to the predicted frame
$P$. These blocks usually have a size of 16x16 pixel.

The use of blocks imples that:
\begin{enumerate}
\item If $N$ is the number of pixels in a frame, $N/256$ is the number
  of motion vectors. Therefore, if the motion vectors field has to be
  sent to the decoder, the data overhead is small (although this
  depends on the length of the representation of the texture).
\item All the coefficients that correspond to the same block has the
  same phase. Thus, if the phase also has to be sent to the decoder,
  again, the data overhead can be considered small.
\end{enumerate}

Unfortunately, there is a problem with mixing the phases. To
reconstruct the border pixels of the blocks, the adjacent (same phase)
coefficients must be also used by the decoder (see this
\href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/study_guide/11-MC_in_DWT_domain/mixing_phases.ipynb}{notebook}). For
this reason, the size of the blocks affects to the compression ratio
(the smaller the blocks, the higher the number of adajacent
coefficients, and therefore, the lower the compression ratio). We can
think about that this effect can be mitigated using larger block
sizes, but this will also affect to the compression ratio because the
quality of the predictions worsen with the increment of the size of
the blocks. This supposes an optimization problem that it's hard to
solve, especially in real-time applications.

\subsection{MC in the Laplacian Pyramid}
The Laplacian Pyramid, that was proposed by Burt and
Adelson~\cite{burt1987laplacian} and has been used for the design of
spatially-scalable image and video codecs, such as
SHVC~\cite{sullivan2012overview}. The

\begin{figure}
  \centering
  \begin{tabular}{ccc}
    \vbox{\png{f0_LP_level1}{300}} & \vbox{\png{f1_LP_level1}{300}} & \vbox{\png{f2_LP_level1}{300}} \\
    \vbox{\png{f0_LP_level0}{300}} & \vbox{\png{f1_LP_level0}{300}} & \vbox{\png{f2_LP_level0}{300}} \\
    & \vbox{\svg{f0_1_LP_level1}{300}} & \vbox{\svg{f0_2_LP_level1}{300}} \\
    & \vbox{\svg{f0_1_LP_level0}{300}} & \vbox{\svg{f0_2_LP_level0}{300}}
  \end{tabular}
  \caption{A demonstration of the shift-invariance of the LP. See this
    \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/study_guide/11-MC_in_DWT_domain/LP_shift_invariance.ipynb}{notebook}.}
\label{fig:LP}
\end{figure}

The LP is a frame expansion that generates an expanded (not critical)
octave-band decomposition, and in some way, it can be considered one
of the precursors of the dyadic DWT. Unlike in the DWT, such expansion
is consequence of that the filters used for creating the LP are not
orthogonal and therefore, they do not cancel the aliasing between them
(see this
\href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/study_guide/11-MC_in_DWT_domain/LP_is_not_critical.ipynb}{notebook}) when their downsampled outputs are added.

As a consequence of that downsampling can not be used without
violating the perfect reconstruction, the redundancy in the LP tends
to 2 with the number of levels of the pyramid, which affects
negatively to the compression ratio. An adventage of this is that the
LP is shift-invariant in the high-frequency subband (see the
Fig.~\ref{fig:LP}), and of course, like the DWT, in the low-frequency
subband when the motion is a multiple of 2.

\begin{figure}
  \centering
  \begin{tabular}{ccc}
    \vbox{\png{f0_haar_iH}{300}} & \vbox{\png{f1_haar_iH}{300}} & \vbox{\png{f2_haar_iH}{300}} \\
    & \vbox{\svg{f0_1_haar_iH}{300}} & \vbox{\svg{f0_2_haar_iH}{300}}
  \end{tabular}
  \caption{A demonstration of the near shift-invariance in the [H]
    subband of the PDWT. See this
    \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/study_guide/11-MC_in_DWT_domain/PDWT_shift_invariance.ipynb}{notebook}.}
\label{fig:PDWT}
\end{figure}

\subsection{Pyramid DWT (PDWT)}
We can estimate and componsensate the motion in an alternative
representation of the DWT decomposition inspired in the LT, that we
have called Pyramid DWT. In fact, a PDWT decomposition is a special
case of a LP where the filters are (bi)orthogonal DWT filters (in this
case, we say that the LP is a tight frame and therefore, it can be
downsampled without lossing the perfect reconstruction).

The 1-levels PDWT (that has two levels in its pyramid) of the frame
$X$ is defined by
\begin{equation}
  \{L, [H]\} = \{LL, \text{DWT}^{-1}(0, LH, HL, HH)\} = \{LL, \text{DWT}^{-1}(0, H)\},
  \label{eq:PDWT}
\end{equation}
where
\begin{equation}
  \{LL, LH, HL, HH\} = \text{DWT}(X).
  \label{eq:DWT}
\end{equation}

The $S$ levels CS-LPT$^S$ is computed simply by appliying the
Eq.~\ref{eq:PDWT} to the subband $L$, recursively.

\begin{figure}
  \centering
  \begin{tabular}{cc}
  \vbox{\svg{f0_1_haar_iH_error}{300}} &
  \vbox{\svg{f0_1_haar_LHHLHH_error}{300}}
  \end{tabular}
  \caption{Prediction error between frames 0 and 1, in the PDWT domain
    (left) and the DWT domain (right). See this
    \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/study_guide/11-MC_in_DWT_domain/PDWT_shift_invariance.ipynb}{notebook}. The
    prediction error between frames 0 and 2 is zero. Remember that we transmit $H$, not $[H]$.}
\label{fig:PDWT_error}
\end{figure}

The PDWT is only near shift-invariant as it can be seen in the
Fig.~\ref{fig:PDWT}. However, it has several advantages:
\begin{enumerate}
\item The CS-LPT as compact as the DWT.
\item The phases do not need to be considered, which simplifies the
  ME/MC process and enables the use of any DWT filter.
\item The error generated by the lack of shift-invariance for the
  odd-type motion is smaller than for the DWT (see
  Fig.~\ref{fig:DWT}). As it can be seen in the
  Fig~\ref{fig:PDWT_error}, the energy of the error is the same in
  $[H]$ and $H$, but the energy is concentrated in only
  one critical subband (HL).
\end{enumerate}

\subsection{MC in the PDWT domain}
It's reasonable to expect that the motion of an object between the
frames $R$ and $P$ must move their low and the high frecuencies in the
same amount of pixels. With this idea in mind, we estimate the motion
in the $[H]$ subband using only the information provided by the
low-frequency subband $L$. More concretely, we implement:

\begin{figure}
  \centering
  \begin{tabular}{ccc}
    \vbox{\png{f0_haar_iL}{300}} & \vbox{\png{f1_haar_iL}{300}} & \vbox{\png{f2_haar_iL}{300}} \\
    & \vbox{\svg{f0_1_haar_iL}{300}} & \vbox{\svg{f0_2_haar_iL}{300}}
  \end{tabular}
  \caption{A demonstration of the near shift-invariance in the [L]
    (PDWT) subband. See this
    \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/study_guide/11-MC_in_DWT_domain/iLL_shift_invariance.ipynb}{notebook}.}
\label{fig:iL}
\end{figure}

\begin{enumerate}

\item In order to increase the accuracy of the ME (see the
  Fig.~\ref{fig:iL}, we interpolate the low-frequency subbands of $R$
  and $P$:
  \begin{equation}
    [P.L] = \text{DWT}^{-1}(P.L, 0),
  \end{equation}
  \begin{equation}
    [R.L] = \text{DWT}^{-1}(R.L, 0).
  \end{equation}

\item Estimate the motion between $[P.L]$ and $[R.L]$.
  The output of this step is a motion vectors field
  $\overset{[P.L]\rightarrow [R.L]}{V}$, that describes how to project
  the $[P.L]$ onto $[R.L]$. Notice that $\overset{[P.L]\rightarrow
  [R.L]}{V}$ should also be a good candidate for mapping $P$ onto
  $R$.\footnote{Notice also that the number of vectors in
  $\overset{[P.L]\rightarrow [L.L]}{V}$ can be as high as the number
  of pixels in $R$ (and $P$), although this will depend on the
  accuracy of the ME/MC.}
  
\item Use $\overset{[P.L]\rightarrow [R.L]}{V}$ and $[R.L]$ to
  generate a prediction $[\hat{P}.L]$, and $[R.H]$ to generate a
  prediction $[\hat{P}.H]$. We define the prediction error in the
  low-frequency subband as
  \begin{equation}
    [E.L] = [P.L] - [\hat{P}.L],
    \label{eq:prediction_error_L}
  \end{equation}
  and the prediction error in the high-frequency subband
  as
  \begin{equation}
    [E.H] = [P.H] - [\hat{P}.H].
    \label{eq:prediction_error}
  \end{equation}
  Notice that $\overset{[P.L]\rightarrow [R.L]}{V}$ depends only on
  $R.L$ and $P.L$, not on the high frequency subbands.

\item Compute the Element-Wise (EW) minimum of $[P.L]$ and $[E.L]$:
  \begin{equation}
    \{[T],[M]\} = \text{EW-min}([P.L], [E.L])
    \label{eq:EW-min}
  \end{equation}
  where
  \begin{equation}
    [T]_{i,j}=\text{min}([P.L]_{i,j}, [E.L]_{i,j})
  \end{equation}
  and $[M]$ is a binary matrix defined by
  \begin{equation}
    [M]_{i,j} = \left\{
      \begin{array}{ll}
        0 & \text{if}~[P.L]_{i,j} < [E.L]_{i,j} \\
        1 & \text{otherwise}.
      \end{array}
    \right.
    \label{eq:matrix}
  \end{equation}
  
\item Output
  \begin{equation}
    [O]_{i,j} = \left\{
      \begin{array}{ll}
        [P.H]_{i,j} & \text{if}~[M]_{i,j} = 0~(\text{intra~coefficient})\\
        {[}E.H{]}_{i,j} & \text{otherwise}~(\text{predicted~coefficient}).
      \end{array}
    \right.
    \label{eq:output}
  \end{equation}
  Realize that it must hold that
  \begin{equation}
    \sigma^2_O \le \sigma^2_E,
    \label{eq:vars}
  \end{equation}
  where $\sigma^2$ denotes the variance, $O=\text{DWT}([O])$, and
  $E=\text{DWT}([O])$. Eq.~\ref{eq:vars} implies that
  \begin{equation}
    \text{CR}_O \ge \text{CR}_E
    \label{eq:crs}
  \end{equation}
  should hold, where CR stands for Compression Ratio.

  $O$ is the high-frequency subband that we will send from the
  encoder to the decoder, and it can be seen, it is composed of
  prediction error coefficients and original coefficients. Making a
  comparison with the procedure followed in most video coding
  standards, the prediction error coefficients represent predicted
  blocks (P-type blocks) or skipped blocks (S-type
  blocks)\footnote{S-type blocks are an special case of P-type blocks
  that have a prediction error so small that is more beneficial not to
  send their texture.}, and the original coefficients are equivalent
  to the intra(coded) blocks (I-type blocks).
\end{enumerate}

\section{What you have to do?}

Please, implement Eq.~\ref{eq:output}, preferiblely in a Jupyter
notebook. Use the moving circle generator (see
this \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/study_guide/11-MC_in_DWT_domain/DWT_shift_invariance.ipynb}{notebook})
to create the video. Verify also Eq.~\ref{eq:crs}.

\section{Timming}

In groups, you will present the results for this milestone during the
examination time.

\section{Deliverables}

The notebook(s) and the presentation.

\section{Resources}

\bibliography{image-pyramids,DWT,motion-estimation,HEVC}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}

\subsection{Using (also) the lost phase in IBMC}
There are different alternatives for recovering the ``lost'' phases
(remember that we have two subbands, and two downsamplers) during the
DWT (in the 1D case). The result of this procedure is know as the
Overcomplete DWT (ODWT). Notice, however, that we are not interested
in encoding the ODWT domain, but in encoding the DWT that is more
compact. To achieve this, at least the following techniques can be
used:

\subsection{Performing IBMC in the 1-levels MDWT domain}
Once that the missing phases have been recovered, the MC procedure
between two frames (the reference frame $R$ and the predicted frame
$P$) that we are going to implement is:
\begin{enumerate}
\item  Therefore, estimate the motion between the overcomplete
  low-frequency subband of the reference frame $[R.L]$ and the
  overcomplete low-frequency subband of the predicted frame
  $[P.L]$. The output of this step is a motion vectors field
  $\overrightarrow{V}$, that describes how to project the $[R.L]$ onto
  $[P.L]$. Notice that $\overrightarrow{V}$ should be also a good
  candidate for mapping $R$ onto $P$.
  
\item Use $\overrightarrow{V}$ and $[R.H]$ (notice that in 2D case,
  $[R.H]=\{[R.HL], [R.LH], [R.HH]\}$) to generate a prediction
  $[\hat{P}.L]$. We define the prediction error
  in the overcomplete low-frequency subband as
  \begin{equation}
    [E.L] = [P.L] - [\hat{P}.L],
    \label{eq:prediction_error_L}
  \end{equation}
  and the prediction error in the overcomplete high-frequency subband
  as
  \begin{equation}
    [E.H] = [P.H] - [\hat{P}.H].
    \label{eq:prediction_error}
  \end{equation}

\item Selectively subsample $[E.H]$, picking out the right phase.
\end{enumerate}  


  Perform first the MC stage directly over the
  output of the analysis filters, and then, selectively downsample the
  result. Notice that the downsampler should select the right phase,
  depending on the type of motion detected (``odd'' or ``even''). This
  information (the selected phase), should be available at the
  decoder, along with the motion fields.
\item Delay-then-DWT: Perform two identical DWTs, one to the original
  signal, and the other to a one-sample delayed signal (remember than
  a movement of one pixel will change the phase at the output of the
  DWT). Thus, the the DWT applied to the original signal will generate
  one of the phases and the DWT applied to the delayed signal will
  generate the other one.
\item CODWT: Use the current (single phase) L and H coefficients to
  compute the missing phase, using the CODWT (Complete-to-Overcomplete
  DWT)~\cite{andreopoulos2005complete} (a new type of DWT applied to
  the DWT coefficients).
\end{enumerate}
Each alternative has pros and cons. If the DWT has been implemented
using convolution, MC-then-downsample should be a fast
alternative. However, if the DWT uses Lifting, Multiple-DWT-then-MC
should be fast also, because only one phase is computed by the
DWT. These two options can be used with any DWT filters. On the other
hand, CODWT needs specific designs form each DWT filters. Notice that, in any case, the solution is reached after using the ODWT domain.

---

is because a 1-pixel motion cannot
be represented selecting always the same phase at the downsamplers
(remember that with the downsampler we are basically selecting only
one the two possible phases of the output of the analysis filters: the
even samples or the odd samples, see this notebook).

Lets suppose that
the downsampler discards the odd coefficients (let's refer them as
odd-phase coefficients).

In this case, the even-phase cofficients of
the reference frame are the same than the odd-phase coefficients of
the predicted frame (this can be seen in this notebook). Therefore, in
the 1D case, when the motion is ``even''-type (that is, a displacement
of a even number of samples) we should compensate the even-phase
coefficients of the reference and the predicted frame, while when the
motion is ``odd''-type we should compensate the odd-phase coefficients
of the predicted frame with a prediction generated with the even-phase
coefficients of the reference frame, or viceversa.


\subsection{Near shift-invariance in the IDWT (Interpolated DWT) domain}
As it was commented before, the causant of the shift-variance in the
critically sampled DWT domain is the use of the downsamplers. At this
point we have basically two different alternatives:
\begin{enumerate}
\item Use the Algorithme \`a Trous (AaT)~\cite{mallat1999wavelet},
  which removes the downsamplers from the DWT, generating the so
  called Overcomplete DWT (ODWT). Notice that, because the
  downsamplers are removed, the aliasing artifacts produced by the
  downsamplers is also avoided.
\item Approximate the AaT coefficients by interpolating the DWT
  coefficients using the DWT synthesis filters. In this case, the
  aliasing is not avoided, but the shift-variance problem is
  reduced.
\end{enumerate}

\subsection{Subpixel accuracy}
Objects in real scenes usually move a rational number of pixels, and
therefore, even when the input frames seems to be the same,
numerically they aren't. To deal with this drawback, interpolation can
be used to increase the resolution of the frames (MC in the frame
domain) or the subbands (MC in the subband domain), performing thus a
MC with increased accuracy.

Interpolation and DWT are both linear operators, and therefore, are
interchangeable. This means that we can interpolate the input frames and work as if the motion where integer-pixel, or we can interpolate the DWT coefficients. In both options, the number of 

\end{comment}
