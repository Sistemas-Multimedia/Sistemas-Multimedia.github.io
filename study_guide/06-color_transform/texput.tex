\input{../definitions}
\title{\SM{} - Study Guide - Milestone 6: Removing Intercomponent Redundancy}

\maketitle

\section{Description}

\href{https://en.wikipedia.org/wiki/Data_redundancy}{Redundancy} in
signals is usually expressed as a
\href{https://en.wikipedia.org/wiki/Correlation_and_dependence}{correlation}
or statistical/spatio/temporal dependency between samples. In the case
of a video, a sample is a color component and one source of redundancy
is the correlation between color components.

One way of estimating this redudancy is to compute the
\href{https://en.wikipedia.org/wiki/Entropy_(information_theory)}{0-order
  entropy} of the signal: the higher the entropy, the lower the
redudancy. In fact, if we suppose that the samples of the signal are
uncorrelated, the 0-order entropy is an exact mesarure of the expected
bit-rate achieved by an arithmetic codec.

An alternative way of estimating the (statistical/spatio/temporal)
redundancy is to use a
\href{https://en.wikipedia.org/wiki/Data_compression}{compressor}: the
higher the length of the compressed file, the higher the
redundancy. This estimation is more accurate than the entropy, in
general, but notice that depends on the encoding algorithm (different
algoritms can provide different extimations).

In this milestone we are going to estimate the intercomponent
redundancy of a video frame, i.e., the redundancy generated by the
\href{https://en.wikipedia.org/wiki/Color_space}{color space} that has
been used. The two color space that we are going to research are: (1)
\href{https://en.wikipedia.org/wiki/RGB_color_model}{the RGB color
  space}, and (2) the \href{https://en.wikipedia.org/wiki/YCbCr}{YCbCr
  color space}. To transform a (color) pixel from RGB to the YCbCr
domain (the analysis transform), we will use
\href{https://docs.opencv.org/3.4/de/d25/imgproc_color_conversions.html}{the
  RGB to YCbCr transform} defined as
\begin{equation}
  \begin{array}{lcl}
    \text{Y}  & = & 0.299\text{R} + 0.587\text{G} + 0.114\text{B} \\
    \text{Cr} & = & 0.713(\text{R} - \text{Y}) + \delta  \\
    \text{Cb} & = & 0.564(\text{B} - \text{Y}) + \delta,
  \end{array}
\end{equation}
where
\begin{equation}
  \delta = \left\{
  \begin{array}{ll}
    128 & \text{for 8 bits (unsigned) images},\\
    32768 & \text{for 16 bits (unsigned) images},\\
    0.5 & \text{for floating point (}[0,1]\text{) images}.
  \end{array}
  \right.
\end{equation}

The sythesis (inverse) transform is defined by
\begin{equation}
  \begin{array}{lcl}
    \text{R} & = & \text{Y} + 1.403(\text{Cr} - \delta) \\
    \text{G} & = & \text{Y} - 0.714(\text{Cr} - \delta) - 0.344(\text{Cb} - \delta)\\
    \text{B} & = & \text{Y} + 1.773(\text{Cb} - \delta).
  \end{array}
\end{equation}

It's easy to see that this transform is not orthogonal, because (for
example), in the analysis transform the value of Cr depends on the
value of Y. This dificults the quantization of the YCbCr coefficients
because, the quantization error generated in one of the components
influences in the quantization error of the rest of components.

Then, why we don't use an orthogonal transform? Why virtually all the
proposed video compression estandars use this transform? The reason
(that will be analyzed in a future milestone) is that the HVS is more
sensitive to the Y component (luminance) than to the CbCr components
(crominance), and for this reason, the croma can be more degraded than
the luma without perceiving this.

\section{What you have to do?}
  
Please, modify the
\href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/study_guide/05-quantization/quantize_a_frame.ipynb}{notebook}
in order to use the
\href{https://docs.opencv.org/master/d4/da8/group__imgcodecs.html}{TIFF
  and JPEG 2000 image formats} instead of PNG. Compare the RD curves.

\section{Timming}

Please, finish this notebook before the next class session.

\section{Deliverables}

None.

\section{Resources}

\bibliography{data-compression,signal-processing}
