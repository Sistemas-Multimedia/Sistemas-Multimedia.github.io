\input{../definitions}
\title{\SM{} - Study Guide - Milestone 6: The Color Transform}

\maketitle

\section{Description}

The main objective of this milestone is to remove the intercomponent
redundancy.
\href{https://en.wikipedia.org/wiki/Data_redundancy}{Redundancy} in
signals is usually expressed as a
\href{https://en.wikipedia.org/wiki/Correlation_and_dependence}{correlation}
or statistical/frequency/spatio/temporal dependency between
samples. In the case of a video, a sample is a color component with 3
bands, and one source of redundancy is the correlation between color
components.

One way of estimating this redudancy is to compute the
\href{https://en.wikipedia.org/wiki/Entropy_(information_theory)}{0-order
  entropy} of the signal: the higher the entropy, the lower the
redudancy. In fact, if we suppose that the samples of the signal are
uncorrelated, the 0-order entropy is an exact mesarure of the expected
bit-rate achieved by an arithmetic codec.

Unfortunately, the 0-order entropy is usally a lower bound for the
estimation of the redundancy. A better way is using a
\href{https://en.wikipedia.org/wiki/Data_compression}{compressor}: the
higher the length of the compressed file, the higher the
redundancy. Notice, however, that although this estimation is more
accurate than the entropy, in general, it depends on the encoding
algorithm (different algoritms can provide different extimations).

In this milestone we are going to estimate the intercomponent
redundancy of a video frame, i.e., the redundancy generated by the
\href{https://en.wikipedia.org/wiki/Color_space}{color space} that has
been used. The two color space that we are going to research are: (1)
\href{https://en.wikipedia.org/wiki/RGB_color_model}{the RGB color
  space}, and (2) the \href{https://en.wikipedia.org/wiki/YCbCr}{YCbCr
  color space}. To transform a (color) pixel from RGB to the YCbCr
domain, we will use
\href{https://docs.opencv.org/3.4/de/d25/imgproc_color_conversions.html}{the
  RGB to YCbCr (analysis) transform}, defined as
\begin{equation}
  \begin{array}{lcl}
    \text{Y}  & = & 0.299\text{R} + 0.587\text{G} + 0.114\text{B} \\
    \text{Cr} & = & 0.713(\text{R} - \text{Y}) + \delta  \\
    \text{Cb} & = & 0.564(\text{B} - \text{Y}) + \delta,
  \end{array}
\end{equation}
where
\begin{equation}
  \delta = \left\{
  \begin{array}{ll}
    128 & \text{for 8 bits (unsigned) images},\\
    32768 & \text{for 16 bits (unsigned) images},\\
    0.5 & \text{for floating point (}[0,1]\text{) images}.
  \end{array}
  \right.
\end{equation}

The inverse (sythesis) transform is defined by
\begin{equation}
  \begin{array}{lcl}
    \text{R} & = & \text{Y} + 1.403(\text{Cr} - \delta) \\
    \text{G} & = & \text{Y} - 0.714(\text{Cr} - \delta) - 0.344(\text{Cb} - \delta)\\
    \text{B} & = & \text{Y} + 1.773(\text{Cb} - \delta).
  \end{array}
\end{equation}

It's easy to see that this transform is not orthogonal, because (for
example), in the analysis transform the value of Cr depends on the
value of Y. This dificults the quantization of the YCbCr coefficients
because, the quantization error generated in one of the components
influences in the quantization error of the rest of components, and
when this happens, we cannot use the constant slope rate distortion
minimization
strategy~\cite{vetterli1995wavelets,sayood2017introduction} for
selecting $\Delta_{\text{Y}}$, $\Delta_{\text{Cb}}$, and
$\Delta_{\text{Cr}}$.

\begin{comment}
The International Consultative Committee for Radio (CCIR)11 Recommendation 601:
\begin{equation}
\begin{array}
Y ′= 219(+0.299R′ + 0.587G′ + 0.114B′) + 16 \\
CB′= 224(-0.169R′ - 0.331G′ + 0.500B′) + 128 \\
CR′= 224(+0.500R′ - 0.419G′ - 0.081B′) + 128
\end{array}
\end{equation}

0.299*(−0.169) + 0.587*(−0.331) + 0.114*0.500 = -.187828

(-0.169)*0.500 + (-0.331)*(-0.419) + 0.500*(-0.081) = .013689
\end{comment}

Then, why we don't use an orthogonal transform? Why virtually all the
proposed video compression estandars use this transform (or a similar
alternative, but never orthogonal)? The reason (that will be analyzed
in a future milestone) is that the
\href{https://en.wikipedia.org/wiki/Visual_system}{HVS (Human Visual
  System)} is more sensitive to the Y component (luminance) than to
the CbCr components (crominance), and for this reason, the
\emph{croma} can be more degraded than the \emph{luma} without
perceiving this, achiving higher compression ratios.

Anyway, as you can see in this notebook, the use of the YCbCr color
domain can be beneficial, even using simple quantization strategies
(such as $\Delta_\text{Y} = \Delta_\text{Cb} = \Delta_\text{Cr}$), the
RD (Rate/Distortion) curves can be improved for most bit-rates, and
therefore, it can be an interesting tool for removing the
intercomponent redundancy.

\section{What you have to do?}

Please, run the previous \href{}{notebook}

Please, modify the
\href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/study_guide/05-quantization/quantize_a_frame.ipynb}{notebook}
in order to use the
\href{https://docs.opencv.org/master/d4/da8/group__imgcodecs.html}{TIFF
  and JPEG 2000 image formats} instead of PNG. Compare the RD curves.

\section{Timming}

Please, finish this notebook before the next class session.

\section{Deliverables}

None.

\section{Resources}

\bibliography{data-compression,signal-processing,DWT}
