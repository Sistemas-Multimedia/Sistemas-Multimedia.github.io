\input{../definitions}
\title{\SM{} - Study Guide - Milestone 10: Motion Estimation}

\maketitle

\section{Description}

\subsection{Motion Estimation (ME) for what?}
Temporal correlation between video frames can be removed by MC (Motion
Compensation). MC implies to substract to the frames those
information, in form of a prediction frame $\hat{P}$, that can be
infered from neighbor frames, as long as these frames are known by the
decoder or the motion information is transmited from the encoder to
the decoder.

A MC Predictor (MCP) inputs one or more reference frames $R$, a
predicted frame $P$ (the frame that we want to compensate), and a
motion vectors field $\overset{\cdot\rightarrow\cdot}{V}$. In the next
milestone we will see how to find
\begin{equation}
  \hat{P} =  \overset{P\rightarrow R}{V}(R)
\end{equation}
and how to use it. In this milestone we show how to compute
$\overset{P\rightarrow R}{V}$.

\subsection{What exactly we need?}
Our main objective is to minimize the differences (for example, the
\href{https://en.wikipedia.org/wiki/Euclidean_distance}{L$_2$
  distance}) between $P$ and $\hat{P}$, because
\begin{equation}
  E=P-\hat{P}
\end{equation}
will be more compressible. To achieve this, we can compute
$\overset{P\rightarrow R}{V}$ that simply minimizes the L$_2$ energy
of $E$, $|E|^2$, or we can compute a $\overset{P\rightarrow R}{V}$
that also describes the Optical Flow~\cite{horn1981determining} (OF)
between the pixels of $R$ and $P$, that although not necessary has to
minimize $|E|^2$, tries to show the true movement of the pixels
between $R$ and $P$. This second option has the advantage of generating
more visually pleasing predictions.

The first type of techniques are simply called ``ME techniques'', and
are usually faster than the based on the computation of the OF.

%Let's see two basic techniques to estimate the motion between 2
%frames, $R$ and $P$. In this discussion it will be supposed that the
%motion of the objects that are in both frames is bounded, and that the
%luminance varies smoothly between adjacent frames.

Let's see some of the most used techniques for estimating the motion
between two frames. Notice that, in general, better estimations can be
found if more than only two frames are used because the intertia of
the objects (with mass). However, this case will not be considered for
now.

\subsection{\href{https://vicente-gonzalez-ruiz.github.io/video_compression/\#x1-40003}{Disjoint block matching}}
$P$ can be divided, for example, in blocks of 16x16 pixels, and we can
use the MSE (that is considered a L$_2$ distance) between each block
of $P$ and its surrounding pixels in $R$ (the so called search
area)~\cite{zhu2000new}. For each block a motion vector that indicates
the best match (smaller distance) is found. The set of motion vectors
form $\overset{P\rightarrow R}{V}$ that obviously, except for a block
size of 1x1, will be less dense than $R$ and $P$. Notice, however,
that, it is not a good idea to use such a small block size because, in
general, the motion vectors will not describe the OF.

\subsection{Overlaped block matching}
A better approximation to the OF can be found if we allow
the blocks to overlap in $P$~\cite{orchard1994overlapped}, and the
overlaped pixels between blocks are averaged for building
$\hat{P}$. The main drawback of this technique is that it more
computationally demanding than the previous one.

\subsection{Transform analysis}
One of the most successful techniques for computing the (dense)
OF is based on the analysis of the coefficients resulting
from transforming the frames using a polynomial expansion (the details
of this transform and the impact of the motion in the coefficients can
be found in the paper or Gunnar
Farneb{\"a}ck~\cite{farneback2003two}). This is the algorithm that we
will use in our experiments for two important reasons: (1) it is quite
efficient in terms both of performance and speed, and (2) it is
already
\href{https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html}{implemented}
in OpenCV.

\subsection{Machine learning}
ANNs (Artifical Neural Networks) can be trained to estimate the
motion between frames~\cite{dosovitskiy2015flownet}.

\subsection{Motion compensation in action}
\begin{figure}
  \svg{R}{1000}
  \svg{P}{1000}
  \svg{y_motion}{1000}
  \svg{x_motion}{1000}
  \svg{hat_P}{1000}
  \svg{without_ME}{1000}
  \svg{with_ME}{1000}
  \caption{Effect of ME over the temporal redundancy removal.}
  \label{fig:MC}
\end{figure}
The Fig.~\ref{fig:MC} shows an example the performace of the use of
OF, comparing the prediction error generated with and without ME. A
motion vector has been computed between each point of $P$ and $R$. As
it can be seen, ME reduces the temporal redundancy significantly.

\begin{comment}
The OF~\cite{horn1981determining} tries to establish connections between the pixels of
the frames $P$ and $R$ supposing that:
\begin{enumerate}
\item $P$ and $R$ are adjacent in time (if $R$ was taken at time $t$,
  $P$ is taken at time $dt+t$) and therefore, similar in
  content.
\item Similarity between images implies that the pixels in both
  frames, $R$ and $P$, will have the same luminance. If $I(x,y,t)$
  measures the luminance of the pixel $(x,y)$ of the frame $R$,
  similarity can be modeled by
  \begin{equation}
    I(x+dx, y+dy, t+dt) = I(x,y,t),
    \label{eq:similarity}
  \end{equation}
  where $I(x+dx, y+dy, t+dt)$ is the corresponding pixel in the frame
  $P$. The first part of the Eq.~\ref{eq:similarity} can be also
  computed by (using the first-order Taylor expansion) as
  \begin{equation}
    I(x+dx, y+dy, t+dt) = I(x,y,t) + \frac{\partial I}{\partial x}dx + \frac{\partial I}{\partial y}dy + \frac{\partial I}{\partial t}dt,
    \label{eq:taylor_exp}
  \end{equation}
  andtherefore, it must be true that
  \begin{equation}
    \frac{\partial I}{\partial x}dx + \frac{\partial I}{\partial y}dy + \frac{\partial I}{\partial t}dt = 0.
    \label{eq:constraint}
  \end{equation}
  Dividing by $dt$, we finally get that
  \begin{equation}
    \frac{\partial I}{\partial x}\frac{dx}{dt} + \frac{\partial I}{\partial y}\frac{dy}{dt} + \frac{\partial I}{\partial t} = 0.
  \end{equation}
\item Adjacent pixels follow parallel
  trajectories~\cite{horn1981determining}, with basically means that
  neighbor pixels will have similar motion.
\end{enumerate}
\end{comment}

\section{What you have to do?}

In this
\href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/study_guide/10-ME/optical_flow.ipynb}{notebook}
you can find how to estimate the OF between two frames. Please, modify
it to find suitable values for the parameters \texttt{levels},
\texttt{winsize} and \texttt{iterations}. Supposing that the impact of
each parameter is independent from the rest, the best way of comparing
two different configurations is render RD curves using quantization
and compressing the residues.

\section{Timming}

Please, finish this milestone before the next class session.

\section{Deliverables}

None.

\section{Resources}

\bibliography{motion-estimation}
