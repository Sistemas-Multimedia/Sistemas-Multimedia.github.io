\input{../definitions}
\title{\SM{} - Study Guide - Milestone 10: Motion Compensation in the DWT Domain}

\maketitle

\section{Description}

In this milestone we will discuss the characteristics of the motion
estimation and compensation of frames (considering only one component)
in the DWT domain.

\subsection{Removing the temporal redundancy through Motion Compensation (MC)}
The next natural step in the process of decorrelating the sequence of
frames is to remove the temporal redundancy by means of Motion
Compensation (MC). Basically, MC consists in substracting to the video
data a prediction performed with the information that is avaliable to
the decoder. If this prediction is accurate, the result of this
operation is a residual video with a lower temporal redundancy, that
can be compressed with a higher compression ratio because there is
less information to encode in the residue sequence than in the
original one.

\subsection{Integer pixel accuracy In-Band Motion Estimation and Compensation}
At this stage of the encoding process, the video data is represented
in the DWT domain, and therefore, we need to perform a In-Band Motion
estimation and Compensation
(IBMC)~\cite{andreopoulos2005complete}). Let's suppose that the number
of levels of the DWT is 1, and therefore, each frame has been
decomposed into two 2D subbands $L$ and $H$ (remember that using the
notation introduces in the previous milestone, $H$ has inside the
three high-frequency subbands: $LH$, $HL$ and $HH$, and that
$L=LL$). This discussion will be also constrained to the case in which
the movement of the objects in the scene is a integer number of
pixels.

\subsection{The lack of shift-invariance in the DWT domain}
Unfortunately, DWT decompositions are shift-variant as a consequence
of the downsampling performed during the DWT to achieve a critical
representation. This can be seen in the
Fig.~\ref{fig:dwt_shift_variance} were some DWT coefficients of a test
video with three test frames has been shown. As it can be seen, when
the circle moves to the left only one pixel (as happens in the frames
0 and 1), the value of the coefficients that correspond to the
circunference of the circle are different between the reference frame
(0) and the predicted frame (1). This makes quite difficult to
estimate and compensate the motion between frames. Notice also that
the effects of shift-variance is also visible after using the inverse
transform when the coefficients are filtered or quantized, because the
aliasing between the filters is not completely cancelled in this
case~\cite{bradley2003shift}.

\begin{figure}
  \centering
  \begin{tabular}{ccc}
    \vbox{\png{frame_0_Y}{300}} & \vbox{\png{frame_1_Y}{300}} & \vbox{\png{frame_2_Y}{300}} \\
    & \vbox{\svg{movement_0}{300}} & \vbox{\svg{movement_1}{300}} \\
    & \vbox{\svg{haar_LL_0}{300}} & \vbox{\svg{haar_LL_1}{300}} \\
    & \vbox{\svg{haar_LH_0}{300}} & \vbox{\svg{haar_LH_1}{300}} \\
    & \vbox{\svg{haar_HL_0}{300}} & \vbox{\svg{haar_HL_1}{300}} \\
    & \vbox{\svg{haar_HH_0}{300}} & \vbox{\svg{haar_HH_1}{300}}
  \end{tabular}
  \caption{A demonstration of the shift-variance of the DWT. Similar
    results have been obtained for other filters. See this
    \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/study_guide/10-MC_in_DWT_domain/DWT_shift_variance.ipynb}{notebook}.}
\label{fig:dwt_shift_variance}
\end{figure}

However, suprisingly, at it can be also seen in the
Fig.~\ref{fig:dwt_shift_variance}, when the circle has traveled two
pixels (frames 0 and 2), a perfect match is achieved! The reason why
the 1-pixel motion generates different coefficients in the
reference and the predicted frames, and the same coefficients for a
2-pixel motion is because, in the first case the right coefficients
were discarded by the downsamplers, and in the second case
not.

Usually, we call phases to the two possible coefficients resulting
from one (1D) filter to be subsampled, being the even phase, the even
coefficients, and the odd phase, the odd coefficients. Therefore, when
the motion is of type ``even'' (when we have a $2N$-pixels motion), we
should use the even phase to compensate the frames, and viceversa (use
the odd phase to compensate a $2N+1$-pixels motion). Notice that in
the 2D case, and always working with only one level of the DWT, we
have up to four different phases: (even, even)-, (even, odd)-, (odd,
even)-, and (odd, odd)-phase coefficients. Thus, depending on the type
of motion detected, the corresponding phase should be selected.

\subsection{Recovering the lost phases}
There are different alternatives for regenerating the phases discarded
by the subsamplers of the DWT. This is equivalent to compute the
Overcomplete DWT (ODWT)~\cite{mallat1999wavelet}.
\begin{enumerate}
\item Use the Algorithme \`a Trous~\cite{mallat1999wavelet}, which
  basically consists in removing the downsamplers, avoiding thus the
  aliasing artifacts generated by the noncompliance with the sampling
  theorem.
\item Considering the previous experiments, it's easy to see that if
  we shift the signal one sample and perform the DWT, we get the
  ``lost'' phase. This method has been used to perform efficient MC in
  the DWT domain~\cite{park2000motion,li2001all}.
\item Apply some transform (such as for example, the CODWT
  (Complete-to-Overcomplete DWT)~\cite{andreopoulos2005complete}) to
  the DWT to reconstruct the ODWT.
\end{enumerate}

\subsection{About using the lost phases in IBMC}
Up to date, all the video codecs that use critically sampled IBMC also
use
\href{https://vicente-gonzalez-ruiz.github.io/video_compression/}{block-based
  motion compensation}. This technique divides the frames into
non-overlaping blocks and computes a motion vector for every block,
that provides a projection (a prediction) $\hat{P}$ of the reference
frame $R$ that must be as close as possible to the predicted frame
$P$. These blocks usually have a size of 16x16 pixel.

The use of blocks imples that:
\begin{enumerate}
\item If $N$ is the number of pixels in a frame, $N/256$ is the number
  of motion vectors. Therefore, if the motion vectors field has to be
  sent to the decoder, the data overhead is small (although this
  depends on the length of the representation of the texture).
\item All the coefficients that correspond to the same block has the
  same phase. Thus, if the phase also has to be sent to the decoder,
  again, the data overhead can be considered small.
\end{enumerate}

Unfortunately, there is a problem with mixing the phases. To
reconstruct the border pixels of the blocks, the adjacent (same phase)
coefficients must be also used by the decoder (see this
\href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/study_guide/10-MC_in_DWT_domain/mixing_phases.ipynb}{notebook}). For
this reason, the size of the blocks affects to the compression ratio
(the smaller the blocks, the higher the number of adajacent
coefficients, and therefore, the lower the compression ratio). We can
think about that this effect can be mitigated using larger block
sizes, but this will also affect to the compression ratio because the
quality of the predictions worsen with the increment of the size of
the blocks. This supposes an optimization problem that it's hard to
solve, especially in real-time applications.

\subsection{Critically Sampled (CS) Laplacian Pyramid Transform (LPT)}
Alternatively, we can estimate and componsensate the motion in an
alternative representation of the DWT decomposition that we have
called CS-LPT. The Laplacian Pyramid was proposed by Burt and
Adelson~\cite{burt1987laplacian} and has been used for the design of
spatially-scalable image and video codecs, such as
SHVC~\cite{sullivan2012overview}.

The LP is considered a frame expansion that generates an expanded (not
critical) octave-band decomposition. Unlike in the DWT, such expansion
is consequence of that the filters used for creating the LP levels
does not cancel the aliasing between them (see this
\href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/study_guide/10-MC_in_DWT_domain/critical_LP.ipynb}{notebook}). On the other hand,
the redundancy in the LP tends to 2 with the number of levels, which
affects negatively to the compression ratio.

CS-LPT is a special case of the LP where the filters are DWT
filters. The 1-levels CS-LPT (that has two levels in its pyramid) is
of the frame $X$ defined by
\begin{equation}
  \{L, [H]\} = \{LL, \text{DWT}^{-1}(0, LH, HL, HH)\} = \{LL, \text{DWT}^{-1}(0, H)\},
  \label{eq:CS-LPT}
\end{equation}
where
\begin{equation}
  \{LL, LH, HL, HH\} = \text{DWT}(X).
  \label{eq:DWT}
\end{equation}

The $S$ levels CS-LPT$^S$ is computed simply by appliying the
Eq.~\ref{eq:CS-LPT} to the subband $L$, recursively.

\subsection{MC in the CS-LPT domain}
It's reasonable to expect that the motion of an object between the
frames $R$ and $P$ must move their low and the high frecuencies in the
same amount of pixels. With this idea in mind, we estimate the
motion in the $[H]$ subband using only the information provided by
the low-frequency subband $L$. More concretely, we implement:
\begin{enumerate}
  
\item Estimate the motion between the interpolated version of $R.L$
  \begin{equation}
    [R.L] = \text{DWT}^{-1}(R.L, 0)
  \end{equation}
  and
  \begin{equation}
    [P.L] = \text{DWT}^{-1}(P.L, 0).
  \end{equation}
  The output of this step is a motion vectors field
  $\overset{[R.L]\rightarrow [P.L]}{V}$, that describes how to project
  the $[R.L]$ onto $[P.L]$. Notice that
  $\overset{[R.L]\rightarrow [P.L]}{V}$ should also be a good
  candidate for mapping $R$ onto $P$. Notice also that the number of
  vectors in $\overset{[R.L]\rightarrow [P.L]}{V}$ can as high as
  the number of pixels in $R$ (and $P$).
  
\item Use $\overset{[R.L]\rightarrow [P.L]}{V}$ and $[R.L]$ to
  generate a prediction $[\hat{P}.L]$, and $[R.H]$ to generate a
  prediction $[\hat{P}.H]$. We define the prediction error in the
  low-frequency subband as
  \begin{equation}
    [E.L] = [P.L] - [\hat{P}.L],
    \label{eq:prediction_error_L}
  \end{equation}
  and the prediction error in the high-frequency subband
  as
  \begin{equation}
    [E.H] = [P.H] - [\hat{P}.H].
    \label{eq:prediction_error}
  \end{equation}
  Notice that $\overset{[R.L]\rightarrow [P.L]}{V}$ depends only on
  $R.L$ and $P.L$, not on the high frequency subbands.

\item Compute the Element-Wise (EW) minimum of $[R.L]$ and $[E.L]$:
  \begin{equation}
    \{T,M\} = \text{EW-min}([P.L], [E.L])
    \label{eq:EW-min}
  \end{equation}
  where
  \begin{equation}
    T_{i,j}=\text{min}([P.L]_{i,j}, [E.L]_{i,j})
  \end{equation}
  and $M$ is a binary matrix defined by
  \begin{equation}
    M_{i,j} = \left\{
      \begin{array}{ll}
        0 & \text{if} [P.L]_{i,j} < [E.L]_{i,j} \\
        1 & \text{otherwise}.
      \end{array}
    \right.
    \label{eq:matrix}
  \end{equation}
\item Output
  \begin{equation}
    O_{i,j} = \left\{
      \begin{array}{ll}
        [P.L]_{i,j} & \text{if} M_{i,j} = 0 \\
        {[}E.L{]}_{i,j} & \text{otherwise}.
        %(E.L) & \text{otherwise}.
      \end{array}
    \right.
    \label{eq:output}
  \end{equation}
  Notice that it must hold that
  \begin{equation}
    \sigma^2_O \le \sigma^2_E,
    \label{eq:vars}
  \end{equation}
  where $\sigma^2$ denotes the variance. Eq.~\ref{eq:vars} implies that
  \begin{equation}
    \text{CR}_O \ge \text{CR}_E
    \label{eq:crs}
  \end{equation}
  should hold, where CR stands for Compression Ratio.

\end{enumerate}

\section{What you have to do?}

Please, implement Eq.~\ref{eq:output}, preferiblely in a Jupyter
notebook. Verify also Eq.~\ref{eq:crs}.

\section{Timming}

In groups, you will present the results for this milestone during the
examination time.

\section{Deliverables}

None.

\section{Resources}

\bibliography{image-pyramids,DWT,motion-estimation,HEVC}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}

\subsection{Using (also) the lost phase in IBMC}
There are different alternatives for recovering the ``lost'' phases
(remember that we have two subbands, and two downsamplers) during the
DWT (in the 1D case). The result of this procedure is know as the
Overcomplete DWT (ODWT). Notice, however, that we are not interested
in encoding the ODWT domain, but in encoding the DWT that is more
compact. To achieve this, at least the following techniques can be
used:

\subsection{Performing IBMC in the 1-levels MDWT domain}
Once that the missing phases have been recovered, the MC procedure
between two frames (the reference frame $R$ and the predicted frame
$P$) that we are going to implement is:
\begin{enumerate}
\item  Therefore, estimate the motion between the overcomplete
  low-frequency subband of the reference frame $[R.L]$ and the
  overcomplete low-frequency subband of the predicted frame
  $[P.L]$. The output of this step is a motion vectors field
  $\overrightarrow{V}$, that describes how to project the $[R.L]$ onto
  $[P.L]$. Notice that $\overrightarrow{V}$ should be also a good
  candidate for mapping $R$ onto $P$.
  
\item Use $\overrightarrow{V}$ and $[R.H]$ (notice that in 2D case,
  $[R.H]=\{[R.HL], [R.LH], [R.HH]\}$) to generate a prediction
  $[\hat{P}.L]$. We define the prediction error
  in the overcomplete low-frequency subband as
  \begin{equation}
    [E.L] = [P.L] - [\hat{P}.L],
    \label{eq:prediction_error_L}
  \end{equation}
  and the prediction error in the overcomplete high-frequency subband
  as
  \begin{equation}
    [E.H] = [P.H] - [\hat{P}.H].
    \label{eq:prediction_error}
  \end{equation}

\item Selectively subsample $[E.H]$, picking out the right phase.
\end{enumerate}  


  Perform first the MC stage directly over the
  output of the analysis filters, and then, selectively downsample the
  result. Notice that the downsampler should select the right phase,
  depending on the type of motion detected (``odd'' or ``even''). This
  information (the selected phase), should be available at the
  decoder, along with the motion fields.
\item Delay-then-DWT: Perform two identical DWTs, one to the original
  signal, and the other to a one-sample delayed signal (remember than
  a movement of one pixel will change the phase at the output of the
  DWT). Thus, the the DWT applied to the original signal will generate
  one of the phases and the DWT applied to the delayed signal will
  generate the other one.
\item CODWT: Use the current (single phase) L and H coefficients to
  compute the missing phase, using the CODWT (Complete-to-Overcomplete
  DWT)~\cite{andreopoulos2005complete} (a new type of DWT applied to
  the DWT coefficients).
\end{enumerate}
Each alternative has pros and cons. If the DWT has been implemented
using convolution, MC-then-downsample should be a fast
alternative. However, if the DWT uses Lifting, Multiple-DWT-then-MC
should be fast also, because only one phase is computed by the
DWT. These two options can be used with any DWT filters. On the other
hand, CODWT needs specific designs form each DWT filters. Notice that, in any case, the solution is reached after using the ODWT domain.

---

is because a 1-pixel motion cannot
be represented selecting always the same phase at the downsamplers
(remember that with the downsampler we are basically selecting only
one the two possible phases of the output of the analysis filters: the
even samples or the odd samples, see this notebook).

Lets suppose that
the downsampler discards the odd coefficients (let's refer them as
odd-phase coefficients).

In this case, the even-phase cofficients of
the reference frame are the same than the odd-phase coefficients of
the predicted frame (this can be seen in this notebook). Therefore, in
the 1D case, when the motion is ``even''-type (that is, a displacement
of a even number of samples) we should compensate the even-phase
coefficients of the reference and the predicted frame, while when the
motion is ``odd''-type we should compensate the odd-phase coefficients
of the predicted frame with a prediction generated with the even-phase
coefficients of the reference frame, or viceversa.


\subsection{Near shift-invariance in the IDWT (Interpolated DWT) domain}
As it was commented before, the causant of the shift-variance in the
critically sampled DWT domain is the use of the downsamplers. At this
point we have basically two different alternatives:
\begin{enumerate}
\item Use the Algorithme \`a Trous (AaT)~\cite{mallat1999wavelet},
  which removes the downsamplers from the DWT, generating the so
  called Overcomplete DWT (ODWT). Notice that, because the
  downsamplers are removed, the aliasing artifacts produced by the
  downsamplers is also avoided.
\item Approximate the AaT coefficients by interpolating the DWT
  coefficients using the DWT synthesis filters. In this case, the
  aliasing is not avoided, but the shift-variance problem is
  reduced.
\end{enumerate}

\subsection{Subpixel accuracy}
Objects in real scenes usually move a rational number of pixels, and
therefore, even when the input frames seems to be the same,
numerically they aren't. To deal with this drawback, interpolation can
be used to increase the resolution of the frames (MC in the frame
domain) or the subbands (MC in the subband domain), performing thus a
MC with increased accuracy.

Interpolation and DWT are both linear operators, and therefore, are
interchangeable. This means that we can interpolate the input frames and work as if the motion where integer-pixel, or we can interpolate the DWT coefficients. In both options, the number of 

\end{comment}
