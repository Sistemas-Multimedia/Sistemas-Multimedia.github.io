<?xml version="1.0" encoding="iso-8859-1" ?> 
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" 
"http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd" > 
<html xmlns="http://www.w3.org/1999/xhtml"  
> 
<head><title>Sistemas Multimedia - Study Guide - Milestone 10: Motion Estimation</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" /> 
<meta name="generator" content="TeX4ht (https://tug.org/tex4ht/)" /> 
<meta name="originator" content="TeX4ht (https://tug.org/tex4ht/)" /> 
<!-- xhtml,mathml,html --> 
<meta name="src" content="index.tex" /> 
<link rel="stylesheet" type="text/css" href="index.css" /> 
</head><body 
>
   <div class="maketitle">
                                                                  

                                                                  
                                                                  

                                                                  

<h2 class="titleHead"><a 
href="https://sistemas-multimedia.github.io/" >Sistemas Multimedia</a> - Study Guide - Milestone
10: Motion Estimation</h2>
 <div class="author" ><span 
class="ecrm-1200">Vicente Gonz</span><span 
class="ecrm-1200">&#x00E1;</span><span 
class="ecrm-1200">lez Ruiz</span></div><br />
<div class="date" ><span 
class="ecrm-1200">March 7, 2021</span></div>
   </div>
   <h3 class="likesectionHead"><a 
 id="x1-1000"></a>Contents</h3>
   <div class="tableofcontents">
   <span class="sectionToc" >1 <a 
href="#x1-20001" id="QQ2-1-2">Description</a></span>
<br />   &#x00A0;<span class="subsectionToc" >1.1 <a 
href="#x1-30001.1" id="QQ2-1-3">Motion Estimation &#x0028;ME&#x0029; for what?</a></span>
<br />   &#x00A0;<span class="subsectionToc" >1.2 <a 
href="#x1-40001.2" id="QQ2-1-4">What exactly we need?</a></span>
<br />   &#x00A0;<span class="subsectionToc" >1.3 <a 
href="#x1-50001.3" id="QQ2-1-5">Disjoint block matching</a></span>
<br />   &#x00A0;<span class="subsectionToc" >1.4 <a 
href="#x1-60001.4" id="QQ2-1-6">Overlaped block matching</a></span>
<br />   &#x00A0;<span class="subsectionToc" >1.5 <a 
href="#x1-70001.5" id="QQ2-1-7">Transform analysis</a></span>
<br />   &#x00A0;<span class="subsectionToc" >1.6 <a 
href="#x1-80001.6" id="QQ2-1-8">Machine learning</a></span>
<br />   &#x00A0;<span class="subsectionToc" >1.7 <a 
href="#x1-90001.7" id="QQ2-1-9">Motion compensation in action</a></span>
<br />   <span class="sectionToc" >2 <a 
href="#x1-100002" id="QQ2-1-10">What you have to do?</a></span>
<br />   <span class="sectionToc" >3 <a 
href="#x1-110003" id="QQ2-1-11">Timming</a></span>
<br />   <span class="sectionToc" >4 <a 
href="#x1-120004" id="QQ2-1-12">Deliverables</a></span>
<br />   <span class="sectionToc" >5 <a 
href="#x1-130005" id="QQ2-1-13">Resources</a></span>
   </div>
<!--l. 8--><p class="noindent" >
</p>
   <h3 class="sectionHead"><span class="titlemark">1   </span> <a 
 id="x1-20001"></a>Description</h3>
                                                                  

                                                                  
<!--l. 10--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">1.1   </span> <a 
 id="x1-30001.1"></a>Motion Estimation &#x0028;ME&#x0029; for what?</h4>
<!--l. 11--><p class="noindent" >Temporal  correlation  between  video
frames<span class="footnote-mark"><a 
href="index2.html#fn1x0"><sup class="textsuperscript">1</sup></a></span><a 
 id="x1-3001f1"></a> 
can be removed by MC &#x0028;Motion Compensation&#x0029;. MC implies to decrease &#x0028;usually by
substracting&#x0029; an amount of information in the frames. The removed information must
be available at both, the encoder and the decoder side, in order to make this a
reversible process.
</p><!--l. 19--><p class="indent" >   Speci&#xFB01;cally, a MCP &#x0028;MC Predictor&#x0029; inputs one &#x0028;or more&#x0029; reference frame&#x0028;s&#x0029;
<!--l. 20--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mi 
>R</mi> <mo 
class="MathClass-rel">=</mo> <mo 
class="MathClass-open">&#x007B;</mo><msub><mrow 
><mi 
>R</mi></mrow><mrow 
><mi 
>k</mi></mrow></msub 
><mo 
class="MathClass-close">&#x007D;</mo></math>, and a motion
vectors &#xFB01;eld <!--l. 21--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mover class="overset"><mrow 
><mi 
>V</mi> </mrow><mrow 
><mo 
class="MathClass-bin">&#x22C5;</mo><mo 
class="MathClass-rel">&#x2192;</mo><mo 
class="MathClass-bin">&#x22C5;</mo></mrow></mover></math>,
and outputs a prediction frame </p><table class="equation"><tr><td> <a 
 id="x1-3002r1"></a>
<!--l. 22--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="block" class="equation">
                        <mover 
accent="true"><mrow 
><msub><mrow 
><mi 
>P</mi></mrow><mrow 
><mi 
>k</mi></mrow></msub 
></mrow><mo accent="true">&#x005E;</mo></mover> <mo 
class="MathClass-rel">=</mo> <mover class="overset"><mrow 
><mi 
>V</mi> </mrow><mrow 
><msub><mrow 
><mi 
>P</mi></mrow><mrow 
><mi 
>k</mi></mrow></msub 
> <mo 
class="MathClass-rel">&#x2192;</mo> <mi 
>R</mi></mrow></mover><mo 
class="MathClass-open">&#x0028;</mo><mi 
>R</mi><mo 
class="MathClass-close">&#x0029;</mo><mo 
class="MathClass-punc">,</mo>
</math></td><td class="eq-no">&#x0028;1&#x0029;</td></tr></table>
<!--l. 26--><p class="indent" >   where <!--l. 26--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><msub><mrow 
><mi 
>P</mi></mrow><mrow 
><mi 
>k</mi></mrow></msub 
></math>
is the predicted frame. In this milestone we show how to compute
<!--l. 27--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mover class="overset"><mrow 
><mi 
>V</mi> </mrow><mrow 
><msub><mrow 
><mi 
>P</mi></mrow><mrow 
><mi 
>k</mi></mrow></msub 
> <mo 
class="MathClass-rel">&#x2192;</mo> <mi 
>R</mi></mrow></mover></math>.
In the futher milestones we will see how to remove the
<!--l. 28--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mover 
accent="true"><mrow 
><mi 
>P</mi></mrow><mo accent="true">&#x005E;</mo></mover></math>&#x2019;s information from
the predicted frame <!--l. 29--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mi 
>P</mi></math>.
At this moment, for the sake of simplicity, in the rest of this discussion it will be
suppose that the number of reference frames in only 1 &#x0028;probably, the closest one to
the predicted frame&#x0029;, and therefore, we don&#x2019;t need to specify which are the reference
images to make the prediction. For this reason, Eq,&#x00A0;<a 
href="#x1-3002r1">1<!--tex4ht:ref: eq:MCP1 --></a> can be rewritten as </p><table class="equation"><tr><td>
<a 
 id="x1-3003r2"></a>
                                                                  

                                                                  
<!--l. 35--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="block" class="equation">
               <mover 
accent="true"><mrow 
><msub><mrow 
><mi 
>P</mi></mrow><mrow 
><mi 
>k</mi></mrow></msub 
></mrow><mo accent="true">&#x005E;</mo></mover> <mo 
class="MathClass-rel">=</mo> <mover class="overset"><mrow 
><mi 
>V</mi> </mrow><mrow 
><msub><mrow 
><mi 
>P</mi></mrow><mrow 
><mi 
>k</mi></mrow></msub 
> <mo 
class="MathClass-rel">&#x2192;</mo> <msub><mrow 
><mi 
>P</mi></mrow><mrow 
><mi 
>k</mi><mo 
class="MathClass-bin">&#x2212;</mo><mn>1</mn></mrow></msub 
></mrow></mover><mo 
class="MathClass-open">&#x0028;</mo><msub><mrow 
><mi 
>P</mi></mrow><mrow 
><mi 
>k</mi><mo 
class="MathClass-bin">&#x2212;</mo><mn>1</mn></mrow></msub 
><mo 
class="MathClass-close">&#x0029;</mo> <mo 
class="MathClass-rel">=</mo> <mover class="overset"><mrow 
><mi 
>V</mi> </mrow><mrow 
><mi 
>k</mi> <mo 
class="MathClass-rel">&#x2192;</mo> <mi 
>k</mi> <mo 
class="MathClass-bin">&#x2212;</mo> <mn>1</mn></mrow></mover><mo 
class="MathClass-punc">,</mo>
</math></td><td class="eq-no">&#x0028;2&#x0029;</td></tr></table>
   <h4 class="subsectionHead"><span class="titlemark">1.2   </span> <a 
 id="x1-40001.2"></a>What exactly we need?</h4>
<!--l. 41--><p class="noindent" >Our main objective is to minimize the di&#xFB00;erences &#x0028;for example, the
<a 
href="https://en.wikipedia.org/wiki/Euclidean_distance" >L<!--l. 43--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><msub><mrow 
></mrow><mrow 
><mn>2</mn></mrow></msub 
></math> distance</a>&#x0029; between
<!--l. 43--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mi 
>P</mi></math> &#x0028;the predicted
frame&#x0029; and <!--l. 43--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mover 
accent="true"><mrow 
><mi 
>P</mi></mrow><mo accent="true">&#x005E;</mo></mover></math>
&#x0028;the prediction frame&#x0029;, by means of </p><table class="equation"><tr><td> <a 
 id="x1-4001r3"></a>
<!--l. 45--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="block" class="equation">
                            <mi 
>E</mi> <mo 
class="MathClass-rel">=</mo> <mi 
>P</mi> <mo 
class="MathClass-bin">&#x2212;</mo><mover 
accent="true"><mrow 
><mi 
>P</mi></mrow><mo accent="true">&#x005E;</mo></mover>
</math></td><td class="eq-no">&#x0028;3&#x0029;</td></tr></table>
<!--l. 48--><p class="indent" >   while expecting that <!--l. 48--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mi 
>E</mi></math> will
be more compressible than <!--l. 48--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mi 
>P</mi></math>. To
achieve this, we can compute <!--l. 49--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mover class="overset"><mrow 
><mi 
>V</mi> </mrow><mrow 
><mi 
>P</mi> <mo 
class="MathClass-rel">&#x2192;</mo> <mi 
>R</mi></mrow></mover></math>
that simply minimizes the L<!--l. 50--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><msub><mrow 
></mrow><mrow 
><mn>2</mn></mrow></msub 
></math>
energy of <!--l. 50--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mi 
>E</mi></math>,
<!--l. 50--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mo 
class="MathClass-rel">&#x007C;</mo><mo 
class="MathClass-rel">&#x007C;</mo><mi 
>E</mi><mo 
class="MathClass-rel">&#x007C;</mo><msup><mrow 
><mo 
class="MathClass-rel">&#x007C;</mo></mrow><mrow 
><mn>2</mn></mrow></msup 
></math>, or we can
compute a <!--l. 51--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mover class="overset"><mrow 
><mi 
>V</mi> </mrow><mrow 
><mi 
>P</mi> <mo 
class="MathClass-rel">&#x2192;</mo> <mi 
>R</mi></mrow></mover></math>
that also describes the Optical Flow&#x00A0;<span class="cite">&#x005B;<a 
href="#Xhorn1981determining">3</a>&#x005D;</span> &#x0028;OF&#x0029; between the pixels of
<!--l. 52--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mi 
>R</mi></math> and
<!--l. 53--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mi 
>P</mi></math>, that although not
necessarily has to minimize <!--l. 53--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mo 
class="MathClass-rel">&#x007C;</mo><mo 
class="MathClass-rel">&#x007C;</mo><mi 
>E</mi><mo 
class="MathClass-rel">&#x007C;</mo><msup><mrow 
><mo 
class="MathClass-rel">&#x007C;</mo></mrow><mrow 
><mn>2</mn></mrow></msup 
></math>,
tries to show the true movement of the pixels between
<!--l. 54--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mi 
>R</mi></math> and
<!--l. 54--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mi 
>P</mi></math>. This
second option has the advantage of generating more visually pleasing predictions and
makes easier to predict the content of the motion &#xFB01;elds.
</p><!--l. 59--><p class="indent" >   The &#xFB01;rst type of techniques are simply called &#x201C;ME techniques&#x201D;, and are usually
faster than the second type, based on the computation of the OF.
</p><!--l. 68--><p class="indent" >   Now, let&#x2019;s see some of the most used techniques for estimating the motion
between two frames. Notice that, in general, better estimations can be found if we
                                                                  

                                                                  
suppose motion models such as that the objects use to exhibit inertia. However, this
case will not be considered for now.
</p><!--l. 73--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">1.3   </span> <a 
 id="x1-50001.3"></a><a 
href="https://vicente-gonzalez-ruiz.github.io/video_compression/#x1-40003" >Disjoint block matching</a></h4>
   <hr class="figure" /><div class="figure" 
>
                                                                  

                                                                  
                                                                  

                                                                  
<!--l. 77--><p class="noindent" ><div style="text-align:center;"> <img width=500 src="graphics/simple.svg" /> </div>  <a 
 id="x1-5001r1"></a>
<a 
 id="x1-5002"></a>
<br />                                                                  </p><div class="caption" 
><span class="id">
Figure&#x00A0;1: </span><span  
class="content">ME using disjoint blocks.                                    </span></div><!--tex4ht:label?: x1-5001r1 -->
                                                                  

                                                                  
   </div><hr class="endfigure" />
<!--l. 82--><p class="indent" >   In the easier ME algorithm, <!--l. 82--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mi 
>P</mi></math>
can be divided, for example, in blocks of 16x16 pixels &#x0028;see the
Fig.&#x00A0;<a 
href="#x1-5001r1">1<!--tex4ht:ref: fig:simple --></a>&#x0029;, and we can use the MSE, that measures the distance in
L<!--l. 84--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><msub><mrow 
></mrow><mrow 
><mn>2</mn></mrow></msub 
></math>, between each
block of <!--l. 84--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mi 
>P</mi></math> and its
surrounding pixels in <!--l. 85--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mi 
>R</mi></math>
&#x0028;the so called search area&#x0029;&#x00A0;<span class="cite">&#x005B;<a 
href="#Xzhu2000new">5</a>&#x005D;</span>. For each block, a motion vector that indicates the best
match &#x0028;smaller distance&#x0029; is found. The set of motion vectors form the motion vectors
&#xFB01;eld <!--l. 89--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mover class="overset"><mrow 
><mi 
>V</mi> </mrow><mrow 
><mi 
>P</mi> <mo 
class="MathClass-rel">&#x2192;</mo> <mi 
>R</mi></mrow></mover></math>
that obviously, except for a block size of 1x1, will be less dense than
<!--l. 90--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mi 
>R</mi></math> and
<!--l. 90--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mi 
>P</mi></math>.
Notice, however, that, it is not a good idea to use such a small block size because, in
general, the motion vectors will not describe the OF.
</p>
   <h4 class="subsectionHead"><span class="titlemark">1.4   </span> <a 
 id="x1-60001.4"></a>Overlaped block matching</h4>
   <hr class="figure" /><div class="figure" 
>
                                                                  

                                                                  
                                                                  

                                                                  
<!--l. 98--><p class="noindent" ><div style="text-align:center;"> <img width=500 src="graphics/overlaped.svg" /> </div>  <a 
 id="x1-6001r2"></a>
<a 
 id="x1-6002"></a>
<br />                                                                  </p><div class="caption" 
><span class="id">
Figure&#x00A0;2: </span><span  
class="content">ME using overlaped blocks.                                  </span></div><!--tex4ht:label?: x1-6001r1 -->
                                                                  

                                                                  
   </div><hr class="endfigure" />
<!--l. 103--><p class="indent" >   A better approximation to the OF for small block sizes can be found if we allow the blocks to
overlap in <!--l. 104--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mi 
>P</mi></math>&#x00A0;<span class="cite">&#x005B;<a 
href="#Xorchard1994overlapped">4</a>&#x005D;</span>,
in which case the block size for perform the comparisons is larger. Again, as it
happens with the disjoint case, only the non overlaped pixels are used for
building the prediction &#x0028;see the Fig.&#x00A0;<a 
href="#x1-6001r2">2<!--tex4ht:ref: fig:overlaped --></a>&#x0029;. Obviously, the main drawback of this
technique is that it more computationally demanding than the previous
one.
</p>
   <hr class="figure" /><div class="figure" 
>
                                                                  

                                                                  
                                                                  

                                                                  
<!--l. 114--><p class="noindent" ><div style="text-align:center;"> <img width=500 src="graphics/average.svg" /> </div>  <a 
 id="x1-6003r3"></a>
<a 
 id="x1-6004"></a>
<br />                                                                  </p><div class="caption" 
><span class="id">
Figure&#x00A0;3: </span><span  
class="content">ME using overlaped blocks, averaging the overlaped pixels.        </span></div><!--tex4ht:label?: x1-6003r1 -->
                                                                  

                                                                  
   </div><hr class="endfigure" />
<!--l. 119--><p class="indent" >   An improvement of the technique can also average the overlaped pixels in the prediction
frame <!--l. 120--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mover 
accent="true"><mrow 
><mi 
>P</mi></mrow><mo accent="true">&#x005E;</mo></mover></math>, as
it has been shown in the Fig.&#x00A0;<a 
href="#x1-6003r3">3<!--tex4ht:ref: fig:average --></a>.
</p>
   <h4 class="subsectionHead"><span class="titlemark">1.5   </span> <a 
 id="x1-70001.5"></a>Transform analysis</h4>
<!--l. 124--><p class="noindent" >One of the most successful techniques for computing the &#x0028;dense&#x0029; OF is based on the
analysis of the coe&#xFB03;cients resulting from transforming the frames using a polynomial
expansion &#x0028;the details of this transform and the impact of the motion in the
coe&#xFB03;cients can be found in the Gunnar Farneb&#x00E4;ck&#x2019;s paper&#x00A0;<span class="cite">&#x005B;<a 
href="#Xfarneback2003two">2</a>&#x005D;</span>&#x0029;. This is the
algorithm that we will use in our experiments for two important reasons: &#x0028;1&#x0029; it is
quite e&#xFB03;cient in terms both of performance and speed, and &#x0028;2&#x0029; it is already
<a 
href="https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html" >implemented</a> in OpenCV.
</p><!--l. 135--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">1.6   </span> <a 
 id="x1-80001.6"></a>Machine learning</h4>
<!--l. 136--><p class="noindent" >ANNs &#x0028;Arti&#xFB01;cal Neural Networks&#x0029; can be trained to estimate the motion between
frames&#x00A0;<span class="cite">&#x005B;<a 
href="#Xdosovitskiy2015flownet">1</a>&#x005D;</span>.
</p><!--l. 139--><p class="noindent" >
</p>
   <h4 class="subsectionHead"><span class="titlemark">1.7   </span> <a 
 id="x1-90001.7"></a>Motion compensation in action</h4>
   <hr class="figure" /><div class="figure" 
>
                                                                  

                                                                  
                                                                  

                                                                  
<!--l. 141--><p class="noindent" ><div style="text-align:center;"> <img width=1000 src="R.svg" /> </div>  <div style="text-align:center;"> <img width=1000 src="P.svg" /> </div>   <div style="text-align:center;"> <img width=1000 src="y_motion.svg" /> </div>   <div style="text-align:center;"> <img width=1000 src="x_motion.svg" /> </div>   <div style="text-align:center;"> <img width=1000 src="hat_P.svg" /> </div>   <div style="text-align:center;"> <img width=1000 src="without_ME.svg" /> </div>   <div style="text-align:center;"> <img width=1000 src="with_ME.svg" /> </div>   <a 
 id="x1-9001r4"></a>
<a 
 id="x1-9002"></a>
<br />                                                                  </p><div class="caption" 
><span class="id">
Figure&#x00A0;4: </span><span  
class="content">E&#xFB00;ect of ME &#x0028;using OF&#x0029; over the temporal redundancy removal.
</span></div><!--tex4ht:label?: x1-9001r1 -->
                                                                  

                                                                  
   </div><hr class="endfigure" />
<!--l. 152--><p class="indent" >   The Fig.&#x00A0;<a 
href="#x1-9001r4">4<!--tex4ht:ref: fig:MC --></a> shows an example the performace of the use of OF,
comparing the prediction error generated with and without ME. In this
experiment, a motion vector has been computed between each point of
<!--l. 155--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mi 
>P</mi></math> and
<!--l. 155--><math 
 xmlns="http://www.w3.org/1998/Math/MathML"  
display="inline" ><mi 
>R</mi></math>. As it
can be seen, ME reduces the temporal redundancy signi&#xFB01;cantly.
</p>
   <h3 class="sectionHead"><span class="titlemark">2   </span> <a 
 id="x1-100002"></a>What you have to do?</h3>
<!--l. 197--><p class="noindent" >In this <a 
href="https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/10-ME/optical_flow.ipynb" >notebook</a> you can &#xFB01;nd how to estimate the OF between two frames. Please,
modify it to &#xFB01;nd suitable values for the parameters <span 
class="ectt-1000">levels</span>, <span 
class="ectt-1000">winsize </span>and
<span 
class="ectt-1000">iterations</span>. Supposing that the impact of each parameter is independent from the
rest, the best way of comparing two di&#xFB00;erent con&#xFB01;gurations is render RD curves
using quantization and compressing the residues.
</p><!--l. 206--><p class="noindent" >
</p>
   <h3 class="sectionHead"><span class="titlemark">3   </span> <a 
 id="x1-110003"></a>Timming</h3>
<!--l. 208--><p class="noindent" >Please, &#xFB01;nish this milestone before the next class session.
</p><!--l. 210--><p class="noindent" >
</p>
   <h3 class="sectionHead"><span class="titlemark">4   </span> <a 
 id="x1-120004"></a>Deliverables</h3>
<!--l. 212--><p class="noindent" >None.
</p><!--l. 214--><p class="noindent" >
</p>
   <h3 class="sectionHead"><span class="titlemark">5   </span> <a 
 id="x1-130005"></a>Resources</h3>
<!--l. 1--><p class="noindent" >
</p>
   <h3 class="likesectionHead"><a 
 id="x1-140005"></a>References</h3>
<!--l. 1--><p class="noindent" >
   </p><div class="thebibliography">
                                                                  

                                                                  
   <p class="bibitem" ><span class="biblabel">
 &#x005B;1&#x005D;<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xdosovitskiy2015flownet"></a>A.&#x00A0;Dosovitskiy, P.&#x00A0;Fischer, E.&#x00A0;Ilg, P.&#x00A0;Hausser, C.&#x00A0;Hazirbas, V.&#x00A0;Golkov,
   P.&#x00A0;Van Der&#x00A0;Smagt, D.&#x00A0;Cremers, and T.&#x00A0;Brox. <a 
href="https://openaccess.thecvf.com/content_iccv_2015/papers/Dosovitskiy_FlowNet_Learning_Optical_ICCV_2015_paper.pdf" >FlowNet: Learning Optical
   Flow with Convolutional Networks</a>. In <span 
class="ecti-1000">Proceedings of the IEEE international</span>
   <span 
class="ecti-1000">conference on computer vision</span>, pages 2758&#x2013;2766, 2015.
   </p>
   <p class="bibitem" ><span class="biblabel">
 &#x005B;2&#x005D;<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xfarneback2003two"></a>G.&#x00A0;Farneb&#x00E4;ck.  <a 
href="https://link.springer.com/content/pdf/10.1007/3-540-45103-X_50.pdf" >Two-Frame Motion Estimation Based on Polynomial
   Pxpansion</a>.  In <span 
class="ecti-1000">Scandinavian conference on Image analysis</span>, pages 363&#x2013;370.
   Springer, 2003.
   </p>
   <p class="bibitem" ><span class="biblabel">
 &#x005B;3&#x005D;<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xhorn1981determining"></a>B.K.P.  Horn  and  B.G.  Schunck.    <a 
href="https://www.caam.rice.edu/~zhang/caam699/opt-flow/horn81.pdf" >Determining  Optical  Flow</a>.    In
   <span 
class="ecti-1000">Techniques and Applications of Image Understanding</span>, volume 281, pages
   319&#x2013;331. International Society for Optics and Photonics, 1981.
   </p>
   <p class="bibitem" ><span class="biblabel">
 &#x005B;4&#x005D;<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xorchard1994overlapped"></a>M.T.   Orchard   and   G.J.   Sullivan.       <a 
href="https://www.semanticscholar.org/paper/Overlapped-block-motion-compensation%3A-an-approach-Orchard-Sullivan/8f46f291825caa786890ef224a28cf513f049799" >Overlapped   Block   Motion
   Compensation: An Estimation-Theoretic Approach</a>. <span 
class="ecti-1000">IEEE Transactions on</span>
   <span 
class="ecti-1000">Image Processing</span>, 3&#x0028;5&#x0029;:693&#x2013;699, 1994.
   </p>
   <p class="bibitem" ><span class="biblabel">
 &#x005B;5&#x005D;<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xzhu2000new"></a>S.&#x00A0;Zhu  and  K.-K.  Ma.     <a 
href="https://www.cl.cam.ac.uk/teaching/1718/SysOnChip/materials.d/mpeg-diamond-search-motion-esimation-zhu-ma-2000.pdf" >A  New  Diamond  Search  Algorithm  for
   Fast  Block-Matching  Motion  Estimation</a>.   <span 
class="ecti-1000">IEEE  transactions  on  Image</span>
   <span 
class="ecti-1000">Processing</span>, 9&#x0028;2&#x0029;:287&#x2013;290, 2000.
</p>
   </div>
    
</body></html> 

                                                                  


