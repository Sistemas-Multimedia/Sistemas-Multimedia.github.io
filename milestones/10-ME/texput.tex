\input{../../definitions}
\title{\SM{} - Study Guide - Milestone 10: Motion Estimation}

\maketitle

\tableofcontents

\section{Description}

\subsection{Motion Estimation (ME) for what?}
Temporal correlation between video frames\footnote{Remember that,
  although this discussion will deal with frames, in our particular
  case, we will decorrelate DWT subbands.} can be removed by MC
(Motion Compensation). MC implies to decrease (usually by
substracting) the amount of information in the frames. The removed
information must be available at both, the encoder and the decoder
side, in order to make the process reversible.

Specifically, an MC Predictor (MCP) inputs one (or more) reference
frame(s) $\{R_k\}$, and one (or more) motion vectors field(s)
$\{\overset{\cdot\rightarrow\cdot}{V}_k\}$, and outputs a prediction
frame
\begin{equation}
  \hat{P} =  \overset{P\rightarrow R}{V}(R)
\end{equation}
and how to use it. In this milestone we show how to compute
$\{\overset{P\rightarrow R}{V}_k\}$. In the next milestone we will see
how to remove the $\hat{P}$'s information from the predicted frame
$P$. For the sake of simplicity, in the rest of this discussion it
will be suppose that the number of reference frames in only 1
(probably, the closest to the predicted frame), and therefore, only
one motion field $\overset{P\rightarrow R}{V}$ will be used.

\subsection{What exactly we need?}
Our main objective is to minimize the differences (for example, the
\href{https://en.wikipedia.org/wiki/Euclidean_distance}{L$_2$
  distance}) between $P$ (the predicted frame) and $\hat{P}$ (the
prediction frame), by means of
\begin{equation}
  E = P - \hat{P}
\end{equation}
while expecting that $E$ will be more compressible than $P$. To
achieve this, we can compute $\overset{P\rightarrow R}{V}$ that simply
minimizes the L$_2$ energy of $E$, $||E||^2$, or we can compute a
$\overset{P\rightarrow R}{V}$ that also describes the Optical
Flow~\cite{horn1981determining} (OF) between the pixels of $R$ and
$P$, that although not necessarily has to minimize $||E||^2$, tries to
show the true movement of the pixels between $R$ and $P$. This second
option has the advantage of generating more visually pleasing
predictions and makes easier to predict the content of the motion
fields.

The first type of techniques are simply called ``ME techniques'', and
are usually faster than the second type, based on the computation of
the OF.

%Let's see two basic techniques to estimate the motion between 2
%frames, $R$ and $P$. In this discussion it will be supposed that the
%motion of the objects that are in both frames is bounded, and that the
%luminance varies smoothly between adjacent frames.

Now, let's see some of the most used techniques for estimating the
motion between two frames. Notice that, in general, better estimations
can be found if we suppose motion models such as that the objects use
to exhibit inertia. However, this case will not be considered for now.

\subsection{\href{https://vicente-gonzalez-ruiz.github.io/video_compression/\#x1-40003}{Disjoint block matching}}

\begin{figure}
  \centering
  \svg{graphics/simple}{500}
  \caption{ME using disjoint blocks.}
  \label{fig:simple}
\end{figure}

In the easier ME algorithm, $P$ can be divided, for example, in blocks
of 16x16 pixels (see the Fig.~\ref{fig:simple}), and we can use the
MSE, that measures the distance in L$_2$, between each block of $P$
and its surrounding pixels in $R$ (the so called search
area)~\cite{zhu2000new}. For each block, a motion vector that
indicates the best match (smaller distance) is found. The set of
motion vectors form the motion vectors field
$\overset{P\rightarrow R}{V}$ that obviously, except for a block size
of 1x1, will be less dense than $R$ and $P$. Notice, however, that, it
is not a good idea to use such a small block size because, in general,
the motion vectors will not describe the OF.

\subsection{Overlaped block matching}

\begin{figure}
  \centering
  \svg{graphics/overlaped}{500}
  \caption{ME using overlaped blocks.}
  \label{fig:overlaped}
\end{figure}

A better approximation to the OF for small block sizes can be found if
we allow the blocks to overlap in $P$~\cite{orchard1994overlapped}, in
which case the block size for perform the comparisons is
larger. Again, as it happens with the disjoint case, only the non
overlaped pixels are used for building the prediction (see the
Fig.~\ref{fig:overlaped}). Obviously, the main drawback of this
technique is that it more computationally demanding than the previous
one.

\begin{figure}
  \centering
  \svg{graphics/average}{500}
  \caption{ME using overlaped blocks, averaging the overlaped pixels.}
  \label{fig:average}
\end{figure}

An improvement of the technique can also average the overlaped pixels
in the prediction frame $\hat{P}$, as it has been shown in the
Fig.~\ref{fig:average}.

\subsection{Transform analysis}
One of the most successful techniques for computing the (dense)
OF is based on the analysis of the coefficients resulting
from transforming the frames using a polynomial expansion (the details
of this transform and the impact of the motion in the coefficients can
be found in the paper or Gunnar
Farneb{\"a}ck~\cite{farneback2003two}). This is the algorithm that we
will use in our experiments for two important reasons: (1) it is quite
efficient in terms both of performance and speed, and (2) it is
already
\href{https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html}{implemented}
in OpenCV.

\subsection{Machine learning}
ANNs (Artifical Neural Networks) can be trained to estimate the
motion between frames~\cite{dosovitskiy2015flownet}.

\subsection{Motion compensation in action}
\begin{figure}
  \svg{R}{1000}
  \svg{P}{1000}
  \svg{y_motion}{1000}
  \svg{x_motion}{1000}
  \svg{hat_P}{1000}
  \svg{without_ME}{1000}
  \svg{with_ME}{1000}
  \caption{Effect of ME (using OF) over the temporal redundancy
    removal.}
  \label{fig:MC}
\end{figure}
The Fig.~\ref{fig:MC} shows an example the performace of the use of
OF, comparing the prediction error generated with and without ME. A
motion vector has been computed between each point of $P$ and $R$. As
it can be seen, ME reduces the temporal redundancy significantly.

\begin{comment}
The OF~\cite{horn1981determining} tries to establish connections between the pixels of
the frames $P$ and $R$ supposing that:
\begin{enumerate}
\item $P$ and $R$ are adjacent in time (if $R$ was taken at time $t$,
  $P$ is taken at time $dt+t$) and therefore, similar in
  content.
\item Similarity between images implies that the pixels in both
  frames, $R$ and $P$, will have the same luminance. If $I(x,y,t)$
  measures the luminance of the pixel $(x,y)$ of the frame $R$,
  similarity can be modeled by
  \begin{equation}
    I(x+dx, y+dy, t+dt) = I(x,y,t),
    \label{eq:similarity}
  \end{equation}
  where $I(x+dx, y+dy, t+dt)$ is the corresponding pixel in the frame
  $P$. The first part of the Eq.~\ref{eq:similarity} can be also
  computed by (using the first-order Taylor expansion) as
  \begin{equation}
    I(x+dx, y+dy, t+dt) = I(x,y,t) + \frac{\partial I}{\partial x}dx + \frac{\partial I}{\partial y}dy + \frac{\partial I}{\partial t}dt,
    \label{eq:taylor_exp}
  \end{equation}
  andtherefore, it must be true that
  \begin{equation}
    \frac{\partial I}{\partial x}dx + \frac{\partial I}{\partial y}dy + \frac{\partial I}{\partial t}dt = 0.
    \label{eq:constraint}
  \end{equation}
  Dividing by $dt$, we finally get that
  \begin{equation}
    \frac{\partial I}{\partial x}\frac{dx}{dt} + \frac{\partial I}{\partial y}\frac{dy}{dt} + \frac{\partial I}{\partial t} = 0.
  \end{equation}
\item Adjacent pixels follow parallel
  trajectories~\cite{horn1981determining}, with basically means that
  neighbor pixels will have similar motion.
\end{enumerate}
\end{comment}

\section{What you have to do?}

In this
\href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/study_guide/10-ME/optical_flow.ipynb}{notebook}
you can find how to estimate the OF between two frames. Please, modify
it to find suitable values for the parameters \texttt{levels},
\texttt{winsize} and \texttt{iterations}. Supposing that the impact of
each parameter is independent from the rest, the best way of comparing
two different configurations is render RD curves using quantization
and compressing the residues.

\section{Timming}

Please, finish this milestone before the next class session.

\section{Deliverables}

None.

\section{Resources}

\bibliography{motion-estimation}
