\input{../../definitions}
\title{\SM{} - Study Guide - Milestone 10: Motion Estimation}

\maketitle

\tableofcontents

\section{Description}

\subsection{Motion Estimation (ME) for what?}
Temporal correlation between video frames\footnote{Remember that,
  although this discussion will deal with frames, in our particular
  case, we will decorrelate DWT subbands.} can be removed by MC
(Motion Compensation). MC implies to decrease (usually by
substracting) an amount of information in the frames. The removed
information must be available at both, the encoder and the decoder
side, in order to make this a reversible process.

Specifically, a MCP (MC Predictor) inputs one (or more) reference
frame(s) ${\mathbf R}=\{{\mathbf R}_i\}$, and a motion vectors field
$\overset{{\mathbf R}\rightarrow{\mathbf P}}{\mathbf V}$ that
indicates how to project ${\mathbf R}$ onto the predicted frame ${\mathbf P}$, and outputs
a prediction frame
\begin{equation}
  \hat{{\mathbf P}} =  \overset{{\mathbf P}\rightarrow {\mathbf R}}{\mathbf V}({\mathbf R}).
  \label{eq:MCP1}
\end{equation}
In this milestone we analyze different algorithms to determine
$\overset{{\mathbf P}\rightarrow {\mathbf R}}{\mathbf V}$ (in a futher
milestone we will see how to remove the $\hat{{\mathbf P}}$'s
information from ${\mathbf P}$). At this moment, for the sake of
simplicity, in the rest of this discussion it will be supposed that the
number of reference frames in only 1 (probably, the closest one to the
predicted frame), and therefore, we don't need to specify which are
the reference images to make the prediction. For this reason,
Eq,~\ref{eq:MCP1} can be rewritten as
\begin{equation}
  \hat{\mathbf W}_k = \overset{{\mathbf W}_{k-1}\rightarrow {\mathbf
  W}_{k}}{\mathbf V}({\mathbf W}_{k-1}) = \overset{(k-1)\rightarrow
          k}{\mathbf V}({\mathbf W}_{k-1})
%  \hat{{\mathbf P}_k} = \overset{{\mathbf P}_k\rightarrow {\mathbf
%  P}_{k-1}}{\mathbf V}({\mathbf P}_{k-1}) = \overset{k\rightarrow
%          k-1}{\mathbf V},
  \label{eq:MCP2}
\end{equation}

\subsection{But ... what exactly do we need?}
Our main objective is to minimize the differences (for example, the
\href{https://en.wikipedia.org/wiki/Euclidean_distance}{L$_2$
  distance}) between ${\mathbf W}_k$ (the predicted frame) and $\hat{\mathbf W}_k$ (the
prediction frame), i.e. minimizing
\begin{equation}
  {\mathbf E}_k = {\mathbf W}_k - \hat{\mathbf W}_k
\end{equation}
in order to get that ${\mathbf E}$ will be more compressible than
${\mathbf W}$. To achieve this, we can compute ${\mathbf V}$ that
simply minimizes the L$_2$ energy of ${\mathbf E}$, $||{\mathbf
  E}||^2$, or we can compute a ${\mathbf V}$ that also describes the
Optical Flow~\cite{horn1981determining} (OF) between the pixels of
${\mathbf W}_{k-1}$ and ${\mathbf W}_k$, that although not necessarily
has to minimize $||{\mathbf E}||^2$, tries to show the true movement
of the pixels between the both frames. This second option has the
advantage of generating more visually pleasing reconstructions when
the code-stream is partially received and makes easier to predict the
content of the motion fields.

The first type of techniques are simply called ``ME techniques'', and
are usually faster than the second type, based on the estimation of
the OF.

%Let's see two basic techniques to estimate the motion between 2
%frames, $R$ and $P$. In this discussion it will be supposed that the
%motion of the objects that are in both frames is bounded, and that the
%luminance varies smoothly between adjacent frames.

Now, let's see some of the most used techniques for estimating the
motion between two frames. Notice that, in general, better estimations
can be found if we suppose motion models such as that the objects
exhibit inertia. However, this case will not be considered for now.

\subsection{\href{https://vicente-gonzalez-ruiz.github.io/video_compression/\#x1-40003}{Block-based disjoint motion estimation}}

\begin{figure}
  \centering
  \svg{graphics/simple}{500}
  \caption{ME using disjoint blocks.}
  \label{fig:simple}
\end{figure}

In the simplest ME algorithm (see the Fig.~\ref{fig:simple}),
${\mathbf P}$ is divided in blocks of (for example) 16x16 pixels, and we can use the
MSE that measures the distance in L$_2$ between each block of
${\mathbf P}$ and its surrounding pixels in ${\mathbf R}$ (the so
called search area)~\cite{zhu2000new}. For each block, a motion vector
that indicates the best match (smaller distance) is found. The set of
motion vectors form the motion vectors field $\overset{{\mathbf
    R}\rightarrow {\mathbf P}}{\mathbf V}$ that obviously, except for
a block size of 1x1, will be less dense than ${\mathbf R}$ and
${\mathbf P}$. Notice, however, that, it is not a good idea to use
such a small block size because, in general, the motion vectors will
not describe the OF.

\subsection{Overlaped block matching}

\begin{figure}
  \centering
  \svg{graphics/overlaped}{500}
  \caption{ME using overlaped blocks.}
  \label{fig:overlaped}
\end{figure}

A better approximation to the OF for small block sizes can be found if
we allow the blocks to overlap in ${\mathbf
  P}$~\cite{orchard1994overlapped}, case in which the block size for
performing the comparisons must be larger. Again, as it happens with the
disjoint case, only the non overlaped pixels are used for building the
prediction (see the Fig.~\ref{fig:overlaped}). Obviously, the main
drawback of this technique is that it more computationally demanding
than the previous one.

\begin{figure}
  \centering
  \svg{graphics/average}{500}
  \caption{ME using overlaped blocks, averaging the overlaped pixels.}
  \label{fig:average}
\end{figure}

An improvement of the previous technique can also average the
overlaped pixels in the prediction frame $\hat{P}$, as it has been
shown in the Fig.~\ref{fig:average}.

\subsection{Transform analysis}
One of the most successful techniques for computing the (dense) OF is
based on the analysis of the coefficients resulting from transforming
the frames using a polynomial expansion (the details of this transform
and the impact of the motion in the coefficients can be found in the
Gunnar Farneb{\"a}ck's paper~\cite{farneback2003two}). This is the
algorithm that we will use in our experiments for two important
reasons: (1) it is quite efficient in terms both of performance and
speed, and (2) it is already
\href{https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html}{implemented}
in OpenCV.

\subsection{Machine learning}
ANNs (Artifical Neural Networks) can be trained to estimate the motion
between frames~\cite{dosovitskiy2015flownet}. For the training of
ANNs, animation videos are generally used where the motion fields are
known with precision.

\subsection{Motion compensation in action}
\begin{figure}
  \svg{R}{1000}
  \svg{P}{1000}
  \svg{y_motion}{1000}
  \svg{x_motion}{1000}
  \svg{hat_P}{1000}
  \svg{without_ME}{1000}
  \svg{with_ME}{1000}
  \caption{Effect of ME (using OF) over the temporal redundancy
    removal.}
  \label{fig:MC}
\end{figure}
The Fig.~\ref{fig:MC} shows an example the performace of the use of
OF, comparing the prediction error generated with and without ME. In
this experiment, a motion vector has been computed between each point
of ${\mathbf P}$ and ${\mathbf R}$. As it can be seen, the OF can
reduce the temporal redundancy significantly.

\begin{comment}
The OF~\cite{horn1981determining} tries to establish connections between the pixels of
the frames $P$ and $R$ supposing that:
\begin{enumerate}
\item $P$ and $R$ are adjacent in time (if $R$ was taken at time $t$,
  $P$ is taken at time $dt+t$) and therefore, similar in
  content.
\item Similarity between images implies that the pixels in both
  frames, $R$ and $P$, will have the same luminance. If $I(x,y,t)$
  measures the luminance of the pixel $(x,y)$ of the frame $R$,
  similarity can be modeled by
  \begin{equation}
    I(x+dx, y+dy, t+dt) = I(x,y,t),
    \label{eq:similarity}
  \end{equation}
  where $I(x+dx, y+dy, t+dt)$ is the corresponding pixel in the frame
  $P$. The first part of the Eq.~\ref{eq:similarity} can be also
  computed by (using the first-order Taylor expansion) as
  \begin{equation}
    I(x+dx, y+dy, t+dt) = I(x,y,t) + \frac{\partial I}{\partial x}dx + \frac{\partial I}{\partial y}dy + \frac{\partial I}{\partial t}dt,
    \label{eq:taylor_exp}
  \end{equation}
  andtherefore, it must be true that
  \begin{equation}
    \frac{\partial I}{\partial x}dx + \frac{\partial I}{\partial y}dy + \frac{\partial I}{\partial t}dt = 0.
    \label{eq:constraint}
  \end{equation}
  Dividing by $dt$, we finally get that
  \begin{equation}
    \frac{\partial I}{\partial x}\frac{dx}{dt} + \frac{\partial I}{\partial y}\frac{dy}{dt} + \frac{\partial I}{\partial t} = 0.
  \end{equation}
\item Adjacent pixels follow parallel
  trajectories~\cite{horn1981determining}, with basically means that
  neighbor pixels will have similar motion.
\end{enumerate}
\end{comment}

\section{What you have to do?}

In this
\href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/10-ME/optical_flow.ipynb}{notebook}
you can find how to estimate the OF between two frames. Please, modify
it to find suitable values for the parameters \texttt{levels},
\texttt{winsize} and \texttt{iterations}. Supposing that the impact of
each parameter is independent from the rest, the best way of comparing
two different configurations is to trace RD curves using quantization
and compressing the residues.

\section{Timming}

Please, finish this milestone before the next class session.

\section{Deliverables}

None.

\section{Resources}

\renewcommand{\addcontentsline}[3]{}% Remove functionality of \addcontentsline
\bibliography{motion-estimation}
