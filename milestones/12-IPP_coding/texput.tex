\input{../../definitions}
\title{\SM{} - Study Guide - Milestone 12: IPP... coding in MRVC (Multi Resolution Video Codec)}

\maketitle

\tableofcontents

\section{Description}

\subsection{The IPP... decorrelation pattern}
It's time to put together all the ``tools'' that we have developed for
encoding a sequence of frames $\{V_k\}$. First, the sequence will be
splitted into GOFs
(\href{https://en.wikipedia.org/wiki/Group_of_pictures}{Group Of
  Frames}), and the structure of each GOF will be
IPP...~\cite{le1991mpeg}, which means that the first frame of each GOF
will be intra-coded (I-type), and the rest of frames of the GOF will
be predicted-coded (P-type), respect to the previous one\footnote{A
  P-type frame except for the second one, that always has a I-frame as
  reference.}. Notice that in an I-type frame all the coefficients
(\emph{coeffs} in short, remember that we are compensating the motion
in the DWT domain) will be I-type coeffs, and in a P-type frame, the
different coeffs will be I-type or P-type.

\subsection{A block diagram of the codec}

\begin{figure}
  \centering
  \svg{graphics/codec2}{1000}
  \caption{The IPP... MRVC video codec.}
  \label{fig:codec}
\end{figure}

The MRVC IPP... codec has been described in the
Fig.~\ref{fig:codec}. The equations that describe this system are:

\begin{equation}
  (V_k.L, V_k.H) \leftarrow \text{DWT}(V_k),
  \tag{a}
\end{equation}
where $\leftarrow$ denotes the
\href{https://en.wikipedia.org/wiki/Assignment_(computer_science)}{assignment}
operator, and $V_k$ is the $k$-th frame of the sequence,

\begin{equation}
  [V_k.L] \leftarrow \text{DWT}^{-1}(V_k.L, 0),
  \tag{E.a}
\end{equation}

\begin{equation}
  Z^{-1}([V_k.L]) = [V_{k-1}.L],
  \tag{E.b}
\end{equation}
and by definition, $Z^{-1}([V_{-1}.L]) = 0$.

\begin{equation}
  \overset{k+1\rightarrow k}{V} \leftarrow \text{ME}([V_k.L], [V_{k-1}.L]),
  \tag{E.c}
\end{equation}
where ME stands for Motion Estimation, and by definition,
$\overset{0\rightarrow (-1)}{V}=0$,

\begin{equation}
  [\hat{V}_k.L] \leftarrow \text{MCP}(\overset{k+1\rightarrow k}{V}, [V_{k-1}.L]),
  \tag{E.d}
\end{equation}
where MCP stands for Motion Compensated Prediction,

\begin{equation}
  [E_k.L] \leftarrow [V_k.L] - [\hat{V}_k.L],
  \tag{E.e}
\end{equation}

\begin{equation}
  \{[M_k],[S_k]\} \leftarrow \text{EW-min}([V_k.L], [E_k.L])
  \tag{E.f}
\end{equation}
where
\begin{equation}
  [M_k]_{i,j}=\text{min}([V_k.L]_{i,j}, [E_k.L]_{i,j}),
\end{equation}
and $[S_k]$ is a binary matrix defined by
\begin{equation}
  [S_k]_{i,j} = \left\{
  \begin{array}{lll}
    0 & \text{if}~[V_k.L]_{i,j} < [E_k.L]_{i,j} & \text{(I-type coeff)} \\
    1 & \text{otherwise}                      & \text{(P-type coeff)},
  \end{array}
  \right.
  \label{eq:matrix}
\end{equation}
(notice that $[M_k]$, that contains the element-wise minimum of both
matrices, is discarded)

\begin{equation}
  [V_k.H] \leftarrow \text{DWT}^{-1}(0, V_k.H),
  \tag{b}
\end{equation}

\begin{equation}
  [E_k.H] \leftarrow [V_k.H] - [\hat{V}_k.H],
  \tag{c}
\end{equation}
where, notice that
\begin{equation}
  [E_k.H]_{i,j} = \left\{
  \begin{array}{ll}
    {[}V_k.H{]}_{i,j}                       & \text{if}~{[}\hat{W}_k.H{]}_{i,j} = 0~\text{(I-type coeff)} \\
    {[}V_k.H{]}_{i,j} - [\hat{W}_k.H]_{i,j} & \text{otherwise}~\text{(P-type coeff)},
  \end{array}
\right.
\end{equation}

\begin{equation}
  [\tilde{E}_k.H] \leftarrow \text{Q}([E_K.H]),
  \tag{d}
\end{equation}

\begin{equation}
  [\tilde{E}_k.H] \leftarrow  \text{Q}^{-1}([\tilde{E}_K.H]),
  \tag{E.g}
\end{equation}

\begin{equation}
  [\tilde{V}_k.H] \leftarrow [\tilde{E}_k.H] + [\hat{V}_k.H],
  \tag{E.h}
\end{equation}
and notice that if $[\hat{V}_k.H]=0$ then $[\tilde{V}_k.H] =
[\tilde{E}_k.H]$,

\begin{equation}
  Z^{-1}([\tilde{V}_k.H]) = [V_{k-1}.H],
  \tag{E.i}
\end{equation}
and by definition, $Z^{-1}([V_{-1}.H]) = 0$,

\begin{equation}
  [\hat{V}_k.H] \leftarrow \text{MCP}(\overset{k+1\rightarrow k}{V}, [V_{k-1}.H]),
  \tag{E.j}
\end{equation}

\begin{equation}
  [\hat{W}_k.H]_{i,j} \leftarrow \left\{
    \begin{array}{ll}
      {[}\hat{V}_k.H{]}_{i,j}  & \text{if}~{[}E_k.L{]}_{i,j} < {[}V_k.L{]}_{i,j} \text{(P-type coeff)} \\
      0                       & \text{otherwise (I-type coeff)},
    \end{array}
  \right.
  \tag{E.k}
\end{equation}
  
\begin{equation}
  (0, \tilde{E}_k.H) \leftarrow \text{DWT}([\tilde{E}_k.H]),
  \tag{f}
\end{equation}

\begin{equation}
  \{V_k.L, \tilde{E}_k.H\} \leftarrow \text{E}(V_k.L, \tilde{E}_k.H),
  \tag{g}
\end{equation}
where E represents the entropy coding of both data sources, in two
different code-streams,

\begin{equation}
  (V_k.L, \tilde{E}_k.H) \leftarrow \text{E}^{-1}(\{V_k.L, \tilde{E}_k.H\}),
  \tag{h}
\end{equation}

\begin{equation}
  [\tilde{E}_k.H] \leftarrow \text{DWT}^{-1}(0, \tilde{E}_k.H),
  \tag{i}
\end{equation}

\begin{equation}
  (0, \tilde{V}_k.H) \leftarrow \text{DWT}(0, [\tilde{V}_k.H]),
  \tag{j}
\end{equation}

and

\begin{equation}
  \tilde{V}_k \leftarrow \text{DWT}^{-1}(V_k.L, \tilde{V}_k.H).
  \tag{k}
\end{equation}

This
\href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/12-IPP_coding/DPCM.ipynb}{notebook}
shows how to implement a simple DPCM codec.

\subsection{Avoding the drift error}
As it can be seen in the previous section, in each IPP... iteration,
only the high-frequency information of the sequence of frames is
decorrelated ($H$ subbands) considering the information provided by
the low-frequency information ($L$ subband), which is losslessly
transmitted between the encoder and the decoder. If the $L$ data
cannot be transmitted to the decoder, a drift error will occur because
the matrix $S_k$ will be different in the encoder and the decoder for
the same frame $k$.

Obviously (and unfortunately), this lossless transmission bounds the
compression ratio that we will get. One solution is to perform more
(than 1) levels at the DWT stage (Eq.~(a)) and to apply the
IPP... MRVC codec using the lowest spatial resolution levels. If we
represent the Spatial Resolution Level (SRL) with an superindex, being
0 the original SRL, we can express the operation of the previous codec
by
\begin{equation}
  \left\{
    \begin{array}{l}
      \text{C}(V^0_k) = \{V^0_k.L, \tilde{E}^0_k.H\} = \{V^1_k, \tilde{E}^0_k.H\} \\
      \text{D}(\{V^1_k, \tilde{E}^0_k.H\}) = \tilde{V}^0_k,
    \end{array}
  \right.
  \label{eq:codec_1l}
\end{equation}
where $\text{C}$ represents the encoder and $\text{D}$ the decoder. As
it can be seen, Eq.~\ref{eq:codec_1l} is only valid when only one
level of the DWT has been applied.

In general, for $s$ levels of the DWT, we have that
\begin{equation}
  \left\{
    \begin{array}{l}
      \text{C}(V^{s-1}_k) = \{V^s_k, \tilde{E}^{s-1}_k.H\} \\
      \text{D}(\{V^s_k, \tilde{E}^{s-1}_k.H\}) = \tilde{V}^{s-1}_k,
    \end{array}
  \right.
  \label{eq:codec_sl}
\end{equation}
where $\tilde{V}^{s-1}$ is the $(s-1)$-th SRL of the reconstructed
sequence $\tilde{V}$.

The next SRL ($s-2$) is determined by
\begin{equation}
  \left\{
    \begin{array}{l}
      \text{C}(\tilde{V}^{s-2}_k) = \{\tilde{V}^{s-1}_k, \tilde{E}^{s-2}_k.H\} \\
      \text{D}(\{\tilde{V}^{s-1}_k, \tilde{E}^{s-2}_k.H\}) = \tilde{V}^{s-2}_k,
    \end{array}
  \right.
  \label{eq:codec_s1l}
\end{equation}
and finally,
\begin{equation}
  \left\{
    \begin{array}{l}
      \text{C}(\tilde{V}^0_k) = \{\tilde{V}^1_k, \tilde{E}^0_k.H\} \\
      \text{D}(\{\tilde{V}^1_k, \tilde{E}^0_k.H\}) = \tilde{V}^0_k,
    \end{array}
  \right.
  \label{eq:codec_s1l}
\end{equation}

\begin{figure}
  \centering
  \svg{graphics/multiresolution}{600}
  \caption{Implementing spatial multiresolution.}
  \label{fig:multiresolution}
\end{figure}

So, only the lowest SRL of $\tilde{V}$, $\tilde{V}^s$ is an
IPP... pure sequence of small frames, losslessly encoded. This
multiresolution (multistage) scheme has been described in the
Fig.~\ref{fig:multiresolution}. The output of an IPP... iteration will be refered as ``a
spatial-layer of code-stream'', or simply as ``a S-layer''.

\subsection{SQ (layers) progression}
The logical order of the S-layers in the code-stream is the one that
allows, when the code-stream is decoded sequentially, the progressive
incrase of the spatial resolution of the video. For example, if $s$
levels of the DWT has been applied, the generated code-stream has
$N_{\text{SRL}}+1$ S-layers
\begin{equation*}
  \{V^s,\tilde{E}^{s-1}.H,\tilde{E}^{s-2}.H,\cdots,\tilde{E}^0.H\},
\end{equation*}
which are able to generate $N_{\text{SRL}}+1$ progressive
reconstructions
\begin{equation*}
  \{V^s,\tilde{V}^{s-1},\tilde{V}^{s-2},\cdots,\tilde{V}^0\}.
\end{equation*}

Moreover, Quality (Q-progression) scalability in each SRL can be also
achieved in the high-frequency textures if a quality-scalable image
codec such as JPEG2000~\cite{taubman2002jpeg2000} replaces the PNG
compressor, generating a number $N_{\text{QL}}$ of quality-layers
(``Q-layers'') by each motion compensated high-frequency subband. A
SQ-progression is defined considering both forms of scalability
(spatial and quality), with a higher number of layers. For example, 
For example, if $N_{\text{SRL}}=3$ (2 IPP...-type iterations) and
$N_{\text{QL}}=2$, the progression of layers would be
\begin{equation*}
  \{V^s[1],V^s[0],\tilde{E}^{s-1}.H[1],\tilde{E}^{s-1}.H[0],\tilde{E}^{s-2}.H[1],\tilde{E}^{s-2}.H[0],\cdots,\tilde{E}^0.H[1],\tilde{E}^0.H[0]\}.
\end{equation*}
%Considering both forms of scalability (spatial and quality),
%the SQ-progression, that can be Pythonically-described by
%\begin{verbatim}
%progression = []
%for S_layer in range(N_SRL-1, 0, -1):
%  for Q_layer in range(N_QL-1, 0, -1):
%    progression.append((S_layer, Q_layer))
%\end{verbatim}

%For example, if $N_{\text{SRL}}=3$ (2 IPP...-type iterations) and
%$N_{\text{QL}}=2$, the progression of layers would be:
%\begin{verbatim}
%progression = [
%  (S_layer=2, Q_layer=1),  <-- First one to be decoded
%  (S_layer=2, Q_layer=0),
%  (S_layer=1, Q_layer=1),
%  (S_layer=1, Q_layer=0),
%  (S_layer=0, Q_layer=1),
%  (S_layer=0, Q_layer=0) ] <-- Last one
%\end{verbatim}

The use of quality scalability boosts the possibilities in real-time
streaming scenarios where the transmission bit-rate can be variable
(sending more or less Q-layers of a given spatial resolution depending
on the bit-rate). Notice that the SQ-progression is free of
drift-error.\footnote{Other progressions such as the QS-progression
  should generate drift because the decoder at some truncation points
  of the code-stream would have using a low-frequency information with
  a different quality than the encoderd used.}

%However, notice that the rendering of the frames at
%the decoder side with a smaller quality (and this will depends on the
%transmission bit-rate) will generate a drift-error in the
%reconstruction of the video because the predictions used at the
%encoder and the decoder are not identical.

%When only spatial scalability is used, this scheme is free of
%drift-error.

\section{What you have to do?}

Implement the previous codec, preferiblemente in a (or several)
Jupyter notebook(s). Test it by tracing a RD curve for different
$\Delta$ values for the quantizer (see Eqs. (j) and (k)). Compare it
with the curve generated without using MC.

\section{Timming}

In groups, you will present the results for this milestone during the
exam time.

\section{Deliverables}

The notebook(s) and the presentation.

\section{Resources}

\renewcommand{\addcontentsline}[3]{}% Remove functionality of \addcontentsline
\bibliography{video_compression,JPEG2000}
