\input{../../definitions}
\title{\SM{} - Study Guide - Milestone 12: IPP... coding in MRVC (Multi Resolution Video Codec)}

\maketitle

\tableofcontents

\section{Description}

\subsection{The IPP... decorrelation pattern}
It's time to put together all the ``tools'' that we have developed for
encoding a sequence of frames $\{V_k\}$. First, the sequence will be
splitted into GOFs
(\href{https://en.wikipedia.org/wiki/Group_of_pictures}{Group Of
  Frames}), and the structure of each GOF will be
IPP...~\cite{le1991mpeg}, which means that the first frame of each GOF
will be intra-coded (I-type), and the rest of frames of the GOF will
be predicted-coded (P-type), respect to the previous one\footnote{A
  P-type frame except for the second one, that always has a I-frame as
  reference.}. Notice that in an I-type frame all the coefficients
(\emph{coeffs} in short, remember that we are compensating the motion
in the DWT domain) will be I-type coeffs, and in a P-type frame, the
different coeffs will be I-type or P-type.

\subsection{A block diagram of the codec}

\begin{figure}
  \centering
  \svg{graphics/codec}{700}
  \caption{The IPP... MRVC video codec.}
  \label{fig:codec}
\end{figure}

The MRVC IPP... codec has been described in the
Fig.~\ref{fig:codec}. The equations that describe this system are:

\begin{equation}
  (V_k.L, V_k.H) \leftarrow \text{DWT}(V_k),
  \tag{a}
\end{equation}
where $\leftarrow$ denotes the
\href{https://en.wikipedia.org/wiki/Assignment_(computer_science)}{assignment}
operator, and $V_k$ is the $k$-th frame of the sequence,

\begin{equation}
  [V_k.L] \leftarrow \text{DWT}^{-1}(V_k.L, 0),
  \tag{b}
\end{equation}

\begin{equation}
  Z^{-1}([V_k.L]) = [V_{k-1}.L],
  \tag{c}
\end{equation}
and by definition, $Z^{-1}([V_{-1}.L]) = 0$.

\begin{equation}
  \overset{k+1\rightarrow k}{V} \leftarrow \text{ME}([V_k.L], [V_{k-1}.L]),
  \tag{d}
\end{equation}
where ME stands for Motion Estimation, and by definition,
$\overset{0\rightarrow (-1)}{V}=0$,

\begin{equation}
  [\hat{V}_k.L] \leftarrow \text{MCP}(\overset{k+1\rightarrow k}{V}, [V_{k-1}.L]),
  \tag{e}
\end{equation}
where MCP stands for Motion Compensated Prediction,

\begin{equation}
  [E_k.L] \leftarrow [V_k.L] - [\hat{V}_k.L],
  \tag{f}
\end{equation}

\begin{equation}
  \{[M_k],[S_k]\} \leftarrow \text{EW-min}([V_k.L], [E_k.L])
  \tag{g}
\end{equation}
where
\begin{equation}
  [M_k]_{i,j}=\text{min}([V_k.L]_{i,j}, [E_k.L]_{i,j}),
\end{equation}
and $[S_k]$ is a binary matrix defined by
\begin{equation}
  [S_k]_{i,j} = \left\{
  \begin{array}{lll}
    0 & \text{if}~[V_k.L]_{i,j} < [E_k.L]_{i,j} & \text{(I-type coeff)} \\
    1 & \text{otherwise}                      & \text{(P-type coeff)},
  \end{array}
  \right.
  \label{eq:matrix}
\end{equation}
(notice that $[T_k]$, that contains the element-wise minimum of both
matrices, is discarded)

\begin{equation}
  [V_k.H] \leftarrow \text{DWT}^{-1}(0, V_k.H),
  \tag{h}
\end{equation}

\begin{equation}
  [E_k.H] \leftarrow [V_k.H] - [\hat{V}_k.H],
  \tag{i}
\end{equation}
where, notice that
\begin{equation}
  [E_k.H] = \left\{
  \begin{array}{ll}
    {[}V_k.H{]}                 & \text{(I-type coeff)}\\
    {[}V_k.H{]} - [\hat{V}_k.H] & \text{(P-type coeff)},
  \end{array}
  \right.
\end{equation}
and notice that if $[\hat{V}_k.H]=0$ then $[E_k.H] = [V_k.H]$,

\begin{equation}
  [\tilde{E}_k.H] \leftarrow \text{Q}([E_K.H]),
  \tag{j}
\end{equation}

\begin{equation}
  [\tilde{E}_k.H] \leftarrow  \text{Q}^{-1}([\tilde{E}_K.H]),
  \tag{k}
\end{equation}

\begin{equation}
  [\tilde{V}_k.H] \leftarrow [\tilde{E}_k.H] + [\hat{V}_k.H],
  \tag{l}
\end{equation}
and notice that if $[\hat{V}_k.H]=0$ then $[\tilde{V}_k.H] =
[\tilde{E}_k.H]$,

\begin{equation}
  Z^{-1}([\tilde{V}_k.H]) = [V_{k-1}.H],
  \tag{m}
\end{equation}
and by definition, $Z^{-1}([V_{-1}.H]) = 0$,

\begin{equation}
  [\hat{V}_k.H] \leftarrow \text{MCP}(\overset{k+1\rightarrow k}{V}, [V_{k-1}.H]),
  \tag{n}
\end{equation}

\begin{equation}
  (0, \tilde{E}_k.H) \leftarrow \text{DWT}([\tilde{E}_k.H]),
  \tag{o}
\end{equation}

\begin{equation}
  \{V_k.L, \tilde{E}_k.H\} \leftarrow \text{E}(V_k.L, \tilde{E}_k.H),
  \tag{p}
\end{equation}
where E represents the entropy coding of both data sources, in two
different code-streams,

\begin{equation}
  (V_k.L, \tilde{E}_k.H) \leftarrow \text{E}^{-1}(\{V_k.L, \tilde{E}_k.H\}),
  \tag{q}
\end{equation}

\begin{equation}
  \tilde{V}_k \leftarrow \text{DWT}^{-1}(V_k.L, V_k.H),
  \tag{r}
\end{equation}

and

\begin{equation}
  [\tilde{E}_k.H] \leftarrow \text{DWT}^{-1}(0, \tilde{E}_k.H).
  \tag{s}
\end{equation}

This
\href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/12-IPP_coding/DPCM.ipynb}{notebook}
shows how to implement a simple DPCM codec.

\subsection{Multiresolution iterations}
In each IPP... iteration, only the high-frequency components of the
sequence of frames are decorrelated ($[H]$ subbands) considering the
information provided by the low-frequency components ($[L]$ subbands),
which are losslessly\footnote{The $[L]$ information must be always
decoded with the quality that the encoder used for reconstruct the
corresponding SRL.} transmitted between the encoder and the decoder.

In order to increase the compression ratio, a minimum number of
IPP... iterations should be carried out, each one generating a new
Spatial Resolution Level (SRL). In fact, the number of iterations
controls the number of SRLs $N_{\text{SRL}}$ generated, where
\begin{equation}
  N_{\text{SRL}} = \text{the number of IPP... iterations} + 1.
\end{equation}
The output of an IPP... iteration will be refered as ``a
spatial-layer of code-stream'', or simply as ``a S-layer''.

\subsection{Scalability}
It's easy to see that an IPP...-type MRVC sequence is progressive in
Spatial-resolution (S-progression) as long as more S-layers are
received. Moreover, Quality (Q-progression) scalability in each SRL
can be achieved in the encoding of the $[H]$ texture if a
quality-scalable image codec such as
JPEG2000~\cite{taubman2002jpeg2000} is used, generating a number
$N_{\text{QL}}$ of quality-layers by each $[H]$ subband. Notice,
however, that using this encoding configuration, the only
allowed\footnote{Suposing that the order of the layers could be
modified in the code-stream.} progression is the SQ-progression, that
can be Pythonically-described by
\begin{verbatim}
progression = []
for S_layer in range(N_SRL-1, 0, -1):
  for Q_layer in range(N_QL-1, 0, -1):
    progression.append((S_layer, Q_layer))
\end{verbatim}

For example, if $N_{\text{SRL}}=3$ (2 IPP...-type iterations) and
$N_{\text{QL}}=2$, the progression of layers would be:
\begin{verbatim}
progression = [
  (S_layer=2, Q_layer=1),  <-- The first one to be recived
  (S_layer=2, Q_layer=0),
  (S_layer=1, Q_layer=1),
  (S_layer=1, Q_layer=0),
  (S_layer=0, Q_layer=1),
  (S_layer=0, Q_layer=0) ] <-- The last one
\end{verbatim}

The use of quality scalability boosts the possibilities in real-time
streaming scenarios where the transmission bit-rate can be variable
(sending more or less quality-layers depending on the
bit-rate). However, notice that the rendering of the
frames at the decoder side with a smaller quality (and this will
depends on the transmission bit-rate) will generate a drift-error in
the reconstruction of the video because the predictions used at the
encoder and the decoder are not identical.

When only spatial scalability is used, this scheme is free of
drift-error.

\section{What you have to do?}

Implement the previous codec, preferiblemente in a (or several)
Jupyter notebook(s). Test it by tracing a RD curve for different
$\Delta$ values for the quantizer (see Eqs. (j) and (k)). Compare it
with the curve generated without using MC.

\section{Timming}

In groups, you will present the results for this milestone during the
exam time.

\section{Deliverables}

The notebook(s) and the presentation.

\section{Resources}

\renewcommand{\addcontentsline}[3]{}% Remove functionality of \addcontentsline
\bibliography{video_compression,JPEG2000}
