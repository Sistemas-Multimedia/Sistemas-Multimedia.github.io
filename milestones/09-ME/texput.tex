\input{../../definitions}
\title{\SM{} - Study Guide - Milestone 10: Motion Estimation}

\maketitle

\tableofcontents

\section{Description}

\href{https://en.wikipedia.org/wiki/Motion_estimation}{ME (Motion
  Estimation)} is the process of determining the MVs (Motion Vectors)
that describe the mapping of the pixels from one frame (2D image) to
another.

\subsection{Motion Estimation (ME) for what?}
Temporal correlation between video frames\footnote{Remember that,
although this discussion will deal with frames, in our particular
case, we will decorrelate subbands.} can be removed by MC (Motion
Compensation). MC implies to decrease (usually by substracting a
prediction frame) the amount of information in the frames. The removed
information must be available at both, the encoder and the decoder
side, in order to make this a reversible process.

Specifically, a MCP (MC Predictor) inputs one (or more) reference
frame(s) ${\mathbf R}=\{{\mathbf R}_i\}$, and a motion vectors field
$\overset{{\mathbf R}\rightarrow{\mathbf P}}{\mathbf V}$ that
indicates how to project ${\mathbf R}$ onto the predicted frame ${\mathbf P}$, and outputs
a prediction frame
\begin{equation}
  \hat{{\mathbf P}} =  \overset{{\mathbf R}\rightarrow {\mathbf P}}{\mathbf V}({\mathbf R}).
  \label{eq:MCP1}
\end{equation}
In this milestone we analyze different algorithms to determine
$\overset{{\mathbf R}\rightarrow {\mathbf P}}{\mathbf V}$ (in a futher
milestone we will see how to remove the $\hat{{\mathbf P}}$'s
information from ${\mathbf P}$). At this moment, for the sake of
simplicity, in the rest of this discussion it will be supposed that the
number of reference frames in only 1.
%(probably, the closest one to the
%predicted frame), and therefore, we don't need to specify which are
%the reference images to make the prediction. For this reason,
%Eq,~\ref{eq:MCP1} can be rewritten as
%\begin{equation}
%  \hat{\mathbf W}_k = \overset{{\mathbf W}_{k-1}\rightarrow {\mathbf
%  W}_{k}}{\mathbf V}({\mathbf W}_{k-1}) = \overset{(k-1)\rightarrow
%          k}{\mathbf V}({\mathbf W}_{k-1})
%%  \hat{{\mathbf P}_k} = \overset{{\mathbf P}_k\rightarrow {\mathbf
%%  P}_{k-1}}{\mathbf V}({\mathbf P}_{k-1}) = \overset{k\rightarrow
%%          k-1}{\mathbf V},
%  \label{eq:MCP2}
%\end{equation}

\subsection{But ... what exactly do we need?}
Our main objective is to minimize the differences (for example, the
\href{https://en.wikipedia.org/wiki/Euclidean_distance}{L$_2$
  distance}) between ${\mathbf P}$ (the predicted frame) and $\hat{\mathbf P}$ (the
prediction frame), i.e. minimizing
\begin{equation}
  {\mathbf E} = {\mathbf P} - \hat{\mathbf P}
\end{equation}
in order to get that ${\mathbf E}$ will be more compressible than
${\mathbf P}$. To achieve this, we can compute $\overset{{\mathbf
    R}\rightarrow {\mathbf P}}{\mathbf V}$ that simply minimizes the
L$_2$ energy of ${\mathbf E}$, $||{\mathbf E}||^2$, or we can compute
a $\overset{{\mathbf R}\rightarrow {\mathbf P}}{\mathbf V}$ that also describes the Optical
Flow~\cite{horn1981determining} (OF) between the pixels of ${\mathbf
  R}$ and ${\mathbf P}$, that although not necessarily has to
minimize $||{\mathbf E}||^2$, tries to show the true movement of the
pixels between the both frames. This second option has the advantage
of generating more visually pleasing reconstructions when the
code-stream is partially received and makes easier to predict the
content of the motion fields.

The first type of techniques are simply called ``ME techniques'', and
are usually faster\footnote{Obviously, depending on the algorithm.}
than the second type, based on the estimation of the OF.

%Let's see two basic techniques to estimate the motion between 2
%frames, $R$ and $P$. In this discussion it will be supposed that the
%motion of the objects that are in both frames is bounded, and that the
%luminance varies smoothly between adjacent frames.

Now, let's see some of the most used techniques for estimating the
motion between two frames. Notice that, in general, better estimations
can be found if we suppose motion models such as that the objects
exhibit
\href{https://en.wikipedia.org/wiki/Inertia}{inertia}. However, this
case will not be considered for now.

\subsection{\href{https://vicente-gonzalez-ruiz.github.io/video_compression/\#x1-40003}{Block-based motion estimation}}

\begin{figure}
  \centering
  \svg{graphics/simple}{400}
  \caption{ME using disjoint blocks. $({\mathbf V}_x, {\mathbf V}_y)$
    is the motion vector that indicates where the block $(x,y)$ of
    ${\mathbf P}$ is found in ${\mathbf R}$.}
  \label{fig:simple}
\end{figure}

Block-based ME is the simplest ME algorithm (see the
Fig.~\ref{fig:simple}), ${\mathbf P}$ is divided in blocks of (for
example) 16x16 pixels, and we can use the (R)MSE that measures the
distance in L$_2$ (also known as the Euclidean distance) between each
block of ${\mathbf P}$ and its surrounding pixels in ${\mathbf R}$
(the so called search area)~\cite{zhu2000new}. For each block, a
motion vector that indicates the best match (smaller distance) is
found. The set of motion vectors form the motion vectors field
$\overset{{\mathbf R}\rightarrow {\mathbf P}}{\mathbf V}$ that
obviously, except for a block size of 1x1, will be less dense than
${\mathbf R}$ and ${\mathbf P}$. Notice, however, that, it is not a
good idea to use such a small block size because, in general, the
motion vectors will not describe the true motion in the scene.

\begin{figure}
  \centering
  \png{stockholm_R_block}{800}
  \caption{A tile of the first image of the \emph{Stockholm}
    sequence. This is the reference (${\mathbf R}$) frame.}
  \label{fig:R_block}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_P_block}{800}
  \caption{The same (coordinates) tile of the second image of the
    \emph{stockholm} sequence. This is the predicted (${\mathbf P}$)
    frame.}
  \label{fig:P_block}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_PR_block}{800}
  \caption{${\mathbf P} - {\mathbf R}$: shows the differences between
    both tiles. The entropy of the residue is displayed between
    parentheses.}
  \label{fig:RP_block}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_hat_P_block}{800}
  \caption{The prediction frame (${\hat{\mathbf P}}$). See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/full_search_block_ME.ipynb}{this}.}
  \label{fig:hat_P_block}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_error_block}{800}
  \caption{The prediction-error frame
    (${\mathbf R} - {\hat{\mathbf P}}$). See
    \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/full_search_block_ME.ipynb}{this}.}
  \label{fig:error_block}
\end{figure}

As it can be seen in the Figures \ref{fig:R_block}, \ref{fig:P_block},
\ref{fig:RP_block}, \ref{fig:hat_P_block}, and \ref{fig:error_block}, the MVs generated
by block-based ME can significantly decrease the entropy.

\begin{figure}
  \centering
  \png{stockholm_MVs_block}{800}
  \caption{Motion vectors to map ${\mathbf P}$ (which is divided into
    disjoint blocks) onto ${\mathbf R}$. See
    \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/full_search_block_ME.ipynb}{this}.}
  \label{fig:MVs_block}
\end{figure}

However, as it can be seen in the Figure~\ref{fig:MVs_block}, the
motion information computed by the block-based ME algorithm not always
represents the true motion in the scene in the case of using
block-based matching. This can be a drawback, for example, for solving
object tracking problems. In the case of video coding, the main
disadvantage of such issue is that the entropy of the motion fields
increases, which also decreases the compression ratio.

\subsection{Overlapped block matching}

\begin{figure}
  \centering
  \svg{graphics/overlaped}{400}
  \caption{ME using overlaped blocks.}
  \label{fig:overlaped}
\end{figure}

A better approximation to the OF for small block sizes can be found if
we allow the blocks to overlap in ${\mathbf
  P}$~\cite{orchard1994overlapped}, case in which the block size for
performing the comparisons must be larger. Again, as it happens with
the disjoint case, only the non overlaped pixels are used for building
the prediction (see the Fig.~\ref{fig:overlaped}). Obviously, the main
drawback of this technique is that it can be more computationally
demanding than the previous one.

\begin{figure}
  \centering
  \png{stockholm_hat_P_dense}{800}
  \caption{The prediction frame (${\hat{\mathbf P}}$). See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/full_search_dense_ME.ipynb}{this}.}
  \label{fig:hat_P_dense}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_error_dense}{800}
  \caption{The prediction error frame (${\mathbf R} - {\hat{\mathbf P}}$). See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/full_search_dense_ME.ipynb}{this}.}
  \label{fig:error_dense}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_MVs_dense}{800}
  \caption{Motion vectors to map ${\mathbf P}$ (from which each pixel has been mapped) onto ${\mathbf R}$. See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/full_search_dense_ME.ipynb}{this}.}
  \label{fig:MVs_dense}
\end{figure}

The dense ME algorithm can obtain better predictions than the
block-based one, as it can be seen in the
Figures~\ref{fig:hat_P_dense} and \ref{fig:error_dense}. The MVs are
also more coherent (see Figure~\ref{fig:MVs_dense}).

\begin{figure}
  \centering
  \svg{graphics/average}{400}
  \caption{ME using overlaped blocks, averaging the overlaped pixels.}
  \label{fig:average}
\end{figure}

An improvement of the previous technique can also average the
overlaped pixels in the prediction frame $\hat{P}$, as it has been
shown in the Fig.~\ref{fig:average}.

\subsection{ME in a transformed domain}

\begin{figure}
  %\begin{tabular}{cccccc}
  %  \png{one} & \png{x} & \png{y} & \png{x2} & \png{y2} & \png{xy} \\
  %\end{tabular}
  \begin{tabular}{cccccc}
    \png{one}{200} & \png{x}{200} & \png{y}{200} & \png{x2}{200} & \png{y2}{200} & \png{xy}{200} \\
    No motion & Constant velocity in $X$ & Constant velocity in $Y$ & Constant acceleration in $X$ & Constant acceleration in $Y$ & Constant accelarion in diagonal
  \end{tabular}
  \caption{Correlation kernels (basis functions) used by the
    \emph{polynomial expansion} of the Farneb{\"a}ck's ME
    algorithm. See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/farneback_ME.ipynb}{this}. The analized motion is depicted below the plot of each basis.}
  \label{fig:FarnebacK_basis}
\end{figure}

The motion can be estimated also in a transformed domain. One of these
estimators is the Farneb{\"a}ck's algorithm~\cite{farneback2003two},
which uses the transform defined by the basis functions
\begin{equation}
    \{1, x, y, x^2, y^2, xy\}
\end{equation}
(see the Figure~\ref{fig:FarnebacK_basis}). In this transform domain,
which is applied by overlapped regions, the corresponding subbands
quantify the tendency of the image to increase its intensity in
different 2D directions, and therefore, it is more efficient to know
the direction in which the objects are moving.

\begin{figure}
  \centering
  \png{stockholm_hat_P_farneback}{800}
  \caption{The prediction frame (${\hat{\mathbf P}}$). See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/farneback_ME.ipynb}{this}.}
  \label{fig:hat_P_farneback}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_error_farneback}{800}
  \caption{The prediction error frame (${\mathbf R} - {\hat{\mathbf P}}$). See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/farneback_ME.ipynb}{this}.}
  \label{fig:error_farneback}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_MVs_farneback}{800}
  \caption{Motion vectors to map ${\mathbf P}$ (from which each pixel has been mapped) onto ${\mathbf R}$. See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/farneback_ME.ipynb}{this}.}
  \label{fig:MVs_farneback}
\end{figure}

The Farneback's ME is a dense ME, and it provides subpixel
interpolation because the MVs are real numbers (see the
Figures~\ref{fig:hat_P_farneback}, \ref{fig:error_farneback} and
\ref{fig:MVs_farneback}). Notice that the prediction is the best of
the all tested algorithms, probably by the subpixel accuracy.

\subsection{Machine learning}
ANNs (Artifical Neural Networks) can be trained to estimate the motion
between frames~\cite{dosovitskiy2015flownet}. For the training of
ANNs, animation videos are generally used where the motion fields are
known with precision.

\begin{comment}
\subsection{Motion compensation in action}
\begin{figure}
  \svg{R}{1000}
  \svg{P}{1000}
  \svg{y_motion}{1000}
  \svg{x_motion}{1000}
  \svg{hat_P}{1000}
  \svg{without_ME}{1000}
  \svg{with_ME}{1000}
  \caption{Effect of ME (using OF) over the temporal redundancy
    removal.}
  \label{fig:MC}
\end{figure}
The Fig.~\ref{fig:MC} shows an example the performace of the use of
OF, comparing the prediction error generated with and without ME. In
this experiment, a motion vector has been computed between each point
of ${\mathbf P}$ and ${\mathbf R}$. As it can be seen, the OF can
reduce the temporal redundancy significantly.
\end{comment}

\begin{comment}
The OF~\cite{horn1981determining} tries to establish connections between the pixels of
the frames $P$ and $R$ supposing that:
\begin{enumerate}
\item $P$ and $R$ are adjacent in time (if $R$ was taken at time $t$,
  $P$ is taken at time $dt+t$) and therefore, similar in
  content.
\item Similarity between images implies that the pixels in both
  frames, $R$ and $P$, will have the same luminance. If $I(x,y,t)$
  measures the luminance of the pixel $(x,y)$ of the frame $R$,
  similarity can be modeled by
  \begin{equation}
    I(x+dx, y+dy, t+dt) = I(x,y,t),
    \label{eq:similarity}
  \end{equation}
  where $I(x+dx, y+dy, t+dt)$ is the corresponding pixel in the frame
  $P$. The first part of the Eq.~\ref{eq:similarity} can be also
  computed by (using the first-order Taylor expansion) as
  \begin{equation}
    I(x+dx, y+dy, t+dt) = I(x,y,t) + \frac{\partial I}{\partial x}dx + \frac{\partial I}{\partial y}dy + \frac{\partial I}{\partial t}dt,
    \label{eq:taylor_exp}
  \end{equation}
  andtherefore, it must be true that
  \begin{equation}
    \frac{\partial I}{\partial x}dx + \frac{\partial I}{\partial y}dy + \frac{\partial I}{\partial t}dt = 0.
    \label{eq:constraint}
  \end{equation}
  Dividing by $dt$, we finally get that
  \begin{equation}
    \frac{\partial I}{\partial x}\frac{dx}{dt} + \frac{\partial I}{\partial y}\frac{dy}{dt} + \frac{\partial I}{\partial t} = 0.
  \end{equation}
\item Adjacent pixels follow parallel
  trajectories~\cite{horn1981determining}, with basically means that
  neighbor pixels will have similar motion.
\end{enumerate}
\end{comment}

\section{What do I have to do?}

\begin{figure}
  \centering
  \myfig{graphics/problem}{3cm}{300}
  \caption{Basic encoding problem.}
  \label{fig:problem}
\end{figure}

Using the encoding system described in the Figure~\ref{fig:problem}, and defined by
\begin{equation}
  \begin{bmatrix}
    \tilde{\mathbf R} \\
    \tilde{\mathbf E}
  \end{bmatrix} =
  \begin{bmatrix}
    \text{Q}_{\mathbf R}(\cdot) & 0 \\
    -\overset{{\mathbf R}\rightarrow {\mathbf P}}{\text{ME}}(\text{Q}_{\mathbf E}(\cdot)) & 1
  \end{bmatrix}
  \begin{bmatrix}
    {\mathbf R } \\
    {\mathbf P}
  \end{bmatrix}
  \label{eq:forward}
\end{equation}
and
\begin{equation}
  \begin{bmatrix}
    \tilde{\mathbf R} \\
    \tilde{\mathbf P}
  \end{bmatrix} =
  \begin{bmatrix}
    \text{Q}^{-1}_{\mathbf R}(\cdot) & 0 \\
    \overset{{\mathbf R}\rightarrow {\mathbf P}}{\text{ME}}(\text{Q}^{-1}_{\mathbf E}(\cdot)) & 1
  \end{bmatrix}
  \begin{bmatrix}
    \tilde{\mathbf R} \\
    \tilde{\mathbf E},
  \end{bmatrix}
  \label{eq:backward}
\end{equation}
find $\text{Q}_{\mathbf{R}}$ and $\text{Q}_{\mathbf{E}}$ that minimize in the RD domain (the RD curve of)
\begin{equation}
  \text{MSE}(\{\mathbf{R},\mathbf{P}\},\{\hat{\mathbf{R}},\hat{\mathbf{P}}\}) = \frac{\text{MSE}({\mathbf R},\hat{\mathbf R}) + \text{MSE}({\mathbf P},\hat{\mathbf P})}{2},
\end{equation}
set that
\begin{equation}
  \text{MSE}({\mathbf R},\hat{\mathbf R}) = \text{MSE}({\mathbf P},\hat{\mathbf P}).
  \label{eq:constant_quality}
\end{equation}
Equation~\ref{eq:constant_quality} indicates that all the decoded
frames should have the same distortion. Notice that the transform
defined by the Equations ~\ref{eq:forward} and \ref{eq:backward} is
not orthogonal and therefore, the ``subbands'' $\tilde{\mathbf R}$ and
$\tilde{\mathbf P}$ are not independent.

\begin{comment}
In this
\href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/optical_flow.ipynb}{notebook}
you can find how to estimate the OF between two frames. Please, modify
it to find suitable values for the parameters \texttt{levels},
\texttt{winsize} and \texttt{iterations}. Supposing that the impact of
each parameter is independent from the rest, a way of comparing two
different configurations is to trace RD curves using a spatial
decorrelation, quantization and compressing the MC residues.
\end{comment}

\section{Timming}

Please, finish this milestone before the next class session.

\section{Deliverables}

None.

\section{Resources}

\renewcommand{\addcontentsline}[3]{}% Remove functionality of \addcontentsline
\bibliography{motion-estimation}
