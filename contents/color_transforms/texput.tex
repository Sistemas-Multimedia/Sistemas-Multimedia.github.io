% Emacs, this is -*-latex-*-

% https://www.youtube.com/watch?v=ZCDF9f1Wo0Q ( Self-Organizing Maps for Color Quantization (python) )
% https://stackoverflow.com/questions/57817645/how-do-i-re-train-an-existing-k-means-clustering-model
% https://www.deeplearningbook.org/
% https://www.researchgate.net/profile/Wenpeng-Ding/publication/242268750_Rate-distortion_optimized_color_quantization_for_compound_image_compression_-_art_no_65082Q/links/5448579f0cf2f14fb8142446/Rate-distortion-optimized-color-quantization-for-compound-image-compression-art-no-65082Q.pdf?origin=publication_detail (RATE-DISTORTION OPTIMIZED COLOR QUANTIZATION FOR COMPOUND IMAGE COMPRESSION)
% https://pyclustering.github.io/docs/0.8.2/html/da/d22/classpyclustering_1_1cluster_1_1kmeans_1_1kmeans.html#details
% https://ieeexplore.ieee.org/document/839021 (Centroid neural network for unsupervised competitive learning)
% https://github.com/tranleanh/centroid-neural-networks
% https://towardsai.net/p/l/centroid-neural-network-and-vector-quantization-for-image-compression
% https://pub.towardsai.net/centroid-neural-network-an-efficient-and-stable-clustering-algorithm-b2fa8cbb2a27
% https://towardsai.net/p/l/centroid-neural-network-for-clustering-with-numpy
% https://matthew-parker.rbind.io/post/2021-01-16-pytorch-keras-clustering/
% https://theswissbay.ch/pdf/Gentoomen%20Library/Information%20Theory/Compression/
% https://scikit-learn-extra.readthedocs.io/en/stable/modules/cluster.html
%

\input{../../definitions}
\title{Color Transforms}

\maketitle
\tableofcontents

\section{Spectral decorrelation}

Most image and video compressors exploit the statistical (and also
perceptual\footnote{This will be explained latter in this course.})
correlation between the $\text{RGB}$ color
\emph{components}\footnote{A component of a pixel in the $\text{RGB}$
domain refer to one of the values $\text{R}$ (red), $\text{G}$ (green)
and $\text{B}$ (blue) coordinates in the $\text{RGB}$ color 3D space.}
of the pixels, using a color transform.

Color transforms are pixel-wise operators. As a result, each pixel is
represented in a different domain where (usually) three new
\emph{coefficients}\footnote{Most part of the transforms, including
the color ones, analyze the signal information from a frequency
perspective, generating the so called coefficients whose index in the
transform domain is related to a different frequency of the signal.}
express the same\footnote{In general, the color transforms can be
considered lossless, although this is only true if fixed-point
arithmetic is used.} information but in a different color domain.

\section{Luma and chromas}

Most color transforms are designed to split the color information of a
pixel into \emph{luminance} (luma) and \emph{chrominance}
(chroma). The luma is basically the low frequency\footnote{It is worth
  understanding that the frequency concept in the color transform
  domain is not related to the frequency concept in the original pixel
  domain. For example, the $\text{R}$ component or a pixel represents
  the amount of red in the pixel, and in the visible spectrum we are
  refering to frequencies that are lower than the frequency that the
  $\text{G}$ and $\text{B}$ components represent. However, in a color
  transformed domain, the luma measures the brightness level of the
  pixel, and we cannot found a subband of frequencies in the visible
  spectrum that can represent such information because we are using a
  different representation domain.} information of the pixel, and the
chroma (logically) high frequency information.

For example, in
\href{https://en.wikipedia.org/wiki/JPEG#JPEG_codec_example}{JPEG} and
\href{https://en.wikipedia.org/wiki/Advanced_Video_Coding#Fidelity_range_extensions_and_professional_profiles}{H.264/AVC}
the color information of each pixel is transformed from the
$\text{RGB}$ color space to the $\text{YCrCb}$ color space, and in
\href{https://en.wikipedia.org/wiki/JPEG_XR#Description}{JPEG XR}, the
color space $\text{YCoCg}$is used. In these luma-based color spaces,
$\text{Y}$ represents the luma (coefficient) of the pixel. The other
two coefficients form the chroma. Note that the chrominance of a pixel
is determined by two chromas.

\subsection*{Channels and subbands (a word about notation)}

Apart from using the terms of component and coefficient, we will use
the word \emph{channel} to refer to the same index component of all
the pixels of an image (or video), and \emph{subband} to denote the
same index coefficient generated after the transformation of all
pixels of an image (or video). For example, the $\mathbf{R}$ channel
of a color image corresponds to the \emph{monochromatic}
image\footnote{That can be considered as a
  \emph{single-channel}/\emph{mono-component}/\emph{scalar} image.}
generated by the $\text{R}$ component of all pixels of the image, and,
for example, the $\mathbf{Y}$ subband of a transformed ($\text{RGB}$)
image corresponds to the $\text{Y}$ coefficients of all pixels (also a
``monochromatic'' image).

In image and video coding, most color transforms map 3 channels
($\text{RGB}$) into 3 subbands.

\section{Benefits of color transforms}

Color transforms applied to natural visual information generally have
two key advantages:
\begin{enumerate}
\item \textbf{Energy concentration:} In general, transforms ``move''
  the energy\footnote{In general, the information provided by the
    signals.} between subbands, accumulating most of the energy in a
  reduced number of them (aspect related to the so-called coding gain
  of the transform~\cite{vruiz__transform_coding}). In our case, where
  the transformations are between color spaces, in the transform
  domain most of the energy is concentrated in the $\text{Y}$
  subband. As a consequence of this, usually, \textbf{the
    entropy~\cite{vruiz__information_theory} is decreased} and
  \textbf{the dynamic range of the signal is increased}. The first
  fact means that we will encode more information\footnote{For the same
    bit-rate.}, and the second that we will be able to use a higher
  range of quantization step
  sizes~\cite{vruiz__scalar_quantization,sayood2017introduction},
  increasing also the number of feasible points in the RD
  curve~\cite{vruiz__information_theory}.
\item \textbf{Luma/chroma analysis:} Our visual system is more
  sensitive in terms of spatial resolution to the luma
  (``black-and-white'') information than to the chroma (``color'')
  information, which basically means that we can quantize more the
  chroma\footnote{Notice again, that we will study this effect in a
  posterior session.} without generating perceptual distortion. 
\end{enumerate}

\section{Scalar quantization in the color transform domain}

If the color transform is orthogonal or biorthogonal, that is, the
luma and the cromas are independent features of the
signal\footnote{The luna does not define the chroma and viceversa.},
the quantization noise generated in the subbands is additive respect
to the reconstructed signal~\cite{burger2016digital}. Therefore, from
a pure RD point of view, the quantization step sizes for each subband
should be selected using the same RD slope in all subbands (see the
notebook
\href{https://github.com/vicente-gonzalez-ruiz/color_transforms/blob/main/docs/RGB/RGB_SQ.ipynb}{Scalar
  Quantization of RGB images}. Notice that this implies to compute the
RD curves.

Therefore, taking a generic luma-chroma transform $\text{YUV}$, we
expect that
\begin{equation}
  \lambda^{\text{Y}} \approx \lambda^{\text{U}} \approx \lambda^{\text{V}}
  \label{eq:optimal_lambda}
\end{equation}
for a given quantization step sizes
\begin{equation}
  \Delta^{\text{Y}} = \Delta^{\text{U}} = \Delta^{\text{V}}
  \label{eq:optimal_lambda}
\end{equation}
the RDO~\cite{vruiz__information_theory} can be ignored. In the
notebook
\href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/contents/RGB_SQ/RGB_SQ.ipynb}{Scalar
  Quantization of RGB images} we can explore (at least visually) the
grade of compilance of Eq.~\eqref{eq:optimal_lambda}.

\subsection*{Resources}
See the notebooks
\href{https://github.com/vicente-gonzalez-ruiz/color_transforms/blob/main/docs/3DCT/3DCT_over_RGB.ipynb}{Removing
  RGB redundancy with the DCT},
\href{https://github.com/vicente-gonzalez-ruiz/color_transforms/blob/main/docs/YCoCg/YCoCg_over_RGB.ipynb}{Removing
  RGB redundancy with the $\text{YCoCg}$ transform} and
\href{https://github.com/vicente-gonzalez-ruiz/color_transforms/blob/main/docs/YCrCb/YCrCb_over_RGB.ipynb}{Removing
  RGB redundancy with the $\text{YCrCb}$ transform}.

\section{Vector quantization applied to the $\text{RGB}$ domain}

SQ (Scalar
Quantization)~\cite{vruiz__scalar_quantization,sayood2017introduction}
would be an optimal solution only if the image colors are uniformly
distributed within
\href{https://en.wikipedia.org/wiki/RGB_color_model}{the RGB
  cube}. However, the typical color distribution in natural images is
anything but uniform, with some regions of the color space being
densely populated and many potentially used colors completely
missing. In this case, depending on the quantization step
size~\cite{vruiz__signal_quantization}, SQ could be suboptimal because
the colors used may not be sampled with sufficient density, while at the
same time the encoding system considers colors that do not appear
in the image at all~\cite{burger2016digital}.

On the other hand, VQ (Vector
Quantization)~\cite{vruiz__vector_quantization,sayood2017introduction}
applied to the color domain does not treat the individual $\text{RGB}$
components separately as does scalar quantization, but each color
vector used ${\mathbf C}_i = (\text{R}_i, \text{G}_i, \text{B}_i )$ in
the image is treated as a minimum structure. VQ determines a code-book
of $K$ code-vectors (centroids) that minimizes the distortion between
the original image and the reconstructed one. Notice that the
code-book must be known by the decoder to find a reconstruction.

\subsection*{Resources}
\begin{enumerate}
\item \href{}{YCrCb.py}: Implementation in VCF.
\item \href{https://github.com/Sistemas-Multimedia/VCF/blob/main/notebooks/YCrCb.ipynb}{YCrCb.ipynb}: Notebook showing the use of YCrCb.py.
\item \href{}{YCoCg.py}: Implementation in VCF. 
\item \href{https://github.com/Sistemas-Multimedia/VCF/blob/main/notebooks/YCoCg.ipynb}{YCoCg.ipynb}: Notebook showing the use of YCoCg.py.
\item See the notebooks
\href{https://github.com/vicente-gonzalez-ruiz/vector_quantization/blob/main/docs/RGB_VQ.ipynb}{Vector
  Quantization (in the color domain) of a RGB image} and
\href{https://github.com/vicente-gonzalez-ruiz/vector_quantization/blob/main/docs/spatial_color_VQ.ipynb}{Vector
  Quantization (in the 2D domain) of a color (RGB) image}.
\end{enumerate}

\section*{To-Do}
\begin{enumerate}
\item Create a new module named \texttt{color\_DCT.py} that use the
  $\text{DCT}$ to exploit the $\text{RGB}$ redundancy in images.
\item Create a new module named \texttt{color\_VQ.py} that use the
  Vector Quantization to exploit the $\text{RGB}$ redundancy in images.
\end{enumerate}

\section{References}

\renewcommand{\addcontentsline}[3]{}% Remove functionality of \addcontentsline
\bibliography{maths,data_compression,signal_processing,DWT,image_compression,image_processing,information_theory,quantization}
