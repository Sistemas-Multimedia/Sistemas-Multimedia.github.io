% Emacs, this is -*-latex-*-

\input{../../definitions}
\title{\SM{} - \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/tree/master/contents/perceptual_coding}{Perceptual Coding}}

\maketitle
\tableofcontents

\section{What is perceptual coding?}

So far, we have focused on minimizing the
Lagrangian~\cite{sullivan1998rate}
\begin{equation}
  J = R + \lambda D,
  \label{eq:RD}
\end{equation}
where $R$ is the data rate, and $D$ is an additive\footnote{The total
distortion of two (or more) sources of distortion is the sum of the
distortions of these two (or more) sources.} distance metric, such as
the RMSE, the PSNR or the SSIM index~\cite{wang2004image}. However, the
way in which human beings perceive distortion is generally different
from how these metrics express it. This chapter introduces some of the
most common ways of exploiting the visual distortion perceived by
humans.

Notice that if, according to the requirements of the encoding process, $D$ is below
a Threshold of Noticeable Distortion (ToND), the RDO process described
by Eq.~\eqref{eq:RD} boilds down to select the option with smaller $R$.

\section{ToND varies with the luma intensity}

The Weber-Fechner law states that the minimum perceivable visual
stimulus difference increases with background
luminance\footnote{In general, this is not true for the
chroma.}~\cite{naccari2014perceptually}, up to a point in which it
decreases. Therefore, the perception of the distortion generated by
the lossy coding of an image is smaller in areas with higher and
lower intensity values. For this reason, one of the most used
quantizers is the deadzone, which also in general changes signal noise
(for example, electronic noise) by quantization noise, where the SNR
of the signal is smaller (arroung 0).

\section{ToND varies with the spatial frequency}
The HVS can be modeled as a low-pass filter whose cutoff frequency
depends on the distance between the observer and the content (in terms
of frequency) of the image.

Some DCT-based image and video coding standards, such as JPEG and
H.264, define quantization matrices designed for perceptual
coding~\cite{ernawan2014optimal}. These matrices indicate a different
quantization step size for each 8x8-DCT coefficient, whose values were
found through a study of the subjective impact of the quantization of
each coefficient in the ToND. In the case of H.264, such matrices can
change between images~\cite{naccari2014perceptually}.

In the case of JPEG2000, each subband uses a different quantization
step size~\cite{liu2020visibility}. However, note that these values
depend on the selected DWT filter.

\section{Visual masking of the quantization noise}

Quantization noise is generated by the quantizer, generating
different coding artifacts\footnote{``Random'' noise, blocking,
  ringing, etc.}, which are hardly perceived when the (area of the)
encoded image is textured~\cite{wu2017digital}. This effect can
occur up to the ToND.

Another important aspect of our perception is its directionality,
which leads the HVS to be more sensitive to distortions added to
horizontal and vertical frequencies than to diagonal
frequencies~\cite{naccari2014perceptually}.

Finally, the rationale behind temporal masking is that the HVS
sensitivity to coding artifacts is lower in areas with very high
motion activity.

In video, modeling temporal masking is more challenging because
the spatio-temporal sensitivity function of the HVS is not separable,
i.e., it depends on both the spatial and temporal
frequencies~\cite{naccari2014perceptually}.

\section{Loop filters}

Loop filters are used in motion-compensated video codecs to improve visual quality (and also the encoding RD performance). For
example, H.264/AVC uses (usually directional\footnote{Anisotropic.})
deblocking filters in the encoding loop to smooth the transitions
between the blocks, when the boundaries between them become
perceptible.

\section{Luma redundancy}

The HVS can perceive only a finite number of different intensities
(luma). This number depends on the dynamic range of the pixels, but, in
general, we are unable to distinguish more than 64 intensity
values~\cite{vruiz__visual_redundancy}.

\section{Chroma redundancy}

Humans do not perceive detail in chrominance as well as in luminance~\cite{burger2016digital}. For this reason, the
croma can be downsampled to 1/4 of the original
\href{https://en.wikipedia.org/wiki/Sampling_(signal_processing)}{sampling
  rate} without noticeable distortion. This feature is used in some of
the last image and video compression systems such as
\href{https://en.wikipedia.org/wiki/JPEG_XR#Description}{JPEG XR} and
\href{https://en.wikipedia.org/wiki/High_Efficiency_Video_Coding#Video_coding_layer}{HEVC}.

\section{Resources}
\begin{enumerate}
\item \href{https://github.com/vicente-gonzalez-ruiz/color_transforms/blob/main/docs/color_redundancy.ipynb}{Spectral
    (color) redundancy}.
\end{enumerate}

\section{To-Do}
\begin{enumerate}
\item Modify the VCF compression pipeline to take advantage of the chroma
  redundancy. Use different quantization step sizes for each color
  subband. Complexity 2.
\item Use the
  \href{http://www.jatit.org/volumes/Vol70No3/24Vol70No3.pdf}{default
    quantization matrices of JPEG} in the DCT pipeline of
  VCF. Complexity 4.
\item Do the same thing, but using the DWT. Complexity 4.
\item The
  \href{https://scikit-image.org/docs/stable/auto_examples/filters/plot_entropy.html}{local
    entropy} of the motion vectors can be a good estimation of the
  motion complexity in a video sequence. In a motion compensated video
  coding pipeline in VCF, adapt the quantization step size to the
  local entropy, trying to increase the compression ratios without
  increasing the perceived distortion. Compexity 5.
\end{enumerate}

\section{References}

\renewcommand{\addcontentsline}[3]{}% Remove functionality of \addcontentsline
\bibliography{image_processing,image_compression,video_compression,rate_control,JPEG,JPEG2000}
