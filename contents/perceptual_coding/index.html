<!DOCTYPE html> 
<html lang='en-US' xml:lang='en-US'> 
<head> <title>Sistemas Multimedia - Perceptual Coding</title> 
<meta charset='utf-8' /> 
<meta content='TeX4ht (https://tug.org/tex4ht/)' name='generator' /> 
<meta content='width=device-width,initial-scale=1' name='viewport' /> 
<link href='index.css' rel='stylesheet' type='text/css' /> 
<meta content='index.tex' name='src' /> 
<script>window.MathJax = { tex: { tags: "ams", }, }; </script> 
 <script async='async' id='MathJax-script' src='https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js' type='text/javascript'></script>  
</head><body>
   <div class='maketitle'>
                                                                  

                                                                  
                                                                  

                                                                  

<h2 class='titleHead'><a href='https://sistemas-multimedia.github.io/'>Sistemas Multimedia</a> - <a href='https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/tree/master/contents/perceptual_coding'>Perceptual Coding</a></h2>
 <div class='author'><a href='https://cms.ual.es/UAL/personas/persona.htm?id=515256515553484875'><span class='ecrm-1200'>Vicente González Ruiz</span></a> <span class='ecrm-1200'>- </span><a href='https://cms.ual.es/UAL/universidad/departamentos/informatica/index.htm'><span class='ecrm-1200'>Depto Informática</span></a> <span class='ecrm-1200'>- </span><a href='https://www.ual.es'><span class='ecrm-1200'>UAL</span></a></div><br />
<div class='date'><span class='ecrm-1200'>January 17, 2023</span></div>
   </div>
   <h3 class='likesectionHead' id='contents'><a id='x1-1000'></a>Contents</h3>
   <div class='tableofcontents'>
    <span class='sectionToc'>1 <a href='#what-is-perceptual-coding' id='QQ2-1-2'>What is perceptual coding?</a></span>
<br />    <span class='sectionToc'>2 <a href='#tond-varies-with-the-luma-intensity' id='QQ2-1-3'>ToND varies with the luma intensity</a></span>
<br />    <span class='sectionToc'>3 <a href='#tond-varies-with-the-spatial-frequency' id='QQ2-1-4'>ToND varies with the spatial frequency</a></span>
<br />    <span class='sectionToc'>4 <a href='#visual-masking-of-the-quantization-noise' id='QQ2-1-5'>Visual masking of the quantization noise</a></span>
<br />    <span class='sectionToc'>5 <a href='#loop-filters' id='QQ2-1-6'>Loop filters</a></span>
<br />    <span class='sectionToc'>6 <a href='#luma-redundancy' id='QQ2-1-7'>Luma redundancy</a></span>
<br />    <span class='sectionToc'>7 <a href='#chroma-redundancy' id='QQ2-1-8'>Chroma redundancy</a></span>
<br />    <span class='sectionToc'>8 <a href='#resources' id='QQ2-1-9'>Resources</a></span>
<br />    <span class='sectionToc'>9 <a href='#todo' id='QQ2-1-10'>To-Do</a></span>
<br />    <span class='sectionToc'>10 <a href='#references' id='QQ2-1-11'>References</a></span>
   </div>
<!-- l. 9 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='what-is-perceptual-coding'><span class='titlemark'>1   </span> <a id='x1-20001'></a>What is perceptual coding?</h3>
<!-- l. 11 --><p class='noindent'>So far, we have been focused on minimizing the lagrangian <span class='cite'>[<a href='#Xsullivan1998rate'>3</a>]</span> \begin {equation}  J = R + \lambda D,  \end {equation}
where \(D\) is an additive<span class='footnote-mark'><a href='#fn1x0' id='fn1x0-bk'><sup class='textsuperscript'>1</sup></a></span><a id='x1-2001f1'></a>
distance metric, such as the RMSE, the PSNR or SSIM index <span class='cite'>[<a href='#Xwang2004image'>4</a>]</span>. However, the way
in which human beings perceive distortion is generally different from how these
metrics express it. This chapter introduces some of the most common ways of
exploiting the distortion perceived by humans.
                                                                  

                                                                  
</p><!-- l. 23 --><p class='indent'>   Notice that if, by requirements of the encoding process, \(D\) is below a threshold of
noticeable distortion (ToND), the RDO process boilds down to select the option with
smaller \(R\).
</p><!-- l. 27 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='tond-varies-with-the-luma-intensity'><span class='titlemark'>2   </span> <a id='x1-30002'></a>ToND varies with the luma intensity</h3>
<!-- l. 29 --><p class='noindent'>The Weber-Fechner law states that the minimum perceivable
visual stimulus difference increases with the background
luminance<span class='footnote-mark'><a href='#fn2x0' id='fn2x0-bk'><sup class='textsuperscript'>2</sup></a></span><a id='x1-3001f2'></a>,
up to a point in which decreases. Therefore, the perception of the distortion
generated by the lossy coding of an image is smaller in those areas with higher and
lower intensity values <span class='cite'>[<a href='#Xnaccari2014perceptually'>2</a>]</span>. For this reason, one of the most used quantizers is the
deadzone, which also in general change signal noise (for example, electronic
noise) by quantization noise, where the SNR of the signal is smaller (arroung
0).
</p><!-- l. 40 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='tond-varies-with-the-spatial-frequency'><span class='titlemark'>3   </span> <a id='x1-40003'></a>ToND varies with the spatial frequency</h3>
<!-- l. 41 --><p class='noindent'>The HVS can be modeled as a low-pass filter whose cut-off frequency depends on the
distance between the observer and the content (in terms of frequency) of the
image.
</p><!-- l. 45 --><p class='indent'>   Some image and video coding standards, such as JPEG and H.264, define
quantization matrices designed to perceptual coding. In the case of video, such
matrices can change between images. Usually, luma and chroma can have different
quantization matrices <span class='cite'>[<a href='#Xnaccari2014perceptually'>2</a>]</span>.
</p><!-- l. 51 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='visual-masking-of-the-quantization-noise'><span class='titlemark'>4   </span> <a id='x1-50004'></a>Visual masking of the quantization noise</h3>
<!-- l. 53 --><p class='noindent'>The quantization noise is generated by the quantizer, generating different coding
artifacts<span class='footnote-mark'><a href='#fn3x0' id='fn3x0-bk'><sup class='textsuperscript'>3</sup></a></span><a id='x1-5001f3'></a>,
which are perceived hardly when the (area of the) encoded image is textured <span class='cite'>[<a href='#Xwu2017digital'>5</a>]</span>.
This effect can happens up to the ToND.
</p><!-- l. 59 --><p class='indent'>   Another important aspect of our perception is its directionality, which leads the
HVS to be more sensitive to distortions added to horizontal and vertical frequencies
rather than to diagonal ones <span class='cite'>[<a href='#Xnaccari2014perceptually'>2</a>]</span>.
</p><!-- l. 64 --><p class='indent'>   Finally, the rationale behind the temporal masking is that the HVS sensitivity to
coding artifacts is lower in areas with very high motion activity.
                                                                  

                                                                  
</p><!-- l. 68 --><p class='indent'>   In video, modeling temporal masking is more challenging because the
spatio-temporal sensitivity function of the HVS is not separable, i.e., it depends on
both the spatial and temporal frequencies <span class='cite'>[<a href='#Xnaccari2014perceptually'>2</a>]</span>.
</p><!-- l. 73 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='loop-filters'><span class='titlemark'>5   </span> <a id='x1-60005'></a>Loop filters</h3>
<!-- l. 75 --><p class='noindent'>Loop filters are used in motion compensated video codecs to improve the visual quality
(and also the encoding RD performance). For example, H.264/AVC uses (usually
directional<span class='footnote-mark'><a href='#fn4x0' id='fn4x0-bk'><sup class='textsuperscript'>4</sup></a></span><a id='x1-6001f4'></a>)
deblocking filters in the encoding loop for smooth the transitions between the blocks,
when the boundaries between them become perceptible.
</p><!-- l. 82 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='luma-redundancy'><span class='titlemark'>6   </span> <a id='x1-70006'></a>Luma redundancy</h3>
<!-- l. 84 --><p class='noindent'>The HVS can perceive only a finite number of different intensities (luma). This
number depends on the dynamic range of the pixels, but in general, we are not able
to distinguish more than 64 intensity values <span class='cite'>[<a href='#Xvruiz__visual_redundancy'>1</a>]</span>.
</p><!-- l. 89 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='chroma-redundancy'><span class='titlemark'>7   </span> <a id='x1-80007'></a>Chroma redundancy</h3>
<!-- l. 91 --><p class='noindent'>Humans do not perceive detail in the chrominance as well as in they does in the
luminance <span class='cite'>[<span class='ecbx-1000'>?</span>]</span>. For this reason, the croma can be downsampled to 1/4 of the original
<a href='https://en.wikipedia.org/wiki/Sampling_(signal_processing)'>sampling rate</a> without noticeable distortion. This feature is used in some
of the last image and video compression systems such as <a href='https://en.wikipedia.org/wiki/JPEG_XR#Description'>JPEG XR</a> and
<a href='https://en.wikipedia.org/wiki/High_Efficiency_Video_Coding#Video_coding_layer'>HEVC</a>.
</p><!-- l. 100 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='resources'><span class='titlemark'>8   </span> <a id='x1-90008'></a>Resources</h3>
<!-- l. 101 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-9002x1'><a href='https://github.com/vicente-gonzalez-ruiz/color_transforms/blob/main/docs/color_redundancy.ipynb'>Spectral (color) redundancy</a>.</li></ol>
                                                                  

                                                                  
<!-- l. 106 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='todo'><span class='titlemark'>9   </span> <a id='x1-100009'></a>To-Do</h3>
<!-- l. 107 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-10002x1'>Modify  the  compression  pipeline  of  VCF  to  exploit  the  chroma
     redundancy. Use different quantization step sizes for each color subband.
     Complexity 2.
     </li>
<li class='enumerate' id='x1-10004x2'>The <a href='https://scikit-image.org/docs/stable/auto_examples/filters/plot_entropy.html'>local entropy</a> of the motion vectors can be a good estimation of the
     motion complexity in a video sequence. In a motion compensated video
     coding  pipeline  in  VCF,  adapt  the  quantization  step  size  to  the  local
     entropy, trying to increase the compression ratios without increasing the
     perceived distortoin. Compexity 5.</li></ol>
<!-- l. 120 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='references'><span class='titlemark'>10   </span> <a id='x1-1100010'></a>References</h3>
   <div class='thebibliography'>
   <p class='bibitem'><span class='biblabel'>
 [1]<span class='bibsp'>   </span></span><a id='Xvruiz__visual_redundancy'></a>V. González-Ruiz. <a href='https://vicente-gonzalez-ruiz.github.io/visual_redundancy/'>Visual Redundancy</a>.
   </p>
   <p class='bibitem'><span class='biblabel'>
 [2]<span class='bibsp'>   </span></span><a id='Xnaccari2014perceptually'></a>M. Naccari and M. Mrak.  Perceptually optimized video compression.
   In <span class='ecti-1000'>Academic Press Library in Signal Processing</span>, volume 5, pages 155–196.
   Elsevier, 2014.
   </p>
   <p class='bibitem'><span class='biblabel'>
 [3]<span class='bibsp'>   </span></span><a id='Xsullivan1998rate'></a>G.J. Sullivan and T. Wiegand.  Rate-distortion optimization for video
   compression. <span class='ecti-1000'>IEEE signal processing magazine</span>, 15(6):74–90, 1998.
   </p>
                                                                  

                                                                  
   <p class='bibitem'><span class='biblabel'>
 [4]<span class='bibsp'>   </span></span><a id='Xwang2004image'></a>Z. Wang, A.C. Bovik, H.R Sheikh, and E.P. Simoncelli.  Image quality
   assessment: from error visibility to structural similarity. <span class='ecti-1000'>IEEE transactions
   </span><span class='ecti-1000'>on image processing</span>, 13(4):600–612, 2004.
   </p>
   <p class='bibitem'><span class='biblabel'>
 [5]<span class='bibsp'>   </span></span><a id='Xwu2017digital'></a>H.R. Wu and K.R. Rao. <span class='ecti-1000'>Digital video image quality and perceptual coding</span>.
   CRC press, 2017.
</p>
   </div>
   <div class='footnotes'><!-- l. 17 --><p class='indent'>     <span class='footnote-mark'><a href='#fn1x0-bk' id='fn1x0'><sup class='textsuperscript'>1</sup></a></span><span class='ecrm-0800'>The total distortion of two (or more) sources of distortion is the sum of the distortions of
</span><span class='ecrm-0800'>these two (or more) sources.</span></p>
<!-- l. 31 --><p class='indent'>     <span class='footnote-mark'><a href='#fn2x0-bk' id='fn2x0'><sup class='textsuperscript'>2</sup></a></span><span class='ecrm-0800'>In general, this is not true for the chroma.</span></p>
<!-- l. 55 --><p class='indent'>     <span class='footnote-mark'><a href='#fn3x0-bk' id='fn3x0'><sup class='textsuperscript'>3</sup></a></span><span class='ecrm-0800'>“Random” noise, blocking, ringing, etc.</span></p>
<!-- l. 77 --><p class='indent'>     <span class='footnote-mark'><a href='#fn4x0-bk' id='fn4x0'><sup class='textsuperscript'>4</sup></a></span><span class='ecrm-0800'>Anisotropic.</span></p>                                                                                               </div>
 
</body> 
</html>