% Emacs, this is -*-latex-*-

% https://github.com/alpharaoh/Quadtree-Compression
% https://www.spiedigitallibrary.org/conference-proceedings-of-spie/594/0000/Image-Compression-Based-On-Hierarchical-Encoding/10.1117/12.952208.short?SSO=1


\input{../../definitions}

\title{\SM{} - \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/tree/master/contents/entropy_coding}{Entropy Coding}}

\maketitle

\tableofcontents

\section{What is entropy coding?}

\href{https://en.wikipedia.org/wiki/Entropy_coding}{Entropy Coding}
(EC)~\cite{vruiz__entropy_coding} encompasses a whole series of coding
techniques that exploit the
\href{https://en.wikipedia.org/wiki/Redundancy_(information_theory)}{statistical
  redundancy} in data with the ultimate goal of finding a more compact
representation, but without lossing information. EC is related to the definition of
\href{https://en.wikipedia.org/wiki/Entropy_(information_theory)}{entropy}
in the context of the Information
Theory~\cite{vruiz__information_theory}. In this area, the entropy
quantifies the average amount of
\href{https://en.wikipedia.org/wiki/Information}{information}
represented by a data set, so that the higher the entropy, the better
the efficiency of such representation
(\href{https://en.wikipedia.org/wiki/Data}{data}).

\section{Why EC?}
All data (text, audio, image, video, etc.) compression techniques rely
on EC to achieve an effective reduction of bits (of data). For example,
\href{https://en.wikipedia.org/wiki/JPEG}{JPEG} uses a combination of
\href{https://en.wikipedia.org/wiki/Huffman_coding}{Huffman Coding}
(see below) and
\href{https://en.wikipedia.org/wiki/Run-length_encoding}{Run-Length
  Encoding} to represent the quantized DCT coefficients.

\section{A classification of entropy encoders}

General speaking, data are sequences of
\href{https://en.wikipedia.org/wiki/Symbol}{symbols}. There are
basically two types of entropy encoders depending on how the symbols
are processed:

\begin{enumerate}
\item \textbf{Symbol encoders}: Those that process the sequence symbol
  by symbol. Examples of this type of algorithms are Huffman
  Coding~\cite{vruiz__huffman_coding} and Arithmetic
  Coding~\cite{vruiz__arithmetic_coding}. These encoders are also
  called ``probabilistic encoders'' because the number of bits of code
  assigned to a symbol $s$ depends on the probability of the symbol
  $p(s)$.
\item \textbf{String encoders:} Those that process the sequence by
  blocks of symbols (strings). Examples are Run-Length Encoding
  (RLE)~\cite{vruiz__rle} and all the dictionary-based text
  compressors, such as LZW~\cite{vruiz__LZW}.
\end{enumerate}

\begin{comment}
The encoders of the first type generally achieve more compact
representations (from a pure Entropy perspective, i.e., without reducing the number of symbols), but are computationally more expensive. The
algorithms of the second class are usually faster, but slightly worse
in compression ratio.
\end{comment}

\section{\href{https://en.wikipedia.org/wiki/Huffman_coding}{Huffman Coding (HC)}}

A Huffman code~\cite{vruiz__huffman_coding,ruiz2000compresion} is a
\href{https://en.wikipedia.org/wiki/Prefix_code}{prefix code} that,
for each code-word, allows to ``navigate'' through the so called
Huffman tree from the trunk to one of its leaves, without
uncertainty. The Huffman tree satisfies that
\begin{equation}
  l(c(s)) = \lceil I(s)\rceil,
  \label{eq:huffman_performance}
\end{equation}
where $l(c(s))$ is the length of the (Huffman) code-word, usually in
bits of data, assigned to the symbol $s$, and $I(s)$ is the amount of
information represented by $s$, measured bits of
information~\cite{vruiz__information_theory}.

Notice that the minimum number of bits that can be used for
representing a symbol (using Huffman) is 1, which can be a inefficient
when the length of the alphabet is small.\footnote{In this case,
  Huffman can be ineffective. In an extreme case, the alphabet could
  have only two symbols, and therefore the Huffman encoder does not
  compress or expand.} Another drawback of HC is that the Huffman tree
must be updated each time the probabilistic model is updated, action
very frequent when using
\href{https://en.wikipedia.org/wiki/Adaptive_Huffman_coding}{adaptive
  models}.

\subsection*{Resources}
\begin{enumerate}
\item \href{https://github.com/Sistemas-Multimedia/VCF/blob/main/src/Huffman.py}{Huffman.py}: Implementation in VCF.
\item \href{https://github.com/Sistemas-Multimedia/VCF/blob/main/notebooks/Huffman.ipynb}: Notebook showing the use of Huffman.py.
\item \href{https://www.inference.org.uk/mackay/python/compress/#Huff}{Compression algorithms in Python}.
\item \href{https://favtutor.com/blogs/huffman-coding}{Huffman Coding Implementation in Python with Examples}.
\item \href{https://www.programiz.com/dsa/huffman-coding}{Huffman Coding}.
\item \href{https://en.wikipedia.org/wiki/Prediction_by_partial_matching}{Prediction by Partial Matching}.
\item \href{https://en.wikipedia.org/wiki/Lossless_JPEG}{Lossless JPEG}.
\item \href{https://en.wikipedia.org/wiki/Context-adaptive_variable-length_coding}{Context-Adaptive Variable-Length Coding}.
\end{enumerate}

\subsection*{To do}
\begin{enumerate}
\item Create a new module named \texttt{adaptive\_Huffman.py} that use
  a context-based adaptive probabilistic model
  \cite{nelson96datacompression}, where the user can select the order
  of the model.
\end{enumerate}

\section{\href{https://en.wikipedia.org/wiki/Arithmetic_coding}{Arithmetic
    Coding (AC)}}

In an arithmetic codec~\cite{vruiz__arithmetic_coding,ruiz2000compresion}, the number of
bits of data that are used for representing symbols match
exactly the number of bits of information provided by the symbols, i.e,
\begin{equation}
  l(c(s)) = I(s).
\end{equation}

This also means that, even if the size of the alphabet is small, the
coding performance of an arithmetic code is optimal, although this
optimality is only fully satisfied if the number of symbols to encode
is infinite. Notice also that, if the alphabet is large, the encoding
performance difference between a Huffman code (or any other prefix
code) and an arithmetic code, vanishes.

\subsection*{Resources}
\begin{enumerate}
\item
  \href{https://github.com/vicente-gonzalez-ruiz/arithmetic_coding/blob/master/src/arithmetic_coding/arithmetic_coding.ipynb}{Arithmetic
    Coding (notebook)}.
\item \href{https://www.nayuki.io/page/reference-arithmetic-coding}{Reference Arithmetic Coding}/\href{https://github.com/nayuki/Reference-arithmetic-coding}{Reference Arithmetic Coding}.
\item \href{https://www.inference.org.uk/mackay/python/compress/#AC}{Compression Algorithms in Python}.
\item \href{https://en.wikipedia.org/wiki/Prediction_by_partial_matching}{Prediction by Partial Matching}.
\item \href{https://en.wikipedia.org/wiki/Lossless_JPEG}{Lossless JPEG}.
\item \href{https://en.wikipedia.org/wiki/Context-adaptive_binary_arithmetic_coding}{Context-Adaptive Binary Arithmetic Coding}.
\end{enumerate}

\subsection*{To do}
\begin{enumerate}
\item Create a new module named \texttt{arith.py} that use
  a context-based adaptive probabilistic model and Arithmetic Coding
  \cite{nelson96datacompression}, where the user can select the order
  of the model.
\end{enumerate}

\section{\href{https://zlib.net/}{\texttt{zlib}}}

\texttt{zlib} is based on
\href{https://en.wikipedia.org/wiki/Deflate}{DEFLATE}, which in turn
is based on LZ77~\cite{vruiz__LZW,ruiz2000compresion} and Huffman
coding. Therefore, \texttt{zlib} exploits the repetition of symbols
and also, the 0-order statistical redundancy\footnote{In other
  words, that some symbols are more frequently than others.}. One of
the main advantages of \texttt{zlib} is that is quite fast compared to
symbol encoders such as HC and AC.

Nowadays, \texttt{zlib} is a keystone in data manipulation because it
is the basic library used in such important applications as
\href{https://en.wikipedia.org/wiki/ZIP_(file_format)}{Zip} and
\href{https://en.wikipedia.org/wiki/Gzip}{Gzip}.

\subsection*{Resources}
\begin{enumerate}
\item \href{https://docs.python.org/3/library/zlib.html}{\texttt{zlib}
    â€” Compression compatible with gzip}.
\item \href{https://en.wikipedia.org/wiki/Lossless_JPEG}{Lossless JPEG}.
\end{enumerate}

\section{\href{https://en.wikipedia.org/wiki/Portable_Network_Graphics}{Portable Network Graphics (PNG)}}
PNG~\cite{vruiz__PNG} (pronounced ``ping'') is a dictionary-based
(string encoding)
\href{https://en.wikipedia.org/wiki/Lossless_compression}{lossless
  image compression format} used for representing
\href{https://en.wikipedia.org/wiki/Digital_data}{digital}
\href{https://en.wikipedia.org/wiki/Digital_image}{images} and
\href{https://en.wikipedia.org/wiki/Video}{videos}~\cite{vruiz__image_video}
in III... format~\cite{vruiz__video_compression}. The entropy encoder
of PNG is based on HC and LZSS, and a pixel predictor that removes the
spatial redundancy.

We must bear in mind that as such an image compressor, we can only
interact with PNG at the image level, that is, it only accepts images
(in shades of gray or in color, with the possibility of an
\href{https://en.wikipedia.org/wiki/Alpha_compositing}{alpha channel}),
and only returns images.

% PNG is the default EC in the \href{https://github.com/Sistemas-Multimedia/VCF}{VCF project} because: (1) is lossless, (2) is fast, and (3) the compression performance is reasonable.

% \section{Entropy Image Coding with \href{https://en.wikipedia.org/wiki/Better_Portable_Graphics}{BPG (Better Portable Graphics)}}

\section{References}

\renewcommand{\addcontentsline}[3]{}% Remove functionality of \addcontentsline
\bibliography{text_compression,image_formats,image_video_theory,information_theory,video_compression}
