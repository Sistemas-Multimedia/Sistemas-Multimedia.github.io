% Emacs, this is -*-latex-*-

\input{../../definitions}

\title{\SM{} - Entropy Coding}

\maketitle

\tableofcontents

\section{What is Entropy Coding?}

Entropy Coding (EC) encompasses a whole series of coding techniques
that exploit the statistical redundancy of data with the ultimate goal
of finding a more compact representation. EC is related to the
definition of entropy in the context of the Information
Theory~\cite{vruiz__information_theory}. In this area, the entropy
quantifies the volume of information represented by a data set, so
that the higher the entropy, the better the efficiency of such
representation (data) would be.

\section{A classification of Entropy Encoders}

There are basically two types of entropy encoders depending on how the
sequence of symbols that make up the data to be encoded are processed:

\begin{enumerate}
\item Those that process the sequence symbol by symbol. Examples of
  this type of algorithms are Huffman
  Coding~\cite{vruiz__huffman_coding} and Arithmetic
  Coding~\cite{vruiz__arithmetic_coding}. These encoders are also
  called ``probabilistic encoders'' because the number of bits of code
  assigned to a symbol $s$ depends on the probability of the symbol
  $p(s)$.
\item Those that process the sequence by blocks of symbols
  (strings). Examples are Run-Length Encoding (RLE)~\cite{vruiz__rle}
  and all the dictionary-based text compressors, such as
  LZW~\cite{vruiz__LZW}.
\end{enumerate}
  
Those of the first type generally achieve more compact
representations, but are more computationally expensive. The
algorithms of the second class are usually faster, but slighlt worse
in compression ratio.

\section{\href{https://en.wikipedia.org/wiki/Huffman_coding}{Huffman Coding}}

A Huffman code is a
\href{https://en.wikipedia.org/wiki/Prefix_code}{prefix code} that
allows to ``navigate'' through the so called Huffman (inverted) tree
from the trunk to one of its leaves without
uncertainty~\cite{vruiz__huffman_coding}. The Huffman tree satisfies that
\begin{equation}
  l(c(s)) = \lceil I(s)\rceil,
  \label{eq:huffman_performance}
\end{equation}
where $l(c(s))$ is the length of the (Huffman) code assigned to the
symbol $s$ and $I(s)$ is the amount of information represented by $s$,
measured bits of information.

Notice that the minimum number of bits that can be used for
representing a symbol (using Huffman) is 1, which can be a problem
when the length of the alphabet is small. Notice also that, if the
probabilistic model remains constant, the code-words assigned to the
symbols will be also remain unchanged.

\subsection*{Resources}



\section{Arithmetic Coding~\cite{vruiz__arithmetic_coding}}

In an
\href{https://en.wikipedia.org/wiki/Arithmetic_coding}{arithmetic
  code}, the number of bits of data that can be used for representing
a symbol can match exactly the number of bits of information, i.e,
\begin{equation}
  l(c(s)) = I(s).
\end{equation}

This also means that even if the size of the alphabet is small, the
coding performance of an arithmetic code is optimal, although this
optimality is only satisfied if the number of symbols to encode is
infinite. Notice also that, if the size of the alphabet is high, the
encoding performance difference between a Huffman code (or any other
prefix code) and an arithmetic code, vanishes.

\subsection*{Resources}
\begin{enumerate}
\item
  \href{https://www.folkstalk.com/tech/arithmetic-encoding-python-with-code-examples/}{Arithmetic
    Encoding Python With Code Examples}.
\item 

\section{Portable Network Graphics (PNG)}

\href{https://en.wikipedia.org/wiki/Portable_Network_Graphics}{PNG}~\cite{vruiz__PNG}
(pronounced ``ping'') a dictionary-based
\href{https://en.wikipedia.org/wiki/Lossless_compression}{lossless
  image compression format} used for representing
\href{https://en.wikipedia.org/wiki/Digital_data}{digital}
\href{https://en.wikipedia.org/wiki/Digital_image}{images} and
\href{https://en.wikipedia.org/wiki/Video}{videos}~\cite{vruiz__image_video}
in III... format. The entropy encoder of PNG is based on Huffman
Coding and LZSS, but PNG also incorporates a pixel predictor that
removes the spatial redundancy.

We must bear in mind that as such an image compressor, we can only
interact with PNG at the image level, that is, it only accepts images
(in shades of gray or in color, with the possibility of alpha channel)
and returns images.

% PNG is the default EC in the \href{https://github.com/Sistemas-Multimedia/VCF}{VCF project} because: (1) is lossless, (2) is fast, and (3) the compression performance is reasonable.

\section{References}

\renewcommand{\addcontentsline}[3]{}% Remove functionality of \addcontentsline
\bibliography{text-compression,image-formats,image-video-theory}
