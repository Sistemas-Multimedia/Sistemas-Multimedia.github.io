\input{../../definitions}
\title{\SM{} - Temporal Transform}

\maketitle

\section{Motion Estimation in the Spatial Domain}

\href{https://en.wikipedia.org/wiki/Motion_estimation}{ME (Motion
  Estimation)} is the process of determining the MVs (Motion Vectors)
that describe the mapping of the pixels from one frame (2D image) to
another.

\subsection{Motion Estimation (ME) for what?}
Temporal correlation between video frames\footnote{Remember that,
although this discussion will deal with frames, in our particular
case, we will decorrelate subbands.} can be removed by MC (Motion
Compensation). MC implies to decrease (usually by substracting a
prediction frame) the amount of information in the frames. The removed
information must be available at both, the encoder and the decoder
side, in order to make this a reversible process.

Specifically, a MCP (MC Predictor) inputs one (or more) reference
frame(s) ${\mathbf R}=\{{\mathbf R}_i\}$, and a motion vectors field
$\overset{{\mathbf R}\rightarrow{\mathbf P}}{\mathbf M}$ that
indicates how to project ${\mathbf R}$ onto the predicted frame ${\mathbf P}$, and outputs
a prediction frame
\begin{equation}
  \hat{{\mathbf P}} =  \overset{{\mathbf R}\rightarrow {\mathbf P}}{\mathbf M}({\mathbf R}).
  \label{eq:MCP1}
\end{equation}
In this milestone we analyze different algorithms to determine
$\overset{{\mathbf R}\rightarrow {\mathbf P}}{\mathbf M}$ (in a futher
milestone we will see how to remove the $\hat{{\mathbf P}}$'s
information from ${\mathbf P}$). At this moment, for the sake of
simplicity, in the rest of this discussion it will be supposed that the
number of reference frames in only 1.
%(probably, the closest one to the
%predicted frame), and therefore, we don't need to specify which are
%the reference images to make the prediction. For this reason,
%Eq,~\ref{eq:MCP1} can be rewritten as
%\begin{equation}
%  \hat{\mathbf W}_k = \overset{{\mathbf W}_{k-1}\rightarrow {\mathbf
%  W}_{k}}{\mathbf M}({\mathbf W}_{k-1}) = \overset{(k-1)\rightarrow
%          k}{\mathbf M}({\mathbf W}_{k-1})
%%  \hat{{\mathbf P}_k} = \overset{{\mathbf P}_k\rightarrow {\mathbf
%%  P}_{k-1}}{\mathbf M}({\mathbf P}_{k-1}) = \overset{k\rightarrow
%%          k-1}{\mathbf M},
%  \label{eq:MCP2}
%\end{equation}

\subsection{But ... what exactly do we need?}
Our main objective is to minimize the differences (for example, the
\href{https://en.wikipedia.org/wiki/Euclidean_distance}{L$_2$
  distance}) between ${\mathbf P}$ (the predicted frame) and $\hat{\mathbf P}$ (the
prediction frame), i.e. minimizing
\begin{equation}
  {\mathbf E} = {\mathbf P} - \hat{\mathbf P}
\end{equation}
in order to get that ${\mathbf E}$ will be more compressible than
${\mathbf P}$. To achieve this, we can compute $\overset{{\mathbf
    R}\rightarrow {\mathbf P}}{\mathbf M}$ that simply minimizes the
L$_2$ energy of ${\mathbf E}$, $||{\mathbf E}||^2$, or we can compute
a $\overset{{\mathbf R}\rightarrow {\mathbf P}}{\mathbf M}$ that also describes the Optical
Flow~\cite{horn1981determining} (OF) between the pixels of ${\mathbf
  R}$ and ${\mathbf P}$, that although not necessarily has to
minimize $||{\mathbf E}||^2$, tries to show the true movement of the
pixels between the both frames. This second option has the advantage
of generating more visually pleasing reconstructions when the
code-stream is partially received and makes easier to predict the
content of the motion fields.

The first type of techniques are simply called ``ME techniques'', and
are usually faster\footnote{Obviously, depending on the algorithm.}
than the second type, based on the estimation of the OF.

%Let's see two basic techniques to estimate the motion between 2
%frames, $R$ and $P$. In this discussion it will be supposed that the
%motion of the objects that are in both frames is bounded, and that the
%luminance varies smoothly between adjacent frames.

Now, let's see some of the most used techniques for estimating the
motion between two frames. Notice that, in general, better estimations
can be found if we suppose motion models such as that the objects
exhibit
\href{https://en.wikipedia.org/wiki/Inertia}{inertia}. However, this
case will not be considered for now.

\subsection{\href{https://vicente-gonzalez-ruiz.github.io/video_compression/\#x1-40003}{Block-based motion estimation}}

\begin{figure}
  \centering
  \svg{graphics/simple}{400}
  \caption{ME using disjoint blocks. $({\mathbf M}_x, {\mathbf M}_y)$
    is the motion vector that indicates where the block $(x,y)$ of
    ${\mathbf P}$ is found in ${\mathbf R}$.}
  \label{fig:simple}
\end{figure}

Block-based ME is the simplest ME algorithm (see the
Fig.~\ref{fig:simple}), ${\mathbf P}$ is divided in blocks of (for
example) 16x16 pixels, and we can use the (R)MSE that measures the
distance in L$_2$ (also known as the Euclidean distance) between each
block of ${\mathbf P}$ and its surrounding pixels in ${\mathbf R}$
(the so called search area)~\cite{zhu2000new}. For each block, a
motion vector that indicates the best match (smaller distance) is
found. The set of motion vectors form the motion vectors field
$\overset{{\mathbf R}\rightarrow {\mathbf P}}{\mathbf M}$ that
obviously, except for a block size of 1x1, will be less dense than
${\mathbf R}$ and ${\mathbf P}$. Notice, however, that, it is not a
good idea to use such a small block size because, in general, the
motion vectors will not describe the true motion in the scene.

\begin{figure}
  \centering
  \png{stockholm_R_block}{800}
  \caption{A tile of the first image of the \emph{Stockholm}
    sequence. This is the reference (${\mathbf R}$) frame.}
  \label{fig:R_block}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_P_block}{800}
  \caption{The same (coordinates) tile of the second image of the
    \emph{stockholm} sequence. This is the predicted (${\mathbf P}$)
    frame.}
  \label{fig:P_block}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_PR_block}{800}
  \caption{${\mathbf P} - {\mathbf R}$: shows the differences between
    both tiles. The entropy of the residue is displayed between
    parentheses.}
  \label{fig:RP_block}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_hat_P_block}{800}
  \caption{The prediction frame (${\hat{\mathbf P}}$). See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/full_search_block_ME.ipynb}{this}.}
  \label{fig:hat_P_block}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_error_block}{800}
  \caption{The prediction-error frame
    (${\mathbf R} - {\hat{\mathbf P}}$). See
    \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/full_search_block_ME.ipynb}{this}.}
  \label{fig:error_block}
\end{figure}

As it can be seen in the Figures \ref{fig:R_block}, \ref{fig:P_block},
\ref{fig:RP_block}, \ref{fig:hat_P_block}, and \ref{fig:error_block}, the MVs generated
by block-based ME can significantly decrease the entropy.

\begin{figure}
  \centering
  \png{stockholm_MVs_block}{800}
  \caption{Motion vectors to map ${\mathbf P}$ (which is divided into
    disjoint blocks) onto ${\mathbf R}$. See
    \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/full_search_block_ME.ipynb}{this}.}
  \label{fig:MVs_block}
\end{figure}

However, as it can be seen in the Figure~\ref{fig:MVs_block}, the
motion information computed by the block-based ME algorithm not always
represents the true motion in the scene in the case of using
block-based matching. This can be a drawback, for example, for solving
object tracking problems. In the case of video coding, the main
disadvantage of such issue is that the entropy of the motion fields
increases, which also decreases the compression ratio.

\subsection{Overlapped block matching}

\begin{figure}
  \centering
  \svg{graphics/overlaped}{400}
  \caption{ME using overlaped blocks.}
  \label{fig:overlaped}
\end{figure}

A better approximation to the OF for small block sizes can be found if
we allow the blocks to overlap in ${\mathbf
  P}$~\cite{orchard1994overlapped}, case in which the block size for
performing the comparisons must be larger. Again, as it happens with
the disjoint case, only the non overlaped pixels are used for building
the prediction (see the Fig.~\ref{fig:overlaped}). Obviously, the main
drawback of this technique is that it can be more computationally
demanding than the previous one.

\begin{figure}
  \centering
  \png{stockholm_hat_P_dense}{800}
  \caption{The prediction frame (${\hat{\mathbf P}}$). See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/full_search_dense_ME.ipynb}{this}.}
  \label{fig:hat_P_dense}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_error_dense}{800}
  \caption{The prediction error frame (${\mathbf R} - {\hat{\mathbf P}}$). See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/full_search_dense_ME.ipynb}{this}.}
  \label{fig:error_dense}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_MVs_dense}{800}
  \caption{Motion vectors to map ${\mathbf P}$ (from which each pixel has been mapped) onto ${\mathbf R}$. See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/full_search_dense_ME.ipynb}{this}.}
  \label{fig:MVs_dense}
\end{figure}

The dense ME algorithm can obtain better predictions than the
block-based one, as it can be seen in the
Figures~\ref{fig:hat_P_dense} and \ref{fig:error_dense}. The MVs are
also more coherent (see Figure~\ref{fig:MVs_dense}).

\begin{figure}
  \centering
  \svg{graphics/average}{400}
  \caption{ME using overlaped blocks, averaging the overlaped pixels.}
  \label{fig:average}
\end{figure}

An improvement of the previous technique can also average the
overlaped pixels in the prediction frame $\hat{P}$, as it has been
shown in the Fig.~\ref{fig:average}.

\subsection{Machine learning}
ANNs (Artifical Neural Networks) can be trained to estimate the motion
between frames~\cite{dosovitskiy2015flownet}. For the training of
ANNs, animation videos are generally used where the motion fields are
known with precision.

\section{Motion Compensation in the DWT Domain}

\subsection{ME in a transformed domain}

\begin{figure}
  %\begin{tabular}{cccccc}
  %  \png{one} & \png{x} & \png{y} & \png{x2} & \png{y2} & \png{xy} \\
  %\end{tabular}
  \begin{tabular}{cccccc}
    \png{one}{200} & \png{x}{200} & \png{y}{200} & \png{x2}{200} & \png{y2}{200} & \png{xy}{200} \\
    No motion & Constant velocity in $X$ & Constant velocity in $Y$ & Constant acceleration in $X$ & Constant acceleration in $Y$ & Constant accelarion in diagonal
  \end{tabular}
  \caption{Correlation kernels (basis functions) used by the
    \emph{polynomial expansion} of the Farneb{\"a}ck's ME
    algorithm. See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/farneback_ME.ipynb}{this}. The analized motion is depicted below the plot of each basis.}
  \label{fig:FarnebacK_basis}
\end{figure}

The motion can be estimated also in a transformed domain. One of these
estimators is the Farneb{\"a}ck's algorithm~\cite{farneback2003two},
which uses the transform defined by the basis functions
\begin{equation}
    \{1, x, y, x^2, y^2, xy\}
\end{equation}
(see the Figure~\ref{fig:FarnebacK_basis}). In this transform domain,
which is applied by overlapped regions, the corresponding subbands
quantify the tendency of the image to increase its intensity in
different 2D directions, and therefore, it is more efficient to know
the direction in which the objects are moving.

\begin{figure}
  \centering
  \png{stockholm_hat_P_farneback}{800}
  \caption{The prediction frame (${\hat{\mathbf P}}$). See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/farneback_ME.ipynb}{this}.}
  \label{fig:hat_P_farneback}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_error_farneback}{800}
  \caption{The prediction error frame (${\mathbf R} - {\hat{\mathbf P}}$). See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/farneback_ME.ipynb}{this}.}
  \label{fig:error_farneback}
\end{figure}

\begin{figure}
  \centering
  \png{stockholm_MVs_farneback}{800}
  \caption{Motion vectors to map ${\mathbf P}$ (from which each pixel has been mapped) onto ${\mathbf R}$. See \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/09-ME/farneback_ME.ipynb}{this}.}
  \label{fig:MVs_farneback}
\end{figure}

The Farneback's ME is a dense ME, and it provides subpixel
interpolation because the MVs are real numbers (see the
Figures~\ref{fig:hat_P_farneback}, \ref{fig:error_farneback} and
\ref{fig:MVs_farneback}). Notice that the prediction is the best of
the all tested algorithms, probably by the subpixel accuracy.

\subsection{Removing the temporal redundancy through Motion Compensation (MC)}
The next natural step in the process of decorrelating the sequence of
frames is to remove the temporal redundancy by means of Motion
Compensation (MC). Basically, MC consists in substracting to the video
data a prediction performed with the information that is avaliable to
the decoder. If this prediction is accurate, the result of this
operation is a residual video with a lower temporal redundancy, that
can be compressed with a higher compression ratio (there is
less information to encode in the residue sequence than in the
original one).

\subsection{Integer pixel accuracy In-Band Motion estimation and Compensation (IBMC)}
At this stage of the encoding process, the video data is represented
in the DWT domain, and therefore, we need to perform an In-Band Motion
estimation and Compensation
(IBMC)~\cite{andreopoulos2005complete}). Let's suppose that the number
of levels of the DWT is 1, and therefore, each frame has been
decomposed into two 2D subbands $L$ and $H$ (remember that using the
notation introduced in the previous milestone, $H$ has inside the
three high-frequency subbands: $LH$, $HL$ and $HH$, and that
$L=LL$). This discussion will be also constrained to the case in which
the movement of the objects in the scene is a integer number of
pixels.

\subsection{The lack of shift-invariance in the DWT domain}
In our case, the video data is represented in the DWT domain, and
therefore, we need to perform the so called In-Band Motion Estimation
and Compensation~\cite{andreopoulos2005complete}). Let's suppose that
the number of levels of the DWT is 1, and therefore, each frame has
been decomposed into two 2D subbands $L$ and $H$ (remember that using
the notation introduces in the previous milestone, $H$ has inside the
three high-frequency subbands: $LH$, $HL$ and $HH$, and that
$L=LL$). So, after using the MDWT, MC must be performed using the DWT
coefficients.

Unfortunately, as a consequence of the downsamplers used during the
DWT to achieve critical sampling and the aliasing between the
subbands, DWT decompositions are shift-variant. This can be seen in
the Fig.~\ref{fig:dwt_shift_variance} were some DWT coefficients of a
test video with three frames (with a 1-pixel constant speed moving (to
the left) cicle``empty'') has been shown. As it can be seen, when the
circle has been moved only one pixel, the value of the coefficients
that correspond to the circunference of the circle are different
between the reference frame and the predicted frame. This makes quite
difficult to estimate the motion, and therefore, compensate
it. However, when the circle has traveled two pixels, a perfect match
is performed.

Notice also that shift-variance is also generated after the inverse
transform when the coefficients are filtered or quantized, because the
aliasing between the filters is not completely cancelled in this
case~\cite{bradley2003shift}.

\begin{figure}
  \centering
  \begin{tabular}{ccc}
    \vbox{\pngfig{moving_circle_000}{4cm}{400}} &
    \vbox{\myfig{movement}{4cm}{400}} &
    \vbox{\pngfig{difference_0}{4cm}{400}} \\
    \vbox{\myfig{haar_LL}{4cm}{400}} &
    \vbox{\myfig{db5_LL}{4cm}{400}} &
    \vbox{\myfig{bior35_LL}{4cm}{400}} \\
    \vbox{\myfig{haar_LH}{4cm}{400}} &
    \vbox{\myfig{db5_LH}{4cm}{400}} &
    \vbox{\myfig{bior35_LH}{4cm}{400}} \\ 
    \vbox{\myfig{haar_HL}{4cm}{400}} &
    \vbox{\myfig{db5_HL}{4cm}{400}} &
    \vbox{\myfig{bior35_HL}{4cm}{400}} \\
    \vbox{\myfig{haar_HH}{4cm}{400}} &
    \vbox{\myfig{db5_HH}{4cm}{400}} &
    \vbox{\myfig{bior35_HH}{4cm}{400}} 
  \end{tabular}
  \caption{A demonstration of the shift-variance of the DWT.}
\label{fig:dwt_shift_variance}
\end{figure}

The reason why the 1-pixel movement is generating different
coefficients in the reference and the predicted frames is because a
1-pixel motion cannot be represented using always the same phase
(remember that with the downsampler we are basically selecting only
one the two possible phases of the output of the analysis filters: the
even samples or the odd samples). Lets suppose that the downsampler
discards the odd coefficients (let's refer them as odd-phase
coefficients). In this case, the even-phase cofficients of the
reference frame are the same than the odd-phase coefficients of the
predicted frame (this can be seen in this notebook). Therefore, in the
1D case, when the motion is ``even''-type (that is, a displacement of
a even number of samples) we should compensate the even-phase
coefficients of the reference and the predicted frame, while when the
motion is ``odd''-type we should compensate the odd-phase coefficients
of the predicted frame with a prediction generated with the even-phase
coefficients of the reference frame, or viceversa.

There are different alternatives for recovering the ``lost'' phase
during the DWT (in the 1D case):
\begin{enumerate}
\item MC-then-downsample: Perform first the MC stage directly over the
  output of the analysis filters, and then, selectively downsample the
  result. Notice that the downsampler should select the right phase,
  depending on the type of motion detected (``odd'' or ``even''). This
  information (the selected phase), should be available at the
  decoder, along with the motion fields.
\item Delay-then-DWT: Perform two identical DWTs, one to the original
  signal, and the other to a one-sample delayed signal (remember than
  a movement of one pixel will change the phase at the output of the
  DWT). Thus, the the DWT applied to the original signal will generate
  one of the phases and the DWT applied to the delayed signal will
  generate the other one.
\item CODWT: Use the current (single phase) L and H coefficients to
  compute the missing phase, using the CODWT (Complete-to-Overcomplete
  DWT)~\cite{andreopoulos2005complete} (a new type of DWT applied to
  the DWT coefficients).
\end{enumerate}
Each alternative has pros and cons. If the DWT has been implemented
using convolution, MC-then-downsample should be a fast
alternative. However, if the DWT uses Lifting, Multiple-DWT-then-MC
should be fast also, because only one phase is computed by the
DWT. These two options can be used with any DWT filters. On the other
hand, CODWT needs specific designs form each DWT filters. Notice that, in any case, the solution is reached after using the ODWT domain.

In the 2D case, and always working with only one level of the DWT, we
have up to four different phases: (even, even)-, (even, odd)-, (odd,
even)-, and (odd, odd)-phase coefficients. Thus, depending on the type
of motion detected, the corresponding phase should be selected.

\subsection{Near shift-invariance in the IDWT (Interpolated DWT) domain}
As it was commented before, the causant of the shift-variance in the
critically sampled DWT domain is the use of the downsamplers. At this
point we have basically two different alternatives:
\begin{enumerate}
\item Use the Algorithme \`a Trous (AaT)~\cite{mallat1999wavelet},
  which removes the downsamplers from the DWT, generating the so
  called Overcomplete DWT (ODWT). Notice that, because the
  downsamplers are removed, the aliasing artifacts produced by the
  downsamplers is also avoided.
\item Approximate the AaT coefficients by interpolating the DWT
  coefficients using the DWT synthesis filters. In this case, the
  aliasing is not avoided, but the shift-variance problem is
  reduced.
\end{enumerate}

Unfortunately, DWT decompositions are shift-variant as a consequence
of the downsampling performed during the DWT to achieve a critical
representation. This can be seen in the Fig.~\ref{fig:DWT} were some
DWT coefficients of a test video with three frames has been shown. As
it can be seen, when the circle moves to the left only one pixel (as
happens between the frames 0 and 1), the value of the coefficients
that correspond to the circunference of the circle are different
between the reference frame (0) and the predicted frame (1). This
makes quite difficult to estimate and compensate the motion between
frames. Notice also that the effects of shift-variance is also visible
after using the inverse transform when the coefficients are filtered
or quantized, because the aliasing between the filters is not
completely cancelled in this case~\cite{bradley2003shift}.

\begin{figure}
  \centering
  \begin{tabular}{ccc}
    \vbox{\png{frame_0_Y}{300}} & \vbox{\png{frame_1_Y}{300}} & \vbox{\png{frame_2_Y}{300}} \\
    & \vbox{\svg{movement_0}{300}} & \vbox{\svg{movement_1}{300}} \\
    \vbox{\png{f0_haar_LL}{300}} & \vbox{\png{f1_haar_LL}{300}} & \vbox{\png{f2_haar_LL}{300}} \\
    \vbox{\png{f0_haar_LH}{300}} & \vbox{\png{f1_haar_LH}{300}} & \vbox{\png{f2_haar_LH}{300}} \\
    \vbox{\png{f0_haar_HL}{300}} & \vbox{\png{f1_haar_HL}{300}} & \vbox{\png{f2_haar_HL}{300}} \\
    \vbox{\png{f0_haar_HH}{300}} & \vbox{\png{f1_haar_HH}{300}} & \vbox{\png{f2_haar_HH}{300}} \\
    & \vbox{\svg{f0_1_haar_LL}{300}} & \vbox{\svg{f0_2_haar_LL}{300}} \\
    & \vbox{\svg{f0_1_haar_LH}{300}} & \vbox{\svg{f0_2_haar_LH}{300}} \\
    & \vbox{\svg{f0_1_haar_HL}{300}} & \vbox{\svg{f0_2_haar_HL}{300}} \\
    & \vbox{\svg{f0_1_haar_HH}{300}} & \vbox{\svg{f0_2_haar_HH}{300}}
  \end{tabular}
  \caption{A demonstration of the shift-variance of the DWT. Similar
    results have been obtained for other filters. See this
    \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/11-MC_in_DWT_domain/DWT_shift_invariance.ipynb}{notebook}.}
\label{fig:DWT}
\end{figure}

However, suprisingly, at it can be also seen in the
Fig.~\ref{fig:DWT}, when the circle has traveled two pixels (frames 0
and 2), a perfect match is achieved! The reason why the 1-pixel motion
generates different coefficients in the reference and the predicted
frames, and the same coefficients for a 2-pixel motion is because, in
the first case the right coefficients were discarded by the
downsamplers, and in the second case not.

Usually, we call \emph{phases} to the two possible coefficients
resulting from one (1D) filter to be subsampled, being the even phase,
the even coefficients, and the odd phase, the odd
coefficients. Therefore, when the motion is of type ``even'' (when we
have a $2N$-pixels motion), we should use the even phase to compensate
the frames, and viceversa (use the odd phase to compensate a
$2N+1$-pixels motion). Notice that in the 2D case, and always working
with only one level of the DWT, we have up to four different phases:
(even, even)-, (even, odd)-, (odd, even)-, and (odd, odd)-phase
coefficients. Thus, depending on the type of motion detected, the
corresponding phase should be selected.

\subsection{Recovering the lost phases}

\begin{figure}
  \centering
  \begin{tabular}{ccc}
    \vbox{\png{f0_ohaar_LL}{300}} & \vbox{\png{f1_ohaar_LL}{300}} & \vbox{\png{f2_ohaar_LL}{300}} \\
    \vbox{\png{f0_ohaar_LH}{300}} & \vbox{\png{f1_ohaar_LH}{300}} & \vbox{\png{f2_ohaar_LH}{300}} \\
    \vbox{\png{f0_ohaar_HL}{300}} & \vbox{\png{f1_ohaar_HL}{300}} & \vbox{\png{f2_ohaar_HL}{300}} \\
    \vbox{\png{f0_ohaar_HH}{300}} & \vbox{\png{f1_ohaar_HH}{300}} & \vbox{\png{f2_ohaar_HH}{300}} \\
    & \vbox{\svg{f0_1_ohaar_LL}{300}} & \vbox{\svg{f0_2_ohaar_LL}{300}} \\
    & \vbox{\svg{f0_1_ohaar_LH}{300}} & \vbox{\svg{f0_2_ohaar_LH}{300}} \\
    & \vbox{\svg{f0_1_ohaar_HL}{300}} & \vbox{\svg{f0_2_ohaar_HL}{300}} \\
    & \vbox{\svg{f0_1_ohaar_HH}{300}} & \vbox{\svg{f0_2_ohaar_HH}{300}}
  \end{tabular}
  \caption{A demonstration of the shift-invariance of the ODWT. See this
    \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/11-MC_in_DWT_domain/ODWT_shift_invariance.ipynb}{notebook}.}
\label{fig:odwt}
\end{figure}

There are different alternatives for regenerating the phases discarded
by the subsamplers of the DWT. This is equivalent to compute the
Overcomplete DWT (ODWT)~\cite{mallat1999wavelet}.
\begin{enumerate}
\item Use the Algorithme \`a Trous~\cite{mallat1999wavelet}, which
  basically consists in removing the downsamplers, avoiding thus the
  aliasing artifacts generated by the noncompliance with the sampling
  theorem. See this
  \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/11-MC_in_DWT_domain/regenerating.ipynb}{notebook}.
\item Considering the previous experiments, it's easy to see that if
  we shift the signal one sample and perform the DWT, we get the
  ``lost'' phase. This method has been used to perform efficient MC in
  the DWT domain~\cite{park2000motion,li2001all}. See this
  \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/11-MC_in_DWT_domain/ODWT_with_delay.ipynb}{notebook}.
\item Apply some transform (such as for example, the CODWT
  (Complete-to-Overcomplete DWT)~\cite{andreopoulos2005complete} to
  the DWT to reconstruct the ODWT.
\end{enumerate}
The Fig.~\ref{fig:odwt} shows the shift invariance of the ODWT.

\subsection{About using the lost phases in IBMC}
Up to date, all the video codecs that use critically sampled IBMC also
use
\href{https://vicente-gonzalez-ruiz.github.io/video_compression/}{block-based
  motion compensation}. This technique divides the frames into
non-overlaping blocks and computes a motion vector for every block,
that provides a projection (a prediction) $\hat{P}$ of the reference
frame $R$ that must be as close as possible to the predicted frame
$P$. These blocks usually have a size of 16x16 pixels.

The use of blocks imples that:
\begin{enumerate}
\item If $N$ is the number of pixels in a frame, $N/256$ (for 16x16
  blocks) is the number of motion vectors. Therefore, if the motion
  vectors field has to be sent to the decoder, the data overhead is
  small (although this depends on the length of the representation of
  the texture).
\item All the coefficients that correspond to the same block has the
  same phase. Thus, if the phase also has to be sent to the decoder,
  again, the data overhead can be considered small.
\end{enumerate}

Unfortunately, there is a problem with mixing the phases. To
reconstruct the border pixels of the blocks, the adjacent (with the
same phase) coefficients must be also used by the decoder (see this
\href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/11-MC_in_DWT_domain/mixing_phases.ipynb}{notebook}). For
this reason, the size of the blocks affects to the compression ratio
(the smaller the blocks, the higher the number of adjacent
coefficients, and therefore, the lower the compression ratio). We can
think that this effect can be mitigated using larger block sizes, but
this will also affect to the compression ratio because the quality of
the predictions worsen with the increment of the size of the
blocks. This carries an optimization problem that it's hard to solve,
especially in real-time applications.

\subsection{MC in the Laplacian Pyramid}
The Laplacian Pyramid, that was proposed by Burt and
Adelson~\cite{burt1987laplacian} and has been used for the design of
spatially-scalable image and video codecs, such as
SHVC~\cite{sullivan2012overview}.

\begin{figure}
  \centering
  \begin{tabular}{ccc}
    \vbox{\png{f0_LP_level1}{300}} & \vbox{\png{f1_LP_level1}{300}} & \vbox{\png{f2_LP_level1}{300}} \\
    \vbox{\png{f0_LP_level0}{300}} & \vbox{\png{f1_LP_level0}{300}} & \vbox{\png{f2_LP_level0}{300}} \\
    & \vbox{\svg{f0_1_LP_level1}{300}} & \vbox{\svg{f0_2_LP_level1}{300}} \\
    & \vbox{\svg{f0_1_LP_level0}{300}} & \vbox{\svg{f0_2_LP_level0}{300}}
  \end{tabular}
  \caption{A demonstration of the shift-invariance of the LP. See this
    \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/11-MC_in_DWT_domain/LP_shift_invariance.ipynb}{notebook}.}
\label{fig:LP}
\end{figure}

The LP is a frame expansion that generates an expanded (not critical)
octave-band decomposition, and in some way, it can be considered one
of the precursors of the dyadic DWT. Unlike in the DWT, such expansion
is consequence of that the filters used for creating the LP are not
orthogonal and therefore, they do not cancel the aliasing between them
(see this
\href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/11-MC_in_DWT_domain/LP_is_not_critical.ipynb}{notebook}) when their downsampled outputs are added.

As a consequence of that the downsampling can not be used without
violating the perfect reconstruction, the redundancy in the LP tends
to 2 with the number of levels of the pyramid, which affects
negatively to the compression ratio. On the contrary, an advantage of
this is that the LP is shift-invariant in the high-frequency subband
(see the Fig.~\ref{fig:LP}), and of course, like the DWT, in the
low-frequency subband when the motion is a multiple of 2 (see the
Fig.~\ref{fig:LP})..

\begin{figure}
  \centering
  \begin{tabular}{ccc}
    \vbox{\png{f0_haar_iH}{300}} & \vbox{\png{f1_haar_iH}{300}} & \vbox{\png{f2_haar_iH}{300}} \\
    & \vbox{\svg{f0_1_haar_iH}{300}} & \vbox{\svg{f0_2_haar_iH}{300}}
  \end{tabular}
  \caption{A demonstration of the near shift-invariance in the [H]
    subband of the PDWT. See this
    \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/11-MC_in_DWT_domain/PDWT_shift_invariance.ipynb}{notebook}.}
\label{fig:PDWT}
\end{figure}

\subsection{Pyramid DWT (PDWT)}
Inspired in the LP, we can estimate and componsensate the motion in an alternative
representation of the DWT decomposition, that we
have called Pyramid DWT. In fact, a PDWT decomposition is a special
case of a LP where the filters are (bi)orthogonal DWT filters (in this
case, we say that the LP is a tight frame and therefore, it can be
downsampled without lossing the perfect reconstruction).

The 1-levels PDWT (that has two levels in its pyramid) of the frame
$X$ is defined by
\begin{equation}
  \{L, [H]\} = \{LL, \text{DWT}^{-1}(0, LH, HL, HH)\} = \{LL, \text{DWT}^{-1}(0, H)\},
  \label{eq:PDWT}
\end{equation}
where
\begin{equation}
  \{LL, LH, HL, HH\} = \text{DWT}(X).
  \label{eq:DWT}
\end{equation}

The $S$ levels CS-LPT$^S$ is computed simply by appliying the
Eq.~\ref{eq:PDWT} to the subband $L$, recursively.

\begin{figure}
  \centering
  \begin{tabular}{cc}
  \vbox{\svg{f0_1_haar_iH_error}{300}} &
  \vbox{\svg{f0_1_haar_LHHLHH_error}{300}}
  \end{tabular}
  \caption{Prediction error between frames 0 and 1, in the PDWT domain
    (left) and the DWT domain (right). See this
    \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/11-MC_in_DWT_domain/PDWT_shift_invariance.ipynb}{notebook}. The
    prediction error between frames 0 and 2 is zero. Remember that we transmit $H$, not $[H]$.}
\label{fig:PDWT_error}
\end{figure}

The PDWT is only near shift-invariant as it can be seen in the
Fig.~\ref{fig:PDWT}. However, it has several advantages:
\begin{enumerate}
\item The CS-LPT is as compact as the DWT.
\item The phases are not considered, which simplifies the
  ME/MC process and enables the use of any DWT filter.
\item The error generated by the lack of shift-invariance for the
  odd-type motion is smaller than for the DWT (see
  Fig.~\ref{fig:DWT}). As it can be seen in the
  Fig~\ref{fig:PDWT_error}, the energy of the error is the same in
  $[H]$ and $H$, but the energy is concentrated in only
  one critical subband (HL).
\end{enumerate}

\subsection{MC in the PDWT domain}
It's reasonable to expect that the motion of an object between the
frames $R$ and $P$ must move their low and the high frecuencies in the
same amount of pixels. With this idea in mind, we estimate the motion
in the $[H]$ subband using only the information provided by the
low-frequency subband $L$. More concretely, we implement:

\begin{figure}
  \centering
  \begin{tabular}{ccc}
    \vbox{\png{f0_haar_iL}{300}} & \vbox{\png{f1_haar_iL}{300}} & \vbox{\png{f2_haar_iL}{300}} \\
    & \vbox{\svg{f0_1_haar_iL}{300}} & \vbox{\svg{f0_2_haar_iL}{300}}
  \end{tabular}
  \caption{A demonstration of the near shift-invariance in the [L]
    (PDWT) subband. See this
    \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/milestones/11-MC_in_DWT_domain/iLL_shift_invariance.ipynb}{notebook}.}
\label{fig:iL}
\end{figure}

\begin{enumerate}

\item In order to increase the accuracy of the ME (see the
  Fig.~\ref{fig:iL}, we interpolate the low-frequency subbands of $R$
  and $P$:
  \begin{equation}
    [P.L] = \text{DWT}^{-1}(P.L, 0),
  \end{equation}
  \begin{equation}
    [R.L] = \text{DWT}^{-1}(R.L, 0).
  \end{equation}

\item Estimate the motion between $[P.L]$ and $[R.L]$.
  The output of this step is a motion vectors field
  $\overset{[P.L]\rightarrow [R.L]}{V}$, that describes how to project
  the $[P.L]$ onto $[R.L]$. Notice that $\overset{[P.L]\rightarrow
  [R.L]}{V}$ should also be a good candidate for mapping $P$ onto
  $R$.\footnote{Notice also that the number of vectors in
  $\overset{[P.L]\rightarrow [L.L]}{V}$ can be as high as the number
  of pixels in $R$ (and $P$), although this will depend on the
  accuracy of the ME/MC.}
  
\item Use $\overset{[P.L]\rightarrow [R.L]}{V}$ and $[R.L]$ to
  generate a prediction $[\hat{P}.L]$, and $[R.H]$ to generate a
  prediction $[\hat{P}.H]$. We define the prediction error in the
  low-frequency subband as
  \begin{equation}
    [E.L] = [P.L] - [\hat{P}.L],
    \label{eq:prediction_error_L}
  \end{equation}
  and the prediction error in the high-frequency subband
  as
  \begin{equation}
    [E.H] = [P.H] - [\hat{P}.H].
    \label{eq:prediction_error}
  \end{equation}
  Notice that $\overset{[P.L]\rightarrow [R.L]}{V}$ depends only on
  $R.L$ and $P.L$, not on the high frequency subbands.

\item Compute the Element-Wise (EW) minimum of $[P.L]$ and $[E.L]$:
  \begin{equation}
    \{[T],[M]\} = \text{EW-min}([P.L], [E.L])
    \label{eq:EW-min}
  \end{equation}
  where
  \begin{equation}
    [T]_{i,j}=\text{min}([P.L]_{i,j}, [E.L]_{i,j})
  \end{equation}
  and $[M]$ is a binary matrix defined by
  \begin{equation}
    [M]_{i,j} = \left\{
      \begin{array}{ll}
        0 & \text{if}~[P.L]_{i,j} < [E.L]_{i,j} ~(\text{I-type~coefficient})\\
        1 & \text{otherwise}~(\text{P-type~coefficient}).
      \end{array}
    \right.
    \label{eq:matrix}
  \end{equation}
  
\item Output
  \begin{equation}
    [O]_{i,j} = \left\{
      \begin{array}{ll}
        [P.H]_{i,j} & \text{if}~[M]_{i,j} = 0~(\text{I-type~coefficient})\\
        {[}E.H{]}_{i,j} & \text{otherwise}~(\text{P-type~coefficient}).
      \end{array}
    \right.
    \label{eq:output}
  \end{equation}
  Realize that it must hold that
  \begin{equation}
    \sigma^2_O \le \sigma^2_E,
    \label{eq:vars}
  \end{equation}
  where $\sigma^2$ denotes the variance, $O=\text{DWT}([O])$, and
  $E=\text{DWT}([O])$. Eq.~\ref{eq:vars} implies that
  \begin{equation}
    \text{CR}_O \ge \text{CR}_E
    \label{eq:crs}
  \end{equation}
  should hold, where CR stands for Compression Ratio.

  $O$ is the high-frequency subband that we will send from the
  encoder to the decoder, and it can be seen, it is composed of
  prediction error coefficients and original coefficients. Making a
  comparison with the procedure followed in most video coding
  standards, the prediction error coefficients represent predicted
  blocks (P-type blocks) or skipped blocks (S-type
  blocks)\footnote{S-type blocks are an special case of P-type blocks
  that have a prediction error so small that is more beneficial not to
  send their texture.}, and the original coefficients are equivalent
  to the intra(coded) blocks (I-type blocks).
\end{enumerate}


\subsection{Subpixel accuracy}
Objects in real scenes usually move a rational number of pixels, and
therefore, even when the input frames seems to be the same,
numerically they aren't. To deal with this drawback, interpolation can
be used to increase the resolution of the frames (MC in the frame
domain) or the subbands (MC in the subband domain), performing thus a
MC with increased accuracy.

Interpolation and DWT are both linear operators, and therefore, are
interchangeable. This means that we can interpolate the input frames and work as if the motion where integer-pixel, or we can interpolate the DWT coefficients. In both options, the number of 

\section{What you have to do?}
  
Please, using this
\href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/study_guide/10-MC_in_DWT_domain/DWT_shift_variance.ipynb}{notebook},
research the posibilities for performing MC of other DWTs available at
\href{https://pywavelets.readthedocs.io/en/latest/}{PyWavelets}.

\section{Timming}

Please, finish this notebook before the next class session.

\section{Deliverables}

None.

\section{What do I have to do?}

\begin{figure}
  \centering
  \myfig{graphics/problem}{3cm}{300}
  \caption{Basic encoding problem.}
  \label{fig:problem}
\end{figure}

Using the encoding system described in the Figure~\ref{fig:problem}, and defined by
\begin{equation}
  \left\{\
    \begin{array}{l}
      \tilde{\mathbf R} = \text{Q}_{\mathbf R}({\mathbf R}) \\
      \tilde{\mathbf E} = \text{Q}_{\mathbf E}\big({\mathbf P}-\overset{{\mathbf R}\rightarrow {\mathbf P}}{\mathbf M}(\tilde{\mathbf R})\big)
    \end{array}
  \right.
  \label{eq:forward}
\end{equation}
and
\begin{equation}
  \begin{array}{l}
    \tilde{\mathbf P} = \tilde{\mathbf E} + \overset{{\mathbf R}\rightarrow {\mathbf P}}{\mathbf M}(\tilde{\mathbf R}),
  \end{array}
  \label{eq:backward}
\end{equation}

find $\text{Q}_{\mathbf{R}}$ and $\text{Q}_{\mathbf{E}}$ that minimize in the RD domain (the RD curve of)
\begin{equation}
  \text{MSE}(\{\mathbf{R},\mathbf{P}\},\{\hat{\mathbf{R}},\hat{\mathbf{P}}\}) = \frac{\text{MSE}({\mathbf R},\hat{\mathbf R}) + \text{MSE}({\mathbf P},\hat{\mathbf P})}{2},
\end{equation}
set that
\begin{equation}
  \text{MSE}({\mathbf R},\tilde{\mathbf R}) = \text{MSE}({\mathbf P},\tilde{\mathbf P}).
  \label{eq:constant_quality}
\end{equation}
Equation~\ref{eq:constant_quality} indicates that all the decoded
frames should have the same distortion (from a human perception point
of view). Notice that the transform defined by the Equations
~\ref{eq:forward} and \ref{eq:backward} is not orthogonal and
therefore, the ``subbands'' $\tilde{\mathbf R}$ and
$\tilde{\mathbf P}$ are not independent. It can be seen that
$\text{Q}_{\mathbf R}$ affects to the selection of
$\text{Q}_{\mathbf E}$, because $\tilde{\mathbf R}$ is used as
reference for finding ${\mathbf E}$.


\section{References}

\renewcommand{\addcontentsline}[3]{}% Remove functionality of \addcontentsline
\bibliography{image-pyramids,DWT,motion-estimation,HEVC}

\begin{comment}

\subsection{Using (also) the lost phase in IBMC}
There are different alternatives for recovering the ``lost'' phases
(remember that we have two subbands, and two downsamplers) during the
DWT (in the 1D case). The result of this procedure is know as the
Overcomplete DWT (ODWT). Notice, however, that we are not interested
in encoding the ODWT domain, but in encoding the DWT that is more
compact. To achieve this, at least the following techniques can be
used:

\subsection{Performing IBMC in the 1-levels MDWT domain}
Once that the missing phases have been recovered, the MC procedure
between two frames (the reference frame $R$ and the predicted frame
$P$) that we are going to implement is:
\begin{enumerate}
\item  Therefore, estimate the motion between the overcomplete
  low-frequency subband of the reference frame $[R.L]$ and the
  overcomplete low-frequency subband of the predicted frame
  $[P.L]$. The output of this step is a motion vectors field
  $\overrightarrow{V}$, that describes how to project the $[R.L]$ onto
  $[P.L]$. Notice that $\overrightarrow{V}$ should be also a good
  candidate for mapping $R$ onto $P$.
  
\item Use $\overrightarrow{V}$ and $[R.H]$ (notice that in 2D case,
  $[R.H]=\{[R.HL], [R.LH], [R.HH]\}$) to generate a prediction
  $[\hat{P}.L]$. We define the prediction error
  in the overcomplete low-frequency subband as
  \begin{equation}
    [E.L] = [P.L] - [\hat{P}.L],
    \label{eq:prediction_error_L}
  \end{equation}
  and the prediction error in the overcomplete high-frequency subband
  as
  \begin{equation}
    [E.H] = [P.H] - [\hat{P}.H].
    \label{eq:prediction_error}
  \end{equation}

\item Selectively subsample $[E.H]$, picking out the right phase.
\end{enumerate}  


  Perform first the MC stage directly over the
  output of the analysis filters, and then, selectively downsample the
  result. Notice that the downsampler should select the right phase,
  depending on the type of motion detected (``odd'' or ``even''). This
  information (the selected phase), should be available at the
  decoder, along with the motion fields.
\item Delay-then-DWT: Perform two identical DWTs, one to the original
  signal, and the other to a one-sample delayed signal (remember than
  a movement of one pixel will change the phase at the output of the
  DWT). Thus, the the DWT applied to the original signal will generate
  one of the phases and the DWT applied to the delayed signal will
  generate the other one.
\item CODWT: Use the current (single phase) L and H coefficients to
  compute the missing phase, using the CODWT (Complete-to-Overcomplete
  DWT)~\cite{andreopoulos2005complete} (a new type of DWT applied to
  the DWT coefficients).
\end{enumerate}
Each alternative has pros and cons. If the DWT has been implemented
using convolution, MC-then-downsample should be a fast
alternative. However, if the DWT uses Lifting, Multiple-DWT-then-MC
should be fast also, because only one phase is computed by the
DWT. These two options can be used with any DWT filters. On the other
hand, CODWT needs specific designs form each DWT filters. Notice that, in any case, the solution is reached after using the ODWT domain.

---

is because a 1-pixel motion cannot
be represented selecting always the same phase at the downsamplers
(remember that with the downsampler we are basically selecting only
one the two possible phases of the output of the analysis filters: the
even samples or the odd samples, see this notebook).

Lets suppose that
the downsampler discards the odd coefficients (let's refer them as
odd-phase coefficients).

In this case, the even-phase cofficients of
the reference frame are the same than the odd-phase coefficients of
the predicted frame (this can be seen in this notebook). Therefore, in
the 1D case, when the motion is ``even''-type (that is, a displacement
of a even number of samples) we should compensate the even-phase
coefficients of the reference and the predicted frame, while when the
motion is ``odd''-type we should compensate the odd-phase coefficients
of the predicted frame with a prediction generated with the even-phase
coefficients of the reference frame, or viceversa.


\subsection{Near shift-invariance in the IDWT (Interpolated DWT) domain}
As it was commented before, the causant of the shift-variance in the
critically sampled DWT domain is the use of the downsamplers. At this
point we have basically two different alternatives:
\begin{enumerate}
\item Use the Algorithme \`a Trous (AaT)~\cite{mallat1999wavelet},
  which removes the downsamplers from the DWT, generating the so
  called Overcomplete DWT (ODWT). Notice that, because the
  downsamplers are removed, the aliasing artifacts produced by the
  downsamplers is also avoided.
\item Approximate the AaT coefficients by interpolating the DWT
  coefficients using the DWT synthesis filters. In this case, the
  aliasing is not avoided, but the shift-variance problem is
  reduced.
\end{enumerate}

\subsection{Subpixel accuracy}
Objects in real scenes usually move a rational number of pixels, and
therefore, even when the input frames seems to be the same,
numerically they aren't. To deal with this drawback, interpolation can
be used to increase the resolution of the frames (MC in the frame
domain) or the subbands (MC in the subband domain), performing thus a
MC with increased accuracy.

Interpolation and DWT are both linear operators, and therefore, are
interchangeable. This means that we can interpolate the input frames and work as if the motion where integer-pixel, or we can interpolate the DWT coefficients. In both options, the number of 

\end{comment}


\begin{comment}
\subsection{Motion compensation in action}
\begin{figure}
  \svg{R}{1000}
  \svg{P}{1000}
  \svg{y_motion}{1000}
  \svg{x_motion}{1000}
  \svg{hat_P}{1000}
  \svg{without_ME}{1000}
  \svg{with_ME}{1000}
  \caption{Effect of ME (using OF) over the temporal redundancy
    removal.}
  \label{fig:MC}
\end{figure}
The Fig.~\ref{fig:MC} shows an example the performace of the use of
OF, comparing the prediction error generated with and without ME. In
this experiment, a motion vector has been computed between each point
of ${\mathbf P}$ and ${\mathbf R}$. As it can be seen, the OF can
reduce the temporal redundancy significantly.
\end{comment}

\begin{comment}
The OF~\cite{horn1981determining} tries to establish connections between the pixels of
the frames $P$ and $R$ supposing that:
\begin{enumerate}
\item $P$ and $R$ are adjacent in time (if $R$ was taken at time $t$,
  $P$ is taken at time $dt+t$) and therefore, similar in
  content.
\item Similarity between images implies that the pixels in both
  frames, $R$ and $P$, will have the same luminance. If $I(x,y,t)$
  measures the luminance of the pixel $(x,y)$ of the frame $R$,
  similarity can be modeled by
  \begin{equation}
    I(x+dx, y+dy, t+dt) = I(x,y,t),
    \label{eq:similarity}
  \end{equation}
  where $I(x+dx, y+dy, t+dt)$ is the corresponding pixel in the frame
  $P$. The first part of the Eq.~\ref{eq:similarity} can be also
  computed by (using the first-order Taylor expansion) as
  \begin{equation}
    I(x+dx, y+dy, t+dt) = I(x,y,t) + \frac{\partial I}{\partial x}dx + \frac{\partial I}{\partial y}dy + \frac{\partial I}{\partial t}dt,
    \label{eq:taylor_exp}
  \end{equation}
  andtherefore, it must be true that
  \begin{equation}
    \frac{\partial I}{\partial x}dx + \frac{\partial I}{\partial y}dy + \frac{\partial I}{\partial t}dt = 0.
    \label{eq:constraint}
  \end{equation}
  Dividing by $dt$, we finally get that
  \begin{equation}
    \frac{\partial I}{\partial x}\frac{dx}{dt} + \frac{\partial I}{\partial y}\frac{dy}{dt} + \frac{\partial I}{\partial t} = 0.
  \end{equation}
\item Adjacent pixels follow parallel
  trajectories~\cite{horn1981determining}, with basically means that
  neighbor pixels will have similar motion.
\end{enumerate}
\end{comment}

\begin{comment}
\begin{equation}
  \begin{bmatrix}
    \tilde{\mathbf R} \\
    \tilde{\mathbf E}
  \end{bmatrix} =
  \begin{bmatrix}
    \text{Q}_{\mathbf R}(\cdot) & 0 \\
    -\overset{{\mathbf R}\rightarrow {\mathbf P}}{\text{ME}}(\text{Q}_{\mathbf E}(\cdot)) & 1
  \end{bmatrix}
  \begin{bmatrix}
    {\mathbf R } \\
    {\mathbf P}
  \end{bmatrix}
  \label{eq:forward}
\end{equation}
and
\begin{equation}
  \begin{bmatrix}
    \tilde{\mathbf R} \\
    \tilde{\mathbf P}
  \end{bmatrix} =
  \begin{bmatrix}
    \text{Q}^{-1}_{\mathbf R}(\cdot) & 0 \\
    \overset{{\mathbf R}\rightarrow {\mathbf P}}{\text{ME}}(\text{Q}^{-1}_{\mathbf E}(\cdot)) & 1
  \end{bmatrix}
  \begin{bmatrix}
    \tilde{\mathbf R} \\
    \tilde{\mathbf E},
  \end{bmatrix}
  \label{eq:backward}
\end{equation}
\end{comment}
