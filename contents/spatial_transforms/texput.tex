% Emacs, this is -*-latex-*-

% TODO: In the DWT domain, use block-based neural compression using autoencoders

% https://stackabuse.com/autoencoders-for-image-reconstruction-in-python-and-keras/
% https://github.com/FireFYF/modulatedautoencoder
% https://www.tensorflow.org/tutorials/generative/data_compression
% https://www.youtube.com/watch?v=x_q7cZviXkY ( PCS 2018 – Learned Image Compression )
% https://keras.io/examples/generative/vq_vae/
% https://jhui.github.io/2017/03/06/Variational-autoencoders/
% https://towardsdatascience.com/difference-between-autoencoder-ae-and-variational-autoencoder-vae-ed7be1c038f2
% https://deepai.org/machine-learning-glossary-and-terms/prior-probability
% https://towardsdatascience.com/auto-regressive-generative-models-pixelrnn-pixelcnn-32d192911173
% https://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w50/Le_Tan_DeepVQ_A_Deep_CVPR_2018_paper.pdf
% https://www.deeplearningbook.org/
% https://arxiv.org/pdf/1711.00937.pdf (Neural Discrete Representation Learning)
% https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/vq_vae.ipynb (Vector-Quantized Variational Autoencoders)
% https://www.tensorflow.org/tutorials/generative/autoencoder
% https://blog.paperspace.com/autoencoder-image-compression-keras/
% https://www.analyticsvidhya.com/blog/2021/06/complete-guide-on-how-to-use-autoencoders-in-python/
% https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/vae.ipynb
% https://blog.keras.io/building-autoencoders-in-keras.html
% https://keras.io/examples/generative/vae/
% https://arxiv.org/pdf/1611.01704.pdf (END-TO-END OPTIMIZED IMAGE COMPRESSION)
% https://www.tensorflow.org/tutorials/generative/data_compression
% https://www.researchgate.net/profile/Asma-Salem-4/publication/319764377_Image_Compression_Using_Wavelet_and_Subband_Based_Transform/links/59bbe3d2458515e9cfc79d0d/Image-Compression-Using-Wavelet-and-Subband-Based-Transform.pdf
% https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/resources/lecture-31-change-of-basis-image-compression/
% https://arxiv.org/abs/1908.08988 (Neural Image Compression and Explanation)
% https://arxiv.org/abs/2011.03029 (CompressAI: a PyTorch library and evaluation platform for end-to-end compression research)
% https://heartbeat.comet.ml/a-2019-guide-to-deep-learning-based-image-compression-2f5253b4d811
% https://scelesticsiva.github.io/assets/image_compression_report.pdf
% https://interdigitalinc.github.io/CompressAI/intro.html
% https://arxiv.org/abs/1802.01436 (Variational image compression with a scale hyperprior)
% 

\input{../../definitions}
\title{\href{https://sistemas-multimedia.github.io/contents/spatial_transforms/}{Spatial Transforms}}

\maketitle

\tableofcontents

\section{Spatial decorrelation}
%{{{

Spatial transforms used in image and video compression exploit the
statistical correlation (and perceptual redundancy\footnote{We will
  see this with more detail later in this course.}) that the pixels
show as a consequence of the spatial (2D) correlation that is present
in most of the images (and video frames). For example, some textures of
an image can occur more than once (in the same image). Also, usually
happens that neighbor pixels have similar values.

While color transforms are pixel-wise computations, spatial transforms
are image-wise (or at least, block-wise). This means that the
transform inputs an image (of pixels) and outputs a matrix of
coefficients, which generally express the resemblance between the
image and a set of basis functions, usually orthogonal. For example,
after using the 2D-DCT (two-dimensional Discrete Cosine
Transform)~\cite{vruiz__DCT}, (the index of) each coefficient
represents a different spatial frequency\footnote{That depends on the
  position of the coefficient in the transformed domain.} and its
value, the amount of the corresponding basis found in the image. In
the case of the dyadic 2D-DWT~\cite{vruiz__DWT}, the coefficients
``speak'' additionally about a spatial resolution in the image pyramid
and position inside of the pyramid level.

%}}}

\section{Benefits of spatial transforms}
%{{{

Most spatial transforms provide:
\begin{enumerate}
\item \textbf{Energy concentration:} Usually, a small set of
  (low-frequency) coefficients represents most of the information
  (energy) of the image. This decreases the entropy\footnote{When the
    entropy is decreased while the information is preserved, this
    usually means that an entropy encoding algorithm will perform
    better.} and increases the range of the quantization step sizes,
  because the dynamic range of the coefficients is higher than the
  dynamic range of the pixels\footnote{Quantization is a discrete
    operation constrained by the number of bits used to represent the
    quantization indexes. When the dynamic range of a signal is high,
    this makes possible to use more quantization levels and therefore,
    a higher number of available RD points.}.
\item \textbf{Low/High frequency analysis:} The human visual system is
  more sensitive to the low frequencies (for this reason, the
  \href{https://en.wikipedia.org/wiki/Contrast_(vision)#Contrast_sensitivity}{contrast
    sensitiviy function} is not flat). This means that we can quantize
  more severely the high frequencies without generating a perceptible
  distortion.
\item \textbf{Multiresolution:} Depending on the transform, it is
  possible to reconstruct the original image by resolution
  levels~\cite{vruiz__DWT}. This option can be interesting when the
  resolution at which the image must be reconstructed is not known a
  priori. For example, JPEG 2000 (which is based on the 2D-DWT) is
  used in \href{https://jpeg.org/jpeg2000/applications.html}{digital
    cinema} because, although movie players do not have the same
  resolution in all movie theaters, the same code-stream (with the
  maximum resolution) can be used in all of them.
\end{enumerate}

%}}}

\section{Scalar quantization and rate-distortion optimization in the transform domain}
%{{{

Scalar quantization is efficient in the transform domain because the
coefficients are decorrelated. The next logical step (after
quantization) is the entropy coding of the quantization indexes. Here,
depending on how the coefficients are quantized, we can trace
different RD curves (all of them starting (and finising) in the same
(rate, distortion) point). For example, if we compress each
subband\footnote{In the case of a spatial transform, a subband is form
  by all the coefficients that describe the same frequency components
  in different areas or resolutions (when available) of the image.}
independently, we must find the quantization step sizes that select
the same slope in the RD curve of each
subband~\cite{vruiz__information_theory}. A RD curve is a discrete
convex function where each line that connects adjacent RD points has a
slope.  In this context, given a target distortion or rate, the
quantization step size used in each subband should generate the same
slope.

%}}}

\section{Learned transforms}
%{{{

Using machine learning techniques (for example, with an artificial
neural network), it is possible to build a machine-krafted transform,
specifically tuned for some type of images, or at least, capable of
determining specific features in the images. This technique receive
the name of analysis–synthesis dictionary learning, linear
autoencoders, or learned transform coding.

Potentially, learned-based image (and video) compressors are adaptive
algorithms that can be more efficient than those in which the
transforms are pre-defined.

%}}}

\section{2D-partitioning}

Depending on the content of an image, it can be necessary to divide
the image into 2D chunks (usually called \emph{tiles} or
\emph{blocks}), and encode each chunk independently. In general:
\begin{enumerate}
\item Tiles are used when the image is made up of very different areas
  (for example, with text and natural picture content). Tiles are
  usually rectangular but can have any size, and are usually defined
  attending to RD or perceptual issues (for example, text is not well
  compressed by lossy codecs).
\item Blocks are smaller than tiles and, in most of cases,
  squared. The block partition can be adaptive and, in this case,
  should be found using RDO (Rate Distortion Optimization)\footnote{If
    no other more important requirement exists, such as
    multiresolution.}.
\end{enumerate}

\section*{To-Do}
\begin{enumerate}
\item \textbf{Learned Block Transform (LBT)}: As other transforms do,
  a neural network can learn to perform energy
  concentration. Implement a 3-layers
  \href{https://en.wikipedia.org/wiki/Autoencoder}{autoencoder}
  trained (using
  \href{https://en.wikipedia.org/wiki/Supervised_learning}{supervised
    learning}) to maximize, for example\footnote{The function to
  optimize during the training of the network can be any, as long as
  such function increases the sparsity of the transform
  coefficients.}, the
  \href{https://web.stanford.edu/class/ee398a/handouts/lectures/07-TransformCoding.pdf}{transform
    coding gain} \cite{sayood2017introduction}, for each different
  image to compress. The size (and shape) of the input layer and the
  output layer should the same, but configurable (typically between
  4x4, 8x8 and 16x16). Notice that, the output of the neurons of the
  center layer should define the transform coefficients, that the
  (learned) weights of the connections between the input layer and the
  center layer should conform the forward transform matrix, and that
  the weights of the connections between the center layer and the
  output layer should define the inverse transform matrix.
\end{enumerate}

\section{Resources}
%{{{

\begin{enumerate}
\item Usage of \href{https://github.com/Sistemas-Multimedia/VCF/blob/main/notebooks/2D-DWT.ipynb}{2D-DWT in VCF}.
\item Usage of \href{https://github.com/Sistemas-Multimedia/VCF/blob/main/notebooks/2D-DCT.ipynb}{2D-DCT in VCF}.
\item
  \href{https://github.com/vicente-gonzalez-ruiz/DCT2D/blob/master/src/DCT2D/YCoCg_2D_DCT_SQ.ipynb}{Image
    Compression with YCoCg + 2D-DCT}.
\item
  \href{https://www.tensorflow.org/tutorials/generative/data_compression}{Learned
    data compression}.
\item
  \href{https://github.com/vicente-gonzalez-ruiz/learned_image_compression/blob/main/notebooks/LIC.ipynb}{Learned
    Image Compression (LIC) using auto-encoders}.
\item
  \href{https://github.com/vicente-gonzalez-ruiz/learned_image_compression/blob/main/notebooks/AutoencoderBlockCompression.ipynb}{AutoencoderBlockCompression.ipynb}.
\item
  \href{https://github.com/fchollet/deep-learning-with-python-notebooks}{Companion
    Jupyter notebooks for the book ``Deep Learning with
    Python''}~\cite{chollet2021deep}.
\end{enumerate}

%}}}

\begin{comment}
\section*{To-Do}
%{{{

\begin{enumerate}
\item Add a new option to
  \href{https://github.com/Sistemas-Multimedia/VCF/blob/main/src/2D-DWT.py}{the
    VCF codec \texttt{2D-DWT.py}}, that throught RDO, determine the
  optimal DWT basis (defined in
  \href{https://pywavelets.readthedocs.io/en/latest/}{PyWavelets}). \textbf{2
    points}.
\item Create a new VCF image codec (similar to
  \href{https://github.com/Sistemas-Multimedia/VCF/blob/main/src/2D-DCT.py}{\texttt{2D-DCT.py}})
  where the 2D-DCT is replaced by an ANN (artificial neural
  network). The forward transform should maximize the concentration of
  energy, and the inverse transform should restore the original
  block. To avoid transmit the coefficients of the ANN, use a model
  previously trained to compute the 2D-DCT, and retrain the model
  using only the previously encoded block of the image. For example, when
  encoding the second block, use the first block to fine-tune the
  model. For the third block, use the second block to retrain the
  model, and so on. \textbf{10 points}.
\item Create a new image codec (similar to \texttt{2D-DWT.py}) where
  the coefficients of the filters used in the 2D-DWT are determined by
  an ANN trained to maximize the transform gain (energy
  concentration). Initially use the coefficients of the filters of a
  well-known DWT kernel, and in subsequent iterations (levels) of the
  transform used the previously analyzed spatial resolution levels to
  fine-tune the coefficients. \textbf{10 points}.
%\item If the right coefficients in the DWT domain are multiplied by a
%  power of 2 before quantization, we can define a ROI of arbitrary
%  shape at compression time~\cite{vruiz__JPEG2000}. Include this
%  possibility in the DWT-based compression pipeline of VCF.
% \item Modify CVF to use SPIHT.
  % \item LPT can replace the DWT. Can we travel from LPT to DWT using any filter?
  % \item Irregular Subsampling + normalized convolution (ver tesis Gunnan Farneback).
\end{enumerate}
%}}}
\end{comment}

\section{References}
%{{{

\renewcommand{\addcontentsline}[3]{}% Remove functionality of \addcontentsline
\bibliography{projects,signal_processing,neural_nets,information_theory,JPEG2000,data_compression}

%}}}

\begin{comment}

  
\section{Tight Laplacian Pyramid Transform (TLPT)}
%{{{

Pyramid frames were introduced in 1983 by Burt and Adelson. In some
way, they can be considered one of the precursors of the wavelet
octave band decompositions.

Technically, the LPT is a frame expansion that generates an expanded
octave-band decomposition. The ratio of redundancy in the
decomposition $R\rightarrow 2$ with the number of levels.

Because of the shift invariance, MC can be incorporated in the enhancememt layer.

The LP with orthogonal filter is a tight frame.

%}}}

\section{Critially Sampled LPT (CS-LPT)}
%{{{

Technically, the LPT is a frame expansion, that generates an expanded
octave-band decomposition. The ratio of redundancy in the
decomposition $R\rightarrow 2$ to the number of DWT levels.

The 2D-DWT can be used on sequences of images, by simply iterating as described in the Algorithm~\ref{alg:MDWT}. $I$ is the number of
images in the sequence $V$.

\begin{pseudocode}{\text{MDWT}}{V}
  \label{alg:MDWT}
  \FOR i \GETS 0 \TO I-1 \DO
  V_i = \text{2D-DWT}(V_i)
\end{pseudocode}

%}}}

\section{Autoencoder Latent Representation}
%{{{

% Johannes Ballé, Valero Laparra, and Eero P. Simoncelli. 2017. End-to-end Optimized Image Compression. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017

In ANN-based image compression, an autoencoder can be used to find the
latent representation of an (input) image. The compressor is the first
part of the autoencoder and the decompressor is the second
part. During compression, the latent representation is quantized
and the entropy compressed.

%}}}

\end{comment}
