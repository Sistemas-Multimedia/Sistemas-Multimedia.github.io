% Emacs, this is -*-latex-*-

% TODO: In the DWT domain, use block-based neural compression using autoencoders

% https://stackabuse.com/autoencoders-for-image-reconstruction-in-python-and-keras/
% https://github.com/FireFYF/modulatedautoencoder
% https://www.tensorflow.org/tutorials/generative/data_compression
% https://www.youtube.com/watch?v=x_q7cZviXkY ( PCS 2018 â€“ Learned Image Compression )
% https://keras.io/examples/generative/vq_vae/
% https://jhui.github.io/2017/03/06/Variational-autoencoders/
% https://towardsdatascience.com/difference-between-autoencoder-ae-and-variational-autoencoder-vae-ed7be1c038f2
% https://deepai.org/machine-learning-glossary-and-terms/prior-probability
% https://towardsdatascience.com/auto-regressive-generative-models-pixelrnn-pixelcnn-32d192911173
% https://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w50/Le_Tan_DeepVQ_A_Deep_CVPR_2018_paper.pdf
% https://www.deeplearningbook.org/
% https://arxiv.org/pdf/1711.00937.pdf (Neural Discrete Representation Learning)
% https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/vq_vae.ipynb (Vector-Quantized Variational Autoencoders)
% https://www.tensorflow.org/tutorials/generative/autoencoder
% https://blog.paperspace.com/autoencoder-image-compression-keras/
% https://www.analyticsvidhya.com/blog/2021/06/complete-guide-on-how-to-use-autoencoders-in-python/
% https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/vae.ipynb
% https://blog.keras.io/building-autoencoders-in-keras.html
% https://keras.io/examples/generative/vae/
% https://arxiv.org/pdf/1611.01704.pdf (END-TO-END OPTIMIZED IMAGE COMPRESSION)
% https://www.tensorflow.org/tutorials/generative/data_compression
% https://www.researchgate.net/profile/Asma-Salem-4/publication/319764377_Image_Compression_Using_Wavelet_and_Subband_Based_Transform/links/59bbe3d2458515e9cfc79d0d/Image-Compression-Using-Wavelet-and-Subband-Based-Transform.pdf
% https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/resources/lecture-31-change-of-basis-image-compression/
% https://arxiv.org/abs/1908.08988 (Neural Image Compression and Explanation)
% https://arxiv.org/abs/2011.03029 (CompressAI: a PyTorch library and evaluation platform for end-to-end compression research)
% https://heartbeat.comet.ml/a-2019-guide-to-deep-learning-based-image-compression-2f5253b4d811
% https://scelesticsiva.github.io/assets/image_compression_report.pdf
% https://interdigitalinc.github.io/CompressAI/intro.html
% https://arxiv.org/abs/1802.01436 (Variational image compression with a scale hyperprior)
% 

\input{../../definitions}
\title{\SM{} - \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/tree/master/contents/spatial_transforms}{Spatial Transforms}}

\maketitle

\tableofcontents

\section{Spatial decorrelation}
%{{{

Spatial transforms used in image and video compression exploit the
statistical correlation (and perceptual redundancy\footnote{We will
  see this with more detail later in this course.}) that the pixels
show as a consequence of the spatial (2D) correlation that is present
in most of the images (and video frames). For example, some areas of
an image can occur more than once (in the same image). Also, usually
happens that neighbor pixels have similar values.

While color transforms are pixel-wise computations, spatial transforms
are image-wise. This means that the transform inputs an image (of
pixels) and outputs a matrix of coefficients, which generally express
the resemblance between the image and a set of basis functions,
usually orthogonal. For example, after using the 2D-DCT
(two-dimensional Discrete Cosine Transform)~\cite{vruiz__DCT}, (the
index of) each coefficient represents a different spatial
frequency\footnote{That depends on the position of the coefficient in
  the transformed domain.} and its value, the amount of the
corresponding basis found in the image. In the case of the
2D-DWT~\cite{vruiz__DWT}, the coefficients ``speak'' additionally
about a spatial resolution in the image pyramid and position inside of
the pyramid level.

%}}}

\section{Benefits of spatial transforms}
%{{{

Most spatial transforms provide:
\begin{enumerate}
\item \textbf{Energy concentration:} Usually, a small set of
  (low-frequency) coefficients represents most of the information
  (energy) of the image. This decreases the entropy\footnote{When the
    entropy is decreased while the information is preserved, this
    usually means that an entropy encoding algorithm will perform
    better.} and increases the range of the quantization step sizes,
  because the dynamic range of the coefficients is higher than the
  dynamic range of the pixels\footnote{Quantization is a discrete
    operation constrained by the number of bits used to represent the
    quantization indexes. When the dynamic range of a signal is high,
    this makes possible to use more quantization levels and therefore,
    a higher number of available RD points.}.
\item \textbf{Low/High frequency analysis:} The human visual system is
  more sensitive to the low frequencies (for this reason, the
  \href{https://en.wikipedia.org/wiki/Contrast_(vision)#Contrast_sensitivity}{contrast
    sensitiviy function} is not flat). This means that we can quantize
  more severely the high frequencies without generating a perceptible
  distortion.
\item \textbf{Multiresolution:} Depending on the transform, it is
  possible to reconstruct the original image by resolution
  levels~\cite{vruiz__DWT}. This option can be interesting when the
  resolution at which the image must be reconstructed is not known a
  priori. For example, JPEG 2000 (which is based on the 2D-DWT) is
  used in \href{https://jpeg.org/jpeg2000/applications.html}{digital
    cinema} because, although movie players do not have the same
  resolution in all movie theaters, the same code-stream (with the
  maximum resolution) can be used in all of them.
\end{enumerate}

%}}}

\section{Scalar quantization and rate/distortion optimization in the transform domain}
%{{{

Scalar quantization is efficient in the transform domain because the
coefficients are decorrelated. The next logical step (after
quantization) is the entropy coding of the quantization indexes. Here,
depending on how the coefficients are quantized, we can trace
different RD curves (all of them starting (and finising) in the same
distortion). For example, if we compress each subband\footnote{In the
  case of a spatial transform, a subband is form by all the
  coefficients that describe the same frequency components in
  different areas or resolutions (when available) of the image.}
independently, we must find the quantization step sizes that select
the same slope in the RD curve of each
subband~\cite{vruiz__information_theory}. A RD curve is a discrete
convex function where each line that connects adjacent RD points has a
slope. Futhermore, each RD point generate a line (with a corresponding
slope) between it and the lowest-quality RD point. In this last
context, given a target distortion or rate, the quantization step size
used in each subband should generate the same slope.

%}}}

\section{Learned transforms}
%{{{

Using machine learning techniques (for example, with an artificial
neural network), it is possible to build a machine-krafted transform,
specifically tuned for some type of images, or at least, capable of
determining specific features in the images.

Potentially, learned-based image (and video) compressors are adaptive
algorithms that can be more efficient than those in which the
transforms are pre-defined.

%}}}

\section{2D-partitioning}

Depending on the content of the image, it can be necessary to divide
the image into 2D chunks (usually called \emph{tiles} or
\emph{blocks}), and encode each chunk independently. In general:
\begin{enumerate}
\item Tiles are used when the image is made up of very different areas
  (for example, with text and natural content). Tiles are usually
  rectangular but can have any size, and are usually defined attending
  to RD or perceptual issues (for example, text is not well compressed by
  lossy configurations).
\item Blocks are smaller than tiles and, in most of cases,
  squared. The block partition can be adaptive and, in this case,
  should be found using RDO (Rate Distortion Optimization).
\end{enumerate}

\section{Resources}
%{{{

\begin{enumerate}
\item Use of \href{https://github.com/Sistemas-Multimedia/VCF/blob/main/src/DWT.py}{2D-DWT in VCF}.
\item
  \href{https://github.com/vicente-gonzalez-ruiz/DCT2D/blob/master/src/DCT2D/YCoCg_2D_DCT_SQ.ipynb}{Image
    Compression with YCoCg + 2D-DCT}.
\item
  \href{https://www.tensorflow.org/tutorials/generative/data_compression}{Learned
    data compression}.
\item
  \href{https://github.com/vicente-gonzalez-ruiz/learned_image_compression/blob/main/LIC.ipynb}{Learned
    Image Compression (LIC) using auto-encoders}.
\item
  \href{https://github.com/fchollet/deep-learning-with-python-notebooks}{Companion
    Jupyter notebooks for the book ``Deep Learning with
    Python''}~\cite{chollet2021deep}.
\end{enumerate}

%}}}

\section{To-Do}
%{{{

\begin{enumerate}
\item Modify VCF to use the block-based 2D-DCT in the compression
  pipeline. Complexity 3.
\item Using RDO, determine the optimal color transform in the codec
  \texttt{2D-DCT.py}. Complexity 2.
\item Using RDO, determine the optimal color transform in the codec
  \texttt{2D-DWT.py}. Complexity 2.
\item Using RDO, determine the optimal number of levels of the 2D-DWT
  in the codec \texttt{2D-DWT.py}. Complexity 2.
\item Using RDO, determine the optimal DWT basis (defined in
  \href{https://pywavelets.readthedocs.io/en/latest/}{PyWavelets}) in
  the codec \texttt{2D-DWT.py}. Complexity 3.
\item Create a new image codec (similar to \texttt{2D-DWT.py} and
  \texttt{2D-DCT.py}) where the latent space generated by an
  autoencoder is entropy encoded (in other words, replace the 2D-DCT
  or the 2D-DWT by an autoencoder). Complexity 15.
\item Autoencoders can learn to analyze signals represented in any
  domain. Create a new DCT- or DWT-based image codec where the
  autoencoder is applied to the transform domain. Complexity 20.
%\item If the right coefficients in the DWT domain are multiplied by a
%  power of 2 before quantization, we can define a ROI of arbitrary
%  shape at compression time~\cite{vruiz__JPEG2000}. Include this
%  possibility in the DWT-based compression pipeline of VCF.
% \item Modify CVF to use SPIHT.
  % \item LPT can replace the DWT. Can we travel from LPT to DWT using any filter?
  % \item Irregular Subsampling + normalized convolution (ver tesis Gunnan Farneback).
\end{enumerate}

%}}}

\section{References}
%{{{

\renewcommand{\addcontentsline}[3]{}% Remove functionality of \addcontentsline
\bibliography{projects,signal_processing,neural_nets,information_theory,JPEG2000}

%}}}

\begin{comment}

  
\section{Tight Laplacian Pyramid Transform (TLPT)}
%{{{

Pyramid frames were introduced in 1983 by Burt and Adelson. In some
way, they can be considered one of the precursors of the wavelet
octave band decompositions.

Technically, the LPT is a frame expansion that generates an expanded
octave-band decomposition. The ratio of redundancy in the
decomposition $R\rightarrow 2$ with the number of levels.

Because of the shift invariance, MC can be incorporated in the enhancememt layer.

The LP with orthogonal filter is a tight frame.

%}}}

\section{Critially Sampled LPT (CS-LPT)}
%{{{

Technically, the LPT is a frame expansion, that generates an expanded
octave-band decomposition. The ratio of redundancy in the
decomposition $R\rightarrow 2$ to the number of DWT levels.

The 2D-DWT can be used on sequences of images, by simply iterating as described in the Algorithm~\ref{alg:MDWT}. $I$ is the number of
images in the sequence $V$.

\begin{pseudocode}{\text{MDWT}}{V}
  \label{alg:MDWT}
  \FOR i \GETS 0 \TO I-1 \DO
  V_i = \text{2D-DWT}(V_i)
\end{pseudocode}

%}}}

\section{Autoencoder Latent Representation}
%{{{

% Johannes BallÃ©, Valero Laparra, and Eero P. Simoncelli. 2017. End-to-end Optimized Image Compression. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017

In ANN-based image compression, an autoencoder can be used to find the
latent representation of an (input) image. The compressor is the first
part of the autoencoder and the decompressor is the second
part. During compression, the latent representation is quantized
and the entropy compressed.

%}}}

\end{comment}
