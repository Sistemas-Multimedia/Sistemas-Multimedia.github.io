{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e367482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tied Linear Autoencoder for Learned Image Transform (8x8)\n",
    "#\n",
    "# Provided by ChatGPT after being prompted with:\n",
    "#\n",
    "# I would like to know if I can achieve such learned transform using\n",
    "# a 3-layers autoencoder, where the weights between the input layer and\n",
    "# the center layer define the forward transform, and the weights between\n",
    "# the center layer and the output layer define the inverse transform.\n",
    "#\n",
    "# Untested!!\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load example image (grayscale)\n",
    "img = Image.open('example.png').convert('L')\n",
    "img = img.resize((256,256))\n",
    "img_np = np.array(img)/255.0\n",
    "\n",
    "# extract 8x8 patches\n",
    "patches=[]\n",
    "for i in range(0,256-7,1):\n",
    "    for j in range(0,256-7,1):\n",
    "        p=img_np[i:i+8,j:j+8].reshape(-1)\n",
    "        patches.append(p)\n",
    "patches=np.array(patches,dtype=np.float32)\n",
    "\n",
    "dataset=TensorDataset(torch.tensor(patches))\n",
    "loader=DataLoader(dataset,batch_size=512,shuffle=True)\n",
    "\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class TiedAE(nn.Module):\n",
    "    def __init__(self,dim):\n",
    "        super().__init__()\n",
    "        self.encoder=nn.Linear(dim,dim,bias=False)\n",
    "    def forward(self,x):\n",
    "        W=self.encoder.weight\n",
    "        z=F.linear(x,W)\n",
    "        xh=F.linear(z,W.t())\n",
    "        return xh,z\n",
    "\n",
    "def orth_loss(W):\n",
    "    I=torch.eye(W.size(0),device=W.device)\n",
    "    return torch.norm(W@W.t()-I)**2\n",
    "\n",
    "model=TiedAE(64).to(device)\n",
    "opt=optim.Adam(model.parameters(),lr=1e-3)\n",
    "lam_ortho=1e-3\n",
    "lam_l1=1e-4\n",
    "\n",
    "for epoch in range(5):\n",
    "    for (batch,) in loader:\n",
    "        batch=batch.to(device)\n",
    "        xh,z=model(batch)\n",
    "        loss=F.mse_loss(xh,batch)+lam_ortho*orth_loss(model.encoder.weight)+lam_l1*z.abs().mean()\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "# visualize learned basis\n",
    "W=model.encoder.weight.detach().cpu().numpy()\n",
    "fig,axs=plt.subplots(8,8,figsize=(8,8))\n",
    "for i in range(64):\n",
    "    axs[i//8][i%8].imshow(W[i].reshape(8,8),cmap='gray')\n",
    "    axs[i//8][i%8].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('/mnt/data/basis.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
