<!DOCTYPE html> 
<html lang='en-US' xml:lang='en-US'> 
<head> <title>Sistemas Multimedia - Spatial Transforms</title> 
<meta charset='utf-8' /> 
<meta content='TeX4ht (https://tug.org/tex4ht/)' name='generator' /> 
<meta content='width=device-width,initial-scale=1' name='viewport' /> 
<link href='index.css' rel='stylesheet' type='text/css' /> 
<meta content='index.tex' name='src' /> 
<script>window.MathJax = { tex: { tags: "ams", }, }; </script> 
 <script async='async' id='MathJax-script' src='https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js' type='text/javascript'></script>  
</head><body>
   <div class='maketitle'>
                                                                  

                                                                  
                                                                  

                                                                  

<h2 class='titleHead'><a href='https://sistemas-multimedia.github.io/'>Sistemas Multimedia</a> - <a href='https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/tree/master/contents/spatial_transforms'>Spatial Transforms</a></h2>
 <div class='author'><a href='https://vicente-gonzalez-ruiz.github.io/'><span class='ecrm-1200'>Vicente González Ruiz</span></a> <span class='ecrm-1200'>- </span><a href='https://www.ual.es/universidad/departamentos/informatica'><span class='ecrm-1200'>Departamento de Informática</span></a> <span class='ecrm-1200'>- </span><a href='https://www.ual.es'><span class='ecrm-1200'>UAL</span></a></div><br />
<div class='date'><span class='ecrm-1200'>August 6, 2024</span></div>
   </div>
   <h3 class='likesectionHead'><a id='x1-1000'></a>Contents</h3>
   <div class='tableofcontents'>
    <span class='sectionToc'>1 <a href='#x1-20001' id='QQ2-1-2'>Spatial decorrelation</a></span>
<br />    <span class='sectionToc'>2 <a href='#x1-30002' id='QQ2-1-3'>Benefits of spatial transforms</a></span>
<br />    <span class='sectionToc'>3 <a href='#x1-40003' id='QQ2-1-4'>Scalar quantization and rate/distortion optimization in the transform domain</a></span>
<br />    <span class='sectionToc'>4 <a href='#x1-50004' id='QQ2-1-5'>Learned transforms</a></span>
<br />    <span class='sectionToc'>5 <a href='#x1-60005' id='QQ2-1-6'>2D-partitioning</a></span>
<br />    <span class='sectionToc'>6 <a href='#x1-70006' id='QQ2-1-7'>Resources</a></span>
<br />    <span class='sectionToc'>7 <a href='#x1-80007' id='QQ2-1-8'>To-Do</a></span>
<br />    <span class='sectionToc'>8 <a href='#x1-90008' id='QQ2-1-9'>References</a></span>
   </div>
<!-- l. 43 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>1   </span> <a id='x1-20001'></a>Spatial decorrelation</h3>
<!-- l. 46 --><p class='noindent'>Spatial transforms used in image and video compression exploit the statistical correlation (and
perceptual redundancy<span class='footnote-mark'><a href='#fn1x0' id='fn1x0-bk'><sup class='textsuperscript'>1</sup></a></span><a id='x1-2001f1'></a>)
that the pixels show as a consequence of the spatial (2D) correlation that is present
in most of the images (and video frames). For example, some areas of an image can
occur more than once (in the same image). Also, usually happens that neighbor
pixels have similar values.
</p><!-- l. 54 --><p class='indent'>   While color transforms are pixel-wise computations, spatial transforms are
image-wise. This means that the transform inputs an image (of pixels) and
outputs a matrix of coefficients, which generally express the resemblance
between the image and a set of basis functions, usually orthogonal.
For example, after using the 2D-DCT (two-dimensional Discrete Cosine
                                                                  

                                                                  
Transform) <span class='cite'>[<a href='#Xvruiz__DCT'>2</a>]</span>, (the index of) each coefficient represents a different spatial
frequency<span class='footnote-mark'><a href='#fn2x0' id='fn2x0-bk'><sup class='textsuperscript'>2</sup></a></span><a id='x1-2003f2'></a>
and its value, the amount of the corresponding basis found in the image. In
the case of the 2D-DWT <span class='cite'>[<a href='#Xvruiz__DWT'>3</a>]</span>, the coefficients “speak” additionally about a
spatial resolution in the image pyramid and position inside of the pyramid
level.
</p><!-- l. 70 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>2   </span> <a id='x1-30002'></a>Benefits of spatial transforms</h3>
<!-- l. 73 --><p class='noindent'>Most spatial transforms provide:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-3002x1'><span class='ecbx-1000'>Energy concentration: </span>Usually, a small set of (low-frequency) coefficients
     represents most of the information (energy) of the image. This decreases
     the entropy<span class='footnote-mark'><a href='#fn3x0' id='fn3x0-bk'><sup class='textsuperscript'>3</sup></a></span><a id='x1-3003f3'></a>
     and increases the range of the quantization step sizes, because the dynamic
     range of the coefficients is higher than the dynamic range of the pixels<span class='footnote-mark'><a href='#fn4x0' id='fn4x0-bk'><sup class='textsuperscript'>4</sup></a></span><a id='x1-3005f4'></a>.
     </li>
<li class='enumerate' id='x1-3008x2'><span class='ecbx-1000'>Low/High  frequency  analysis:  </span>The  human  visual  system  is  more
     sensitive to the low frequencies (for this reason, the <a href='https://en.wikipedia.org/wiki/Contrast_(vision)#Contrast_sensitivity'>contrast sensitiviy
     function</a> is not flat). This means that we can quantize more severely the
     high frequencies without generating a perceptible distortion.
     </li>
<li class='enumerate' id='x1-3010x3'><span class='ecbx-1000'>Multiresolution:  </span>Depending  on  the  transform,  it  is  possible  to
     reconstruct the original image by resolution levels <span class='cite'>[<a href='#Xvruiz__DWT'>3</a>]</span>. This option can be
     interesting when the resolution at which the image must be reconstructed
     is not known a priori. For example, JPEG 2000 (which is based on the
     2D-DWT) is used in <a href='https://jpeg.org/jpeg2000/applications.html'>digital cinema</a> because, although movie players do
     not have the same resolution in all movie theaters, the same code-stream
     (with the maximum resolution) can be used in all of them.</li></ol>
<!-- l. 106 --><p class='noindent'>
</p>
                                                                  

                                                                  
   <h3 class='sectionHead'><span class='titlemark'>3   </span> <a id='x1-40003'></a>Scalar quantization and rate/distortion optimization in the transform
domain</h3>
<!-- l. 109 --><p class='noindent'>Scalar quantization is efficient in the transform domain because the coefficients
are decorrelated. The next logical step (after quantization) is the entropy
coding of the quantization indexes. Here, depending on how the coefficients
are quantized, we can trace different RD curves (all of them starting
(and finising) in the same distortion). For example, if we compress each
subband<span class='footnote-mark'><a href='#fn5x0' id='fn5x0-bk'><sup class='textsuperscript'>5</sup></a></span><a id='x1-4001f5'></a>
independently, we must find the quantization step sizes that select the same slope in
the RD curve of each subband <span class='cite'>[<a href='#Xvruiz__information_theory'>4</a>]</span>. A RD curve is a discrete convex function where
each line that connects adjacent RD points has a slope. Futhermore, each
RD point generate a line (with a corresponding slope) between it and the
lowest-quality RD point. In this last context, given a target distortion or rate,
the quantization step size used in each subband should generate the same
slope.
</p><!-- l. 129 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>4   </span> <a id='x1-50004'></a>Learned transforms</h3>
<!-- l. 132 --><p class='noindent'>Using machine learning techniques (for example, with an artificial neural network), it
is possible to build a machine-krafted transform, specifically tuned for some
type of images, or at least, capable of determining specific features in the
images.
</p><!-- l. 137 --><p class='indent'>   Potentially, learned-based image (and video) compressors are adaptive
algorithms that can be more efficient than those in which the transforms are
pre-defined.
</p><!-- l. 143 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>5   </span> <a id='x1-60005'></a>2D-partitioning</h3>
<!-- l. 145 --><p class='noindent'>Depending on the content of the image, it can be necessary to divide the image into
2D chunks (usually called <span class='ecti-1000'>tiles </span>or <span class='ecti-1000'>blocks</span>), and encode each chunk independently. In
general:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-6002x1'>Tiles are used when the image is made up of very different areas (for
     example, with text and natural content). Tiles are usually rectangular but
     can have any size, and are usually defined attending to RD or perceptual
     issues (for example, text is not well compressed by lossy configurations).
                                                                  

                                                                  
     </li>
<li class='enumerate' id='x1-6004x2'>Blocks are smaller than tiles and, in most of cases, squared. The block
     partition can be adaptive and, in this case, should be found using RDO
     (Rate Distortion Optimization).</li></ol>
<!-- l. 159 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>6   </span> <a id='x1-70006'></a>Resources</h3>
<!-- l. 162 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-7002x1'>Use of <a href='https://github.com/Sistemas-Multimedia/VCF/blob/main/src/DWT.py'>2D-DWT in VCF</a>.
     </li>
<li class='enumerate' id='x1-7004x2'><a href='https://github.com/vicente-gonzalez-ruiz/DCT2D/blob/master/src/DCT2D/YCoCg_2D_DCT_SQ.ipynb'>Image Compression with YCoCg + 2D-DCT</a>.
     </li>
<li class='enumerate' id='x1-7006x3'><a href='https://www.tensorflow.org/tutorials/generative/data_compression'>Learned data compression</a>.
     </li>
<li class='enumerate' id='x1-7008x4'><a href='https://github.com/vicente-gonzalez-ruiz/learned_image_compression/blob/main/LIC.ipynb'>Learned Image Compression (LIC) using auto-encoders</a>.
     </li>
<li class='enumerate' id='x1-7010x5'><a href='https://github.com/fchollet/deep-learning-with-python-notebooks'>Companion  Jupyter  notebooks  for  the  book  “Deep  Learning  with
     Python”</a> <span class='cite'>[<a href='#Xchollet2021deep'>1</a>]</span>.</li></ol>
                                                                  

                                                                  
<!-- l. 181 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>7   </span> <a id='x1-80007'></a>To-Do</h3>
<!-- l. 184 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-8002x1'>Modify VCF to use the block-based 2D-DCT in the compression pipeline.
     Complexity 3.
     </li>
<li class='enumerate' id='x1-8004x2'>Using  RDO,  determine  the  optimal  color  transform  in  the  codec
     <span class='ectt-1000'>2D-DCT.py</span>. Complexity 2.
     </li>
<li class='enumerate' id='x1-8006x3'>Using  RDO,  determine  the  optimal  color  transform  in  the  codec
     <span class='ectt-1000'>2D-DWT.py</span>. Complexity 2.
     </li>
<li class='enumerate' id='x1-8008x4'>Using RDO, determine the optimal number of levels of the 2D-DWT in
     the codec <span class='ectt-1000'>2D-DWT.py</span>. Complexity 2.
     </li>
<li class='enumerate' id='x1-8010x5'>Using RDO, determine the optimal DWT basis (defined in <a href='https://pywavelets.readthedocs.io/en/latest/'>PyWavelets</a>)
     in the codec <span class='ectt-1000'>2D-DWT.py</span>. Complexity 3.
     </li>
<li class='enumerate' id='x1-8012x6'>Create a new image codec (similar to <span class='ectt-1000'>2D-DWT.py </span>and <span class='ectt-1000'>2D-DCT.py</span>) where
     the  latent  space  generated  by  an  autoencoder  is  entropy  encoded  (in
     other words, replace the 2D-DCT or the 2D-DWT by an autoencoder).
     Complexity 15.
     </li>
<li class='enumerate' id='x1-8014x7'>Autoencoders can learn to analyze signals represented in any domain.
     Create a new DCT- or DWT-based image codec where the autoencoder is
     applied to the transform domain. Complexity 20.</li></ol>
<!-- l. 214 --><p class='noindent'>
</p>
   <h3 class='sectionHead'><span class='titlemark'>8   </span> <a id='x1-90008'></a>References</h3>
   <div class='thebibliography'>
   <p class='bibitem'><span class='biblabel'>
 [1]<span class='bibsp'>   </span></span><a id='Xchollet2021deep'></a>Francois Chollet. <a href='https://sourestdeeds.github.io/pdf/Deep%20Learning%20with%20Python.pdf'><span class='ecti-1000'>Deep learning with Python</span></a>. Simon and Schuster, 2021.
   </p>
   <p class='bibitem'><span class='biblabel'>
 [2]<span class='bibsp'>   </span></span><a id='Xvruiz__DCT'></a>V. González-Ruiz. <a href='https://github.com/vicente-gonzalez-ruiz/DCT'>The DCT (Discrete Cosine Transform)</a>.
   </p>
   <p class='bibitem'><span class='biblabel'>
 [3]<span class='bibsp'>   </span></span><a id='Xvruiz__DWT'></a>V. González-Ruiz. <a href='https://github.com/vicente-gonzalez-ruiz/DWT'>The DWT (Discrete Wavelet Transform)</a>.
   </p>
   <p class='bibitem'><span class='biblabel'>
 [4]<span class='bibsp'>   </span></span><a id='Xvruiz__information_theory'></a>V. González-Ruiz. <a href='https://github.com/vicente-gonzalez-ruiz/information_theory'>Information Theory</a>.
</p>
   </div>
   <div class='footnotes'><a id='x1-2002x1'></a>
<!-- l. 48 --><p class='indent'>     <span class='footnote-mark'><a href='#fn1x0-bk' id='fn1x0'><sup class='textsuperscript'>1</sup></a></span><span class='ecrm-0800'>We will see this with more detail later in this course.</span></p><a id='x1-2004x1'></a>
<!-- l. 62 --><p class='indent'>     <span class='footnote-mark'><a href='#fn2x0-bk' id='fn2x0'><sup class='textsuperscript'>2</sup></a></span><span class='ecrm-0800'>That depends on the position of the coefficient in the transformed domain.</span></p><a id='x1-3004x3'></a>
<!-- l. 80 --><p class='noindent'><span class='footnote-mark'><a href='#fn3x0-bk' id='fn3x0'><sup class='textsuperscript'>3</sup></a></span><span class='ecrm-0800'>When the entropy is decreased while the information is preserved, this usually means that
</span><span class='ecrm-0800'>an entropy encoding algorithm will perform better.</span></p><a id='x1-3006x4'></a>
<!-- l. 86 --><p class='noindent'><span class='footnote-mark'><a href='#fn4x0-bk' id='fn4x0'><sup class='textsuperscript'>4</sup></a></span><span class='ecrm-0800'>Quantization is a discrete operation constrained by the number of bits used to represent
</span><span class='ecrm-0800'>the quantization indexes. When the dynamic range of a signal is high, this makes possible to
</span><span class='ecrm-0800'>use more quantization levels and therefore, a higher number of available RD points.</span></p><a id='x1-4002x3'></a>
<!-- l. 117 --><p class='indent'>     <span class='footnote-mark'><a href='#fn5x0-bk' id='fn5x0'><sup class='textsuperscript'>5</sup></a></span><span class='ecrm-0800'>In the case of a spatial transform, a subband is form by all the coefficients that describe
</span><span class='ecrm-0800'>the same frequency components in different areas or resolutions (when available) of the
</span><span class='ecrm-0800'>image.</span></p>                                                                                                                 </div>
 
</body> 
</html>