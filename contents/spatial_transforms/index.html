<!DOCTYPE html> 
<html lang='en-US' xml:lang='en-US'> 
<head> <title>Sistemas Multimedia - Spatial Transforms</title> 
<meta charset='utf-8' /> 
<meta content='TeX4ht (https://tug.org/tex4ht/)' name='generator' /> 
<meta content='width=device-width,initial-scale=1' name='viewport' /> 
<link href='index.css' rel='stylesheet' type='text/css' /> 
<meta content='index.tex' name='src' /> 
<script>window.MathJax = { tex: { tags: "ams", }, }; </script> 
 <script async='async' id='MathJax-script' src='https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js' type='text/javascript'></script>  
</head><body>
   <div class='maketitle'>
                                                                  

                                                                  
                                                                  

                                                                  

<h2 class='titleHead'><a href='https://sistemas-multimedia.github.io/'>Sistemas Multimedia</a> - Spatial Transforms</h2>
 <div class='author'><a href='https://cms.ual.es/UAL/personas/persona.htm?id=515256515553484875'><span class='ecrm-1200'>Vicente González Ruiz</span></a> <span class='ecrm-1200'>- </span><a href='https://www.ual.es/universidad/departamentos/informatica'><span class='ecrm-1200'>Departamento de Informática</span></a> <span class='ecrm-1200'>- </span><a href='https://www.ual.es'><span class='ecrm-1200'>UAL</span></a></div><br />
<div class='date'><span class='ecrm-1200'>December 28, 2022</span></div>
   </div>
   <h3 class='likesectionHead' id='contents'><a id='x1-1000'></a>Contents</h3>
   <div class='tableofcontents'>
    <span class='sectionToc'>1 <a href='#spatial-decorrelation' id='QQ2-1-2'>Spatial decorrelation</a></span>
<br />    <span class='sectionToc'>2 <a href='#benefits-of-spatial-transforms' id='QQ2-1-3'>Benefits of spatial transforms</a></span>
<br />    <span class='sectionToc'>3 <a href='#scalar-quantization-and-ratedistortion-optimization-in-the-transform-domain' id='QQ2-1-4'>Scalar quantization and rate/distortion optimization in the transform domain</a></span>
<br />    <span class='sectionToc'>4 <a href='#learned-transforms' id='QQ2-1-5'>Learned transforms</a></span>
<br />    <span class='sectionToc'>5 <a href='#resources' id='QQ2-1-6'>Resources</a></span>
<br />    <span class='sectionToc'>6 <a href='#todo' id='QQ2-1-7'>To-Do</a></span>
<br />    <span class='sectionToc'>7 <a href='#references' id='QQ2-1-8'>References</a></span>
   </div>
<!-- l. 10 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='spatial-decorrelation'><span class='titlemark'>1   </span> <a id='x1-20001'></a>Spatial decorrelation</h3>
<!-- l. 12 --><p class='noindent'>Spatial transforms used in image and video compression exploit the statistical correlation (and
perceptual redundancy<span class='footnote-mark'><a href='#fn1x0' id='fn1x0-bk'><sup class='textsuperscript'>1</sup></a></span><a id='x1-2001f1'></a>)
that the pixels show as a consequence of the spatial (2D) correlation. For example,
some areas of an image can occur more than once, and usually, neighbor pixels tend
to have similar values.
</p><!-- l. 19 --><p class='indent'>   Spatial transforms are image-wise operators. This means that the transform
inputs an image of pixels and output a matrix of coefficients, which generally express
the resemblance between the image and a set of basis function. For example, after
using the DCT <span class='cite'>[<a href='#Xvruiz__DCT'>2</a>]</span>, each (index) coefficient represents a different frequency and its
value, the amount of the corresponding basis found in the image. In the case of the
                                                                  

                                                                  
DWT <span class='cite'>[<a href='#Xvruiz__DWT'>3</a>]</span>, the index of the coefficients “speaks” also about a spatial resolution and
position.
</p><!-- l. 29 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='benefits-of-spatial-transforms'><span class='titlemark'>2   </span> <a id='x1-30002'></a>Benefits of spatial transforms</h3>
<!-- l. 31 --><p class='noindent'>Spatial transforms provide:
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-3002x1'><span class='ecbx-1000'>Energy   concentration:  </span>A   small   set   of   (usually   low-frequency)
     coefficients (in general) represents most of the information (energy) of
     the  image.  This  decreases  the  entropy  and  increases  the  range  of  the
     quantization step sizes, because the dynamic range of the coefficients is
     higher than the dynamic range of the pixels.
     </li>
<li class='enumerate' id='x1-3004x2'><span class='ecbx-1000'>Low/High frequency analysis: </span>Our visual system is more sensitive to
     the low frequencies (for this reason, the <a href='https://en.wikipedia.org/wiki/Contrast_(vision)#Contrast_sensitivity'>contrast sensitiviy function</a> is not
     flat.
     </li>
<li class='enumerate' id='x1-3006x3'><span class='ecbx-1000'>Multiresolution:  </span>Depending  on  the  transform,  it  is  possible  to
     reconstruct the original image by resolution levels <span class='cite'>[<a href='#Xvruiz__DWT'>3</a>]</span>.</li></ol>
<!-- l. 48 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='scalar-quantization-and-ratedistortion-optimization-in-the-transform-domain'><span class='titlemark'>3   </span> <a id='x1-40003'></a>Scalar quantization and rate/distortion optimization in the transform
domain</h3>
<!-- l. 50 --><p class='noindent'>Scalar quantization is efficient in the transform domain because the coefficients are
decorrelated. The next logical step (after quantization) is the entropy coding of the
quantization indexes. Here, depending on how the coefficients are quantized we can
trace different RD curves (all of them starting (and finising) in the same distortion).
For example, if we compress each subband independently, we must find the
quantization step sizes that select the same slope in the RD curve of each
subband.
                                                                  

                                                                  
</p><!-- l. 59 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='learned-transforms'><span class='titlemark'>4   </span> <a id='x1-50004'></a>Learned transforms</h3>
<!-- l. 61 --><p class='noindent'>Using machine learning techniques (for example, with an artificial neural network), it
is possible to build a machine-krafted transform, specifically tuned for some
type of images, or at least, capable of determining specific features in the
images.
</p><!-- l. 66 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='resources'><span class='titlemark'>5   </span> <a id='x1-60005'></a>Resources</h3>
<!-- l. 67 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-6002x1'><a href='https://github.com/Sistemas-Multimedia/VCF/blob/main/src/DWT2D.py'>2D-DWT in VCF</a>.
     </li>
<li class='enumerate' id='x1-6004x2'><a href='https://github.com/vicente-gonzalez-ruiz/DCT/blob/master/docs/YCoCg_2D_DCT_SQ.ipynb'>Image Compression with YCoCg + 2D-DCT</a>.
     </li>
<li class='enumerate' id='x1-6006x3'><a href='https://www.tensorflow.org/tutorials/generative/data_compression'>Learned data compression</a>.
     </li>
<li class='enumerate' id='x1-6008x4'><a href='https://github.com/vicente-gonzalez-ruiz/learned_image_compression/blob/main/LIC.ipynb'>Learned Image Compression (LIC) using autoencoders</a>.
     </li>
<li class='enumerate' id='x1-6010x5'><a href='https://github.com/fchollet/deep-learning-with-python-notebooks'>Companion  Jupyter  notebooks  for  the  book  “Deep  Learning  with
     Python”</a> <span class='cite'>[<a href='#Xchollet2021deep'>1</a>]</span>.</li></ol>
                                                                  

                                                                  
<!-- l. 84 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='todo'><span class='titlemark'>6   </span> <a id='x1-70006'></a>To-Do</h3>
<!-- l. 85 --><p class='noindent'>
     </p><ol class='enumerate1'>
<li class='enumerate' id='x1-7002x1'>Modify VCF to use the block-based 2D-DCT in the compression pipeline.
     Complexity 3.
     </li>
<li class='enumerate' id='x1-7004x2'>Modify VCF to use an autoencoder (at the image level) in the compression
     pipeline. Complexity 5.</li></ol>
<!-- l. 134 --><p class='noindent'>
</p>
   <h3 class='sectionHead' id='references'><span class='titlemark'>7   </span> <a id='x1-80007'></a>References</h3>
   <div class='thebibliography'>
   <p class='bibitem'><span class='biblabel'>
 [1]<span class='bibsp'>   </span></span><a id='Xchollet2021deep'></a>Francois  Chollet.   <a href='http://silverio.net.br/heitor/disciplinas/eeica/papers/Livros/[Chollet#-' id='D'><span class='ecti-1000'>eep_Learning_with_Python.pdf]Deep learning with
   </span><span class='ecti-1000'>Python</span></a>. Simon and Schuster, 2021.
   </p>
   <p class='bibitem'><span class='biblabel'>
 [2]<span class='bibsp'>   </span></span><a id='Xvruiz__DCT'></a>V. González-Ruiz. <a href='https://github.com/vicente-gonzalez-ruiz/DCT'>The DCT (Discrete Cosine Transform)</a>.
   </p>
   <p class='bibitem'><span class='biblabel'>
 [3]<span class='bibsp'>   </span></span><a id='Xvruiz__DWT'></a>V. González-Ruiz. <a href='https://github.com/vicente-gonzalez-ruiz/DWT'>The DWT (Discrete Wavelet Transform)</a>.
</p>
   </div>
   <div class='footnotes'><!-- l. 14 --><p class='indent'>     <span class='footnote-mark'><a href='#fn1x0-bk' id='fn1x0'><sup class='textsuperscript'>1</sup></a></span><span class='ecrm-0800'>We will see this later in this course.</span></p>                                                                </div>
 
</body> 
</html>