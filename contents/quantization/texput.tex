% Emacs, this is -*-latex-*-

\input{../../definitions}

\title{\SM{} - Digital (Re-)Quantization}

\maketitle

\tableofcontents

\section{What is Quantization?}

In Information Theory, Quantization is any mapping process between two
sets of elements $A$ and $B$, where all the elements of $A$ are
associated to one element of $B$, and happens that $|A|>|B|$, where
$|\cdot|$ represents the orde (the number of elements) of a group.

Notice that because $|A|>|B|$, quantization is an irreversible process
because two o more elements of $A$ will be mapped to the same element
of $B$.

\section{What is Digital Quantization?}

In Digital Quantization, the elements of $A$ and $B$ are digital
samples of a signal. Since, by definition, a digital sample has been
quantized to be converted from the analog world to the digital one, we
are appliying a re-quantization of the signal. Again, such digital
quantization implies a loss of information in the re-quantized signal.

\section{Basic terminology}

Several types of quantizers divide the range of possible values that
the samples can take into a collection of intervals. The values that
define such intervals are called decision levels and the value that in
the quantized domain represents to all the input posible values that
fall in a interval is called the representation level of the interval.

The size of each interval is called the quantization step size.

\section{Classification of Quantizers}

\subsection{Scalar VS Vector Quantizers}

When quantization maps single elments of $A$ to single elements of
$B$, the quantizer is said scalar. When we map tuples of elements, we
are using Vector Quantization.

\subsection{Uniform VS Non-Uniform Quantizers}

In a uniform quantizer, all the intervals have the same size. In a
non-uniform quantizer, at least one of the intervals is different to
the rest. For example, a deadzone quantizer has a interval size for
the representation level 0 that doubles the size of the rest of
intervals. For this reason, a deadzone quantizer is a non-uniform
quantizer.

\subsection{Static VS Adaptive Quantizers}

Static quantizers are defined without considering the specific
characteristics of the sequence of samples to quantize. Adaptive
quantizers adapt to the sequence of samples that are quantized. For
example, a Lloyd-Max quantizer divides the range of input values in a
set of intervals whose size is inversely proportional to the
probability of using such interval. On the contrary, a static
quantizer keeps fixed the size of the intervals.

\section{Deazone Quantization}

Deadzone quantizers are static quasi-uniform scalar quantizers. These
are used frequently in lossy compression systems because: (1) when the
quantization step size is a power of two, and (2) the input sample
values are integers (and negative integers are represented using two's
complement), then the representation levels can be found by simply
discarding low-significant bits of the input samples (in other words,
we only need to perform a bit-shift operation to find the corresponding quantization index).

Another reason that make deadzone quantization popular in lossy
encoding systems is that tends to remove electronic noise more than
other quantizers. If we supose that the signal has 0 average (the
electronic noise has 0 average and a flat spectrum) the deadzone is
places where the SNR is smaller, and therefore, we are replacing
electronic noise by quantization noise. Obviously, this does not
improve the RD performance of the encoder, but the perceived
(subjective) quality is increased for the same bit-rate (if the
electronic noise is perceived as a source of distortion).

\subsection*{Resources}
\begin{enumerate}
\item
  \href{https://github.com/Sistemas-Multimedia/VCF/blob/main/src/deadzone.py}{Deadzone
    Quantization in VCF}.
\end{enumerate}

\section{Lloyd-Max Quantization}

A Lloyd-Max quantizer minimizes the quantization noise given a number
of quantization intervals. To do this, the quantizer must know the
histogram (the probabilities) of the input samples. As a result, the
density of quantization intervals is higher where the probability of
the samples is higher and viceversa.

Notice that the histogram must be known by the decoder to ``restore''
the information.

\subsection*{Resources}

\begin{enumerate}
\item
  \href{https://github.com/Sistemas-Multimedia/VCF/blob/main/src/LloydMax.py}{A
    partial implementation in VCF}.
\end{enumerate}

\section{Vector Quantization~\cite{vruiz__vector_quantization}}

Vector Quantizers input (usually squared) blocks of samples and output
a quantization index per block. In most of natural images, the spatial
correlation generates that some blocks of the image are similar to
other blocks. If this is true, we can compute a set of centroids
(blocks) and use them to represent the original blocks. The encoding
performance of a Vector Quantizer is superior compared to a Scalar
Quantizer because the number of quantization indexes (centroid
indexes) is smaller.

\subsection*{Resources}

\begin{enumerate}
\item \href{https://scikit-learn.org/stable/auto_examples/cluster/plot_face_compress.html#sphx-glr-auto-examples-cluster-plot-face-compress-py}{Vector Quantization Example}.
\item
  \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/contents/gray_VQ/gray_VQ.ipynb}{Vector
    Quantization (in the 2D domain) of a gray-scale image}.
\item \href{https://github.com/vicente-gonzalez-ruiz/image_vector_quantization_LBG}{Image compression using LBG}.
\end{enumerate}

\section{To-Do}
\begin{enumerate}
\item Modify VCF to allow the use of Lloyd-Max Quantization in the
  compression pipeline. Up to 2 people in the group.
\item Modify VCF to allow the use of Vector Quantization in the
  compression pipeline. Up to 3 people in the group.
\end{emumerate}

\section{References}

\renewcommand{\addcontentsline}[3]{}% Remove functionality of \addcontentsline
\bibliography{text-compression,image-formats,image-video-theory,quantization}
