% Emacs, this is -*-latex-*-

% https://web.stanford.edu/class/ee398a/handouts/lectures/05-Quantization.pdf

\input{../../definitions}

\title{\href{https://sistemas-multimedia.github.io/contents/quantization/}{(Digital Re-)Quantization}}

\maketitle

\tableofcontents

\section{What is quantization?}

In Information Theory~\cite{vruiz__information_theory}, quantization
\cite{vruiz__signal_quantization,vruiz__scalar_quantization,vruiz__vector_quantization,vruiz__trellis_quantization}
is any mapping process between two sets of elements $A$ and $B$, where
all elements of $A$ are associated with one (not necessarily the same)
element of $B$, being $|A|>|B|$, where $|\cdot|$ represents the order
(the number of elements) of a set.

Notice that, when $|A|>|B|$, quantization is an irreversible process
because at least two elements of $A$ will be mapped to the same
element of $B$, and therefore, there is no way to find the reverse
mapping\footnote{Dequantization does not exist.}.

\section{What is (digital re-)quantization?}

In the quantization of digital signals
\cite{vruiz__scalar_quantization,vruiz__vector_quantization}, the
elements of $A$ and $B$ are digital samples
\cite{vruiz__signal_quantization}. Since, by definition, digital
samples have been quantized to be converted from the analog world to
the digital one, we are actually applying a re-quantization. Again, if
$|A|>|B|$, such quantization implies a loss of information in the
quantized signal.

\section{Basic terminology}

When working with 1D signals, quantizers divide the range of possible
input values that the samples can take into a collection of
non-overlapped intervals\footnote{If the signal has more than 1
  dimension, the process is exactly the same, but we don't speak of
  intervals, but regions, for example, in the 2D case.}. The values
that define such intervals are called \emph{decision levels}, and the
value that in the quantized domain represents all the input possible
values that fall into an interval is called the \emph{representation
  level} of the interval.

\section{Types of quantizers}

\subsection{Scalar quantizers (SQ) VS Vector Quantizers (VQ)}

When quantization maps single elements of $A$ to single elements of
$B$, the quantizer is said to be
\emph{scalar}~\cite{vruiz__scalar_quantization}. When tuples of
elements are mapped, VQ is
used~\cite{vruiz__vector_quantization}. Notice that this
classification is not related to the dimensionality of the signal, but
whether the samples are processed independently or whether they are
quantized by tuples. In this last case, we can exploit the correlation
between samples.

\subsection{Uniform VS non-uniform quantizers}

In an uniform quantizer~\cite{vruiz__scalar_quantization}, all the
intervals have the same the size, which is equal to the quantization
step size, $\Delta$.\footnote{Notice that, in an uniform quantizer,
  the distance between all the decision levels is also $\Delta$.} In a
non-uniform quantizer~\cite{vruiz__scalar_quantization}, at least one
of the intervals is different to the rest. For example, a deadzone
quantizer \cite{vruiz__scalar_quantization} has an interval size for
the representation level 0 that doubles the size of the the rest of
intervals. Another example of a non-uniform quantizer is a
\href{https://en.wikipedia.org/wiki/Lloyd%27s_algorithm}{Lloyd-Max
  quantizer} \cite{vruiz__scalar_quantization} because it divides the
range of input values in a set of intervals whose size is inversely
proportional to the probability of using such interval.

\subsection{Static VS adaptive quantizers}

Static quantizers do not modify the partitioning of the input signal's
dynamic range (the decision levels), nor the representation level
assigned to each interval. On the contrary, adaptive quantizers adapt
such parameters to the sequence of samples that are
quantizing.\footnote{Notice that, in the case of the quantizer depends
  on the characteristics of the signal but they are known \emph{a
    priori}, it should be considered static.} A Jayant
quantizer~\cite{vruiz__scalar_quantization} is a example of adaptive
quantization.

\section{Deadzone quantization}

Deadzone quantizers~\cite{vruiz__scalar_quantization} are static
quasi-uniform scalar quantizers. These are used frequently in lossy
compression systems because when the quantization step size is a power
of two and the input sample values are integers\footnote{... and negative
integers are represented using two's complement ...}, then the
representation levels can be found simply by discarding
low-significant bits of the input samples (in other words, we only
need to perform a bit-shift operation to find the corresponding
quantization index). This also means that it is possible to build a
progressive encoding system using a deadzone quantizer.

Another reason why dead-zone quantization is popular in lossy encoding
systems is that it tends to remove electronic noise more than other
scalar quantizers where the signal is weaker. If we suppose that the
signal has 0 mean\footnote{The electronic noise has 0 arithmetic mean
  and a flat spectrum.} and that the dead-zone is placed where the
\href{https://en.wikipedia.org/wiki/Signal-to-noise_ratio}{SNR} is
smaller, we basically replace electronic noise by quantization
noise\footnote{Which also has 0 average and a flat
  spectrum}. Obviously, this does not improve the RD performance of
the encoder, but the perceived (subjective) quality is increased for
the same bit rate.\footnote{Obviously, if the electronic noise is
  perceived as a source of distortion comparable to the quantization
  noise.}

\section{Lloyd-Max quantization}

A Lloyd-Max quantizer~\cite{vruiz__scalar_quantization} minimizes
quantization noise given a signal with a known probabilistic
distribution (histogram) of the input samples. As a result, the
density of quantization intervals is higher where the probability of
the samples is higher, and \textit{vice versa}.

Lloyd-Max quantizers are considered non-uniform quantizers. Notice
that the histogram must be also known by the decoder to
``restore''\footnote{Remember that quantization is a irreversible
  process and therefore, the original signal is never restored (except
  if $\Delta=1$ and we are using digital quantization).} the
information.

\section{Vector quantization applied to the spatial domain}

Vector quantizers~\cite{vruiz__vector_quantization} input
blocks\footnote{If we are removing spatial redundancy, the blocks are
  usually squared tiles of pixels. If we remove color redundancy, the
  blocks are multicomponent pixels, for example, RGB values.} of
samples and output a quantization index per block. For example, in
most natural images, the spatial
correlation~\cite{vruiz__visual_redundancy} generates that some blocks
of the image are similar to other blocks. If this is true, we can
compute a set of centroids (blocks) and use them to represent the
original blocks. As a result, we will obtain a matrix of quantization
indexes that can be entropy coded.

Notice that VQ exploits the spatial correlation. For this reason, the
encoding performance of a vector quantizer is usually superior
compared to that of a scalar quantizer, because the number of output
quantization indexes\footnote{Indexes of the centroids.} is smaller.

\section{Vector quantization applied to the $\text{RGB}$ domain}

SQ (Scalar
Quantization)~\cite{vruiz__scalar_quantization,sayood2017introduction}
would be an optimal solution only if the image colors are uniformly
distributed within
\href{https://en.wikipedia.org/wiki/RGB_color_model}{the RGB
  cube}. However, the typical color distribution in natural images is
anything but uniform, with some regions of the color space being
densely populated and many potentially used colors completely
missing. In this case, depending on the quantization step
size~\cite{vruiz__signal_quantization}, SQ could be suboptimal because
the colors used may not be sampled with sufficient density, while at the
same time the encoding system considers colors that do not appear
in the image at all~\cite{burger2016digital}.

On the other hand, VQ (Vector
Quantization)~\cite{vruiz__vector_quantization,sayood2017introduction}
applied to the color domain does not treat the individual $\text{RGB}$
components separately as does scalar quantization, but each color
vector used ${\mathbf C}_i = (\text{R}_i, \text{G}_i, \text{B}_i )$ in
the image is treated as a minimum structure. VQ determines a code-book
of $K$ code-vectors (centroids) that minimizes the distortion between
the original image and the reconstructed one. Notice that the
code-book must be known by the decoder to find a reconstruction.

\section*{Resources}

\begin{enumerate}
\item \href{https://github.com/Sistemas-Multimedia/VCF/blob/main/notebooks/deadzone.ipynb}{deadzone.ipynb}: Notebook showing the use of deadzone.py.
\item Usage of \href{https://github.com/Sistemas-Multimedia/VCF/blob/main/notebooks/LloydMax.ipynb}{LloydMax in VCF}.
  \item Usage of \href{https://github.com/Sistemas-Multimedia/VCF/blob/main/notebooks/VQ.ipynb}{VQ} in VQ.
\item
  \href{https://scikit-learn.org/stable/auto_examples/cluster/plot_face_compress.html#sphx-glr-auto-examples-cluster-plot-face-compress-py}{Vector
    Quantization Example}.
\item
  \href{https://github.com/vicente-gonzalez-ruiz/vector_quantization/blob/main/docs/gray_VQ.ipynb}{Vector
    Quantization (in the 2D domain) of a grayscale image}.
\item
  \href{https://github.com/vicente-gonzalez-ruiz/vector_quantization/blob/main/docs/spatial_color_VQ.ipynb}{Vector
    Quantization (in the 2D domain) of a color (RGB) image}.
  \item \href{https://github.com/droidadroit/LBG}{Image compression using LBG}.
\end{enumerate}

\section*{To do}
\begin{enumerate}
\item Quantizers generate quantization noise, which can be modeled as
  flat-spectrum zero-mean uniform noise. On the contrary, most of the
  energy of natural images is concentrated in the low
  frequencies. Consequently, when an image is quantized, the
  \href{https://en.wikipedia.org/wiki/Signal-to-noise_ratio}{SNR} is
  higher in the low frequencies than in the high frequencies, which
  means that if the quantized image is low-pass filtered, the overall
  SNR should increase (the filter will remove the high frequencies
  that basically are generated by the quantization noise). Modify the
  current quantization VCF modules to, optionally, apply a low-pass
  filter to the quantized image. Consider selecting between different
  filtering techniques. Complexity 4.

\item Create a new module named \texttt{color\_VQ.py} that use the
  Vector Quantization to exploit the $\text{RGB}$ redundancy in
  images. Complexity 5.
\end{enumerate}

\section{References}

\renewcommand{\addcontentsline}[3]{}% Remove functionality of \addcontentsline
\bibliography{quantization,perception,information_theory}
