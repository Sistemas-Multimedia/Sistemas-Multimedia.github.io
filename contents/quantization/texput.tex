% Emacs, this is -*-latex-*-

\input{../../definitions}

\title{\SM{} - Digital (Re-)Quantization}

\maketitle

\tableofcontents

\section{What is quantization?}

In information theory~\cite{vruiz__information_theory}, quantization
is any mapping process between two sets of elements $A$ and $B$, where
all the elements of $A$ are associated to one element of $B$, and
happens that $|A|>|B|$, where $|\cdot|$ represents the order (the
number of elements) of a group.

Notice that because $|A|>|B|$, quantization is an irreversible process
because two o more elements of $A$ will be mapped to the same element
of $B$, and there is not way to find the inverse mapping.

\section{What is digital quantization?}

In digital
quantization~\cite{vruiz__scalar_quantization,vruiz__vector_quantization},
the elements of $A$ and $B$ are digital samples of a
signal~\cite{vruiz__signal_quantization}. Since, by definition, a
digital sample has been quantized to be converted from the analog
world to the digital one, we are actually appliying a re-Quantization
to the signal. Again, such Digital Quantization implies a loss of
information in the re-quantized signal.

\section{Basic terminology}

Several types of quantizers divide the range of possible values that
the samples can take into a collection of non-overlapped
intervals. The values that define such intervals are called \emph{decision
levels} and the value that in the quantized domain represents to all
the input posible values that fall in a interval is called the
\emph{representation level} of the interval.

The size of each interval is called the quantization step size,
usually represented by $\Delta$.

\section{A classification of quantizers}

\subsection{Scalar VS vector quantizers}

When quantization maps single elments of $A$ to single elements of
$B$, the quantizer is said
scalar~\cite{vruiz__scalar_quantization}. When tuples of elements are
mapped, vector quantization is being
used~\cite{vruiz__vector_quantization}.

\subsection{Uniform VS non-uniform quantizers}

In an uniform quantizer~\cite{vruiz__scalar_quantization}, all the
intervals have the same size. In a non-uniform
quantizer~\cite{vruiz__scalar_quantization}, at least one of the
intervals is different to the rest. For example, a deadzone
quantizer~\cite{vruiz__scalar_quantization} has a interval size for
the representation level 0 that doubles the size of the rest of
intervals. For this reason, a deadzone quantizer is a non-uniform
quantizer.

\subsection{Static VS adaptive quantizers}

Static quantizers are defined without considering the specific
characteristics of the sequence of samples to quantize. Adaptive
quantizers adapt to the sequence of samples that are quantized. For
example, a Lloyd-Max quantizer divides the range of input values in a
set of intervals whose size is inversely proportional to the
probability of using such interval. On the contrary, a static
quantizer keeps fixed the size of the intervals.

\section{Deazone quantization}

Deadzone quantizers~\cite{vruiz__scalar_quantization} are static
quasi-uniform scalar quantizers. These are used frequently in lossy
compression systems because: (1) when the quantization step size is a
power of two, and (2) the input sample values are integers (and
negative integers are represented using two's complement), then the
representation levels can be found by simply discarding
low-significant bits of the input samples (in other words, we only
need to perform a bit-shift operation to find the corresponding
quantization index).

Another reason that makes deadzone quantization popular in lossy
encoding systems is that tends to remove electronic noise more than
other quantizers where the signal is weaker. If we supose that the
signal has 0 average (the electronic noise has 0 average and a flat
spectrum) and that the deadzone is placed where the SNR is smaller, we
are basically replacing electronic noise by quantization
noise. Obviously, this does not improve the RD performance of the
encoder, but the perceived (subjective) quality is increased for the
same bit-rate (if the electronic noise is perceived as a source of
distortion).

\subsection*{Resources}
\begin{enumerate}
\item
  \href{https://github.com/Sistemas-Multimedia/VCF/blob/main/src/deadzone.py}{Deadzone
    Quantization in VCF}.
\end{enumerate}

\section{Lloyd-Max quantization}

A Lloyd-Max quantizer~\cite{vruiz__scalar_quantization} minimizes the
quantization noise given a signal with a known probabilistic
distribution (histogram) of the input samples and a number of
quantization intervals. As a result, the density of quantization
intervals is higher where the probability of the samples is higher and
viceversa.

Notice that the histogram must be also known by the decoder to
``restore''\footnote{Remember that quantization is a irreversible
process and therefore, never is restores the original signal (except
if the signal is digital and $\Delta=1$).} the information.

\subsection*{Resources}

\begin{enumerate}
\item
  \href{https://github.com/Sistemas-Multimedia/VCF/blob/main/src/LloydMax.py}{A
    partial implementation in VCF}.
\end{enumerate}

\section{Vector quantization}

Vector quantizers~\cite{vruiz__vector_quantization} input (usually
squared, in the case of images) blocks of samples and output a
quantization index per block. In most of natural images, the spatial
correlation~\cite{vruiz__visual_redundancy} generates that some blocks
of the image are similar to other blocks. If this is true, we can
compute a set of (block) centroids and use them to represent the
original blocks. As a result, we will obtain an matrix of quantization
indexes that can be entropy coded.

Notice that VQ exploits the spatial correlation. For this reason, the
encoding performance of a vector quantizer is superior compared to a
scalar quantizer because the number of quantization indexes (indexes
of the centroids) is smaller.

\subsection*{Resources}

\begin{enumerate}
\item \href{https://scikit-learn.org/stable/auto_examples/cluster/plot_face_compress.html#sphx-glr-auto-examples-cluster-plot-face-compress-py}{Vector Quantization Example}.
\item
  \href{https://github.com/Sistemas-Multimedia/Sistemas-Multimedia.github.io/blob/master/contents/gray_VQ/gray_VQ.ipynb}{Vector
    Quantization (in the 2D domain) of a gray-scale image}.
\item \href{https://github.com/vicente-gonzalez-ruiz/image_vector_quantization_LBG}{Image compression using LBG}.
\end{enumerate}

\section{The rate/distortion curve}

The gray-scale values or the $\text{RGB}$ components of an image (and
therefore, of a sequence of images) can be quantized and compressed
using some entropy encoder, generating a set of points in the
Rate/Distortion (RD) space. Such set usually form the so called RD
curve in terms of RMSE (between the original image and its
reconstruction) and the bit-rate achieved by the entropy codec.

\section{To-Do}
\begin{enumerate}
\item Modify VCF to allow the use of Lloyd-Max Quantization in the
  compression pipeline. Complexity 2.
\item Modify VCF to allow the use of Vector Quantization (applied to
  the spatial domain) in the compression pipeline. Complexity 3.
\end{enumerate}

\section{References}

\renewcommand{\addcontentsline}[3]{}% Remove functionality of \addcontentsline
\bibliography{quantization,perception}
